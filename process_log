[yugabyte@wang-tu-yba ~]$ ./w.py
2025-06-02 14:37:08,862 - INFO - Running: gcloud compute instances list --filter networkInterfaces[0].networkIP=10.128.15.231 --format json --project yuga-rjw
2025-06-02 14:37:10,651 - INFO - Processing node yb-dev-TU-n1 (IP: 10.128.15.231)
2025-06-02 14:37:10,651 - INFO - Running: /opt/yugabyte/software/active/yb-platform/yugaware/yba-cli/yba_cli-2024.2.2.2-b2-linux-amd64/yba universe node stop -H https://35.184.240.7/ -a 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb --name TU --node-name yb-dev-TU-n1

Waiting for node yb-dev-TU-n1 operation Stop to be completed
The node yb-dev-TU-n1 operation Stop has been completed
Node Name      Node UUID                              IP              State     Is Master process running   Is Tserver process running   Master state
yb-dev-TU-n1   ed729a1c-89d8-44cf-8031-110cddfdd47f   10.128.15.231   Stopped   false                       false                        -
2025-06-02 14:38:30,143 - INFO - âœ… Stopped node yb-dev-TU-n1 using YBA CLI
2025-06-02 14:38:30,144 - INFO - Running: gcloud compute instances stop yb-tu-dev-n5 --zone us-central1-c --project yuga-rjw
Stopping instance(s) yb-tu-dev-n5...done.                                                                                                                                                                                             
Updated [https://compute.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-c/instances/yb-tu-dev-n5].


Updates are available for some Google Cloud CLI components.  To install them,
please run:
  $ gcloud components update

2025-06-02 14:38:52,585 - INFO - Running: gcloud compute instances describe yb-tu-dev-n5 --zone us-central1-c --project yuga-rjw --format=json
2025-06-02 14:38:54,090 - INFO - Running: gcloud compute instances describe yb-tu-dev-n5 --zone us-central1-c --project yuga-rjw --format=json
2025-06-02 14:38:55,575 - INFO - Running: gcloud compute disks create yb-tu-dev-n5-boot-1748875135 --image rhel-9-v20240415 --image-project rhel-cloud --size 150GB --type pd-balanced --zone us-central1-c --project yuga-rjw
Created [https://www.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-c/disks/yb-tu-dev-n5-boot-1748875135].
WARNING: Some requests generated warnings:
 - Disk size: '150 GB' is larger than image size: '20 GB'. You might need to resize the root repartition manually if the operating system does not support automatic resizing. See https://cloud.google.com/compute/docs/disks/add-persistent-disk#resize_pd for details.
 - The resource 'projects/rhel-cloud/global/images/rhel-9-v20240415' is deprecated. A suggested replacement is 'projects/rhel-cloud/global/images/rhel-9-v20240515'.

NAME                          ZONE           SIZE_GB  TYPE         STATUS
yb-tu-dev-n5-boot-1748875135  us-central1-c  150      pd-balanced  READY
2025-06-02 14:39:20,002 - INFO - Running: gcloud compute instances detach-disk yb-tu-dev-n5 --disk yb-tu-dev-n5 --zone us-central1-c --project yuga-rjw
Updated [https://www.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-c/instances/yb-tu-dev-n5].
2025-06-02 14:39:23,522 - INFO - Running: gcloud compute instances attach-disk yb-tu-dev-n5 --disk yb-tu-dev-n5-boot-1748875135 --zone us-central1-c --project yuga-rjw --boot
Updated [https://www.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-c/instances/yb-tu-dev-n5].
2025-06-02 14:39:28,802 - INFO - Running: gcloud compute instances start yb-tu-dev-n5 --zone us-central1-c --project yuga-rjw
Starting instance(s) yb-tu-dev-n5...done.                                                                                                                                                                                             
Updated [https://compute.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-c/instances/yb-tu-dev-n5].
Instance internal IP is 10.128.15.231
Instance external IP is 35.224.24.113
2025-06-02 14:39:40,351 - INFO - Running: gcloud compute instances describe yb-tu-dev-n5 --zone us-central1-c --project yuga-rjw --format=json
2025-06-02 14:40:11,859 - INFO - Running: gcloud compute ssh weiwang@yb-tu-dev-n5 --zone us-central1-c --project yuga-rjw --internal-ip --command lsblk -o NAME,FSTYPE,UUID | grep xfs | grep -v sda
2025-06-02 14:40:14,351 - INFO - Running: gcloud compute ssh weiwang@yb-tu-dev-n5 --zone us-central1-c --project yuga-rjw --internal-ip --command sudo mkdir -p /data && sudo mount UUID=4f81302d-6eb0-496b-b20e-add3bd60fe47 /data && sudo chmod 777 /data && grep -q UUID=4f81302d-6eb0-496b-b20e-add3bd60fe47 /etc/fstab || echo 'UUID=4f81302d-6eb0-496b-b20e-add3bd60fe47 /data xfs defaults,nofail 0 2' | sudo tee -a /etc/fstab
UUID=4f81302d-6eb0-496b-b20e-add3bd60fe47 /data xfs defaults,nofail 0 2
2025-06-02 14:40:16,673 - INFO - Running: gcloud compute ssh weiwang@yb-tu-dev-n5 --zone us-central1-c --project yuga-rjw --internal-ip --command echo 'ðŸ›  Reprovisioning yugabyte user and running node-agent-provision.sh...' && sudo pkill -u yugabyte || true && id yugabyte && sudo userdel -r yugabyte || true && getent group yugabyte && (getent passwd | awk -F: '$4 == "$(getent group yugabyte | cut -d: -f3)"' | grep -q . || sudo groupdel yugabyte) || true && sudo useradd -m -d /data/home/yugabyte -s /bin/bash yugabyte && sudo mkdir -p /data/home/yugabyte && sudo chown -R yugabyte:yugabyte /data/home/yugabyte && sudo chmod 755 /data/home/yugabyte && cd /data/2024.2.2.2-b2/scripts && sudo ./node-agent-provision.sh
ðŸ›  Reprovisioning yugabyte user and running node-agent-provision.sh...
uid=1015(yugabyte) gid=1016(yugabyte) groups=1016(yugabyte),4(adm),39(video),1000(google-sudoers)
useradd: warning: the home directory /data/home/yugabyte already exists.
useradd: Not copying any file from skel directory into it.
[2025-06-02_14_40_19 common.sh:626 activate_pex] Using pex virtualenv python executable now.
2025-06-02 14:40:19,297 - root - INFO - Logging Setup Done
2025-06-02 14:40:19,298 - __main__ - DEBUG - YNP config {'logging': {'directory': './logs', 'file': 'app.log', 'level': 'DEBUG'},
 'yba': {'api_key': '3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb',
         'customer_uuid': 'cf6d0b42-bae0-49f2-8fa1-28a3f9216a77',
         'instance_type': {'cores': 4,
                           'memory_size': 16,
                           'mount_points': ['/data'],
                           'name': 'n2',
                           'volume_size': 100},
         'node_external_fqdn': '10.128.15.231',
         'node_name': 'yb-dev-tu-n1',
         'provider': {'name': 'onPrem',
                      'region': {'name': 'us-central1',
                                 'zone': {'name': 'us-central1-a'}}},
         'url': 'https://35.184.240.7'},
 'ynp': {'chrony_servers': ['0.pool.ntp.org', '1.pool.ntp.org'],
         'is_airgap': False,
         'is_install_node_agent': True,
         'node_ip': '10.128.15.231',
         'tmp_directory': '/tmp',
         'use_system_level_systemd': False,
         'yb_home_dir': '/data/home/yugabyte',
         'yb_user_id': 1994}}
2025-06-02 14:40:19,298 - __main__ - INFO - Config here: {'Preprovision': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875219', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp'}, 'ConfigureChrony': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875219', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'chrony_servers': '"0.pool.ntp.org, 1.pool.ntp.org"'}, 'CreateYugabyteUser': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875219', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'yb_user_id': '1994', 'yb_user_password': '', 'mount_points': '/data'}, 'ConfigureSystemd': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875219', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'user_name': 'yugabyte', 'use_system_level_systemd': 'False', 'service_files': '"yb-tserver.service, yb-master.service, clock-sync.sh.j2, yb-bind_check.service, yb-clean_cores.service, yb-clean_cores.timer, yb-collect_metrics.service, yb-collect_metrics.timer, yb-controller.service, yb-zip_purge_yb_logs.service, yb-zip_purge_yb_logs.timer"'}, 'ConfigureOs': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875219', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'message': '"Configure limits and sysctl parameters"', 'fd_limit': '1048576', 'nproc_limit': '12000', 'vm_swappiness': '0', 'kernel_core_pattern': '/data/home/yugabyte/cores/core_%p_%t_%E', 'vm_max_map_count': '262144', 'mount_points': '/data', 'limits': {'core': 'unlimited', 'data': 'unlimited', 'fsize': 'unlimited', 'sigpending': '119934', 'memlock': '64', 'rss': 'unlimited', 'nofile': '1048576', 'msgqueue': '819200', 'stack': '8192', 'cpu': 'unlimited', 'nproc': '12000', 'locks': 'unlimited'}}, 'ConfigureOs.limits': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875219', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'core': 'unlimited', 'data': 'unlimited', 'fsize': 'unlimited', 'sigpending': '119934', 'memlock': '64', 'rss': 'unlimited', 'nofile': '1048576', 'msgqueue': '819200', 'stack': '8192', 'cpu': 'unlimited', 'nproc': '12000', 'locks': 'unlimited'}, 'ConfigureNodeExporter': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875219', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'prometheus_user': 'prometheus'}, 'ConfigureNetwork': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875219', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'ip_address': '10.128.15.231', 'ports': '7000 7100 9000 9100 18018 22 5433 9042 9070 9300 12000 13000'}, 'InstallNodeAgent': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875219', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'url': 'https://35.184.240.7', 'customer_uuid': 'cf6d0b42-bae0-49f2-8fa1-28a3f9216a77', 'api_key': '3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb', 'node_name': 'yb-dev-tu-n1', 'node_external_fqdn': '10.128.15.231', 'provider_name': 'onPrem', 'provider_region_name': 'us-central1', 'provider_region_zone_name': 'us-central1-a', 'instance_type_name': 'n2', 'instance_type_cores': '4', 'instance_type_memory_size': '16', 'instance_type_volume_size': '100', 'instance_type_mount_points': "['/data']", 'node_ip': '10.128.15.231', 'bind_ip': '10.128.15.231'}, 'RebootNode': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875219', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp'}, 'DEFAULT': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875219', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp'}}
2025-06-02 14:40:19,298 - __main__ - INFO - Python Version: 3.9.18 (main, Jan  4 2024, 00:00:00) 
[GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]
2025-06-02 14:40:19,304 - __main__ - INFO - Jinja2 -- 3.0.3
2025-06-02 14:40:19,305 - __main__ - INFO - MarkupSafe -- 3.0.2
2025-06-02 14:40:19,308 - __main__ - INFO - ansible-vault -- 2.1.0
2025-06-02 14:40:19,310 - __main__ - INFO - setuptools -- 78.1.0
2025-06-02 14:40:19,312 - __main__ - INFO - ansible -- 2.9.27
2025-06-02 14:40:19,317 - __main__ - INFO - PyYAML -- 6.0.2
2025-06-02 14:40:19,320 - __main__ - INFO - cryptography -- 44.0.2
2025-06-02 14:40:19,324 - __main__ - INFO - cffi -- 1.17.1
2025-06-02 14:40:19,326 - __main__ - INFO - pycparser -- 2.22
2025-06-02 14:40:19,328 - __main__ - INFO - distro -- 1.5.0
2025-06-02 14:40:19,328 - __main__ - INFO - mitogen -- 0.2.9
2025-06-02 14:40:19,329 - __main__ - INFO - requests -- 2.32.3
2025-06-02 14:40:19,332 - __main__ - INFO - charset-normalizer -- 3.4.1
2025-06-02 14:40:19,334 - __main__ - INFO - idna -- 3.10
2025-06-02 14:40:19,335 - __main__ - INFO - urllib3 -- 2.3.0
2025-06-02 14:40:19,336 - __main__ - INFO - certifi -- 2025.1.31
2025-06-02 14:40:19,338 - __main__ - INFO - semver -- 3.0.2
2025-06-02 14:40:19,339 - __main__ - INFO - six -- 1.10.0
2025-06-02 14:40:19,339 - __main__ - INFO - Jinja2 -- 3.0.3
2025-06-02 14:40:19,339 - __main__ - INFO - MarkupSafe -- 3.0.2
2025-06-02 14:40:19,340 - __main__ - INFO - ansible-vault -- 2.1.0
2025-06-02 14:40:19,340 - __main__ - INFO - setuptools -- 78.1.0
2025-06-02 14:40:19,341 - __main__ - INFO - ansible -- 2.9.27
2025-06-02 14:40:19,341 - __main__ - INFO - PyYAML -- 6.0.2
2025-06-02 14:40:19,342 - __main__ - INFO - cryptography -- 44.0.2
2025-06-02 14:40:19,342 - __main__ - INFO - cffi -- 1.17.1
2025-06-02 14:40:19,343 - __main__ - INFO - pycparser -- 2.22
2025-06-02 14:40:19,343 - __main__ - INFO - distro -- 1.5.0
2025-06-02 14:40:19,343 - __main__ - INFO - mitogen -- 0.2.9
2025-06-02 14:40:19,344 - __main__ - INFO - requests -- 2.32.3
2025-06-02 14:40:19,345 - __main__ - INFO - charset-normalizer -- 3.4.1
2025-06-02 14:40:19,346 - __main__ - INFO - idna -- 3.10
2025-06-02 14:40:19,346 - __main__ - INFO - urllib3 -- 2.3.0
2025-06-02 14:40:19,347 - __main__ - INFO - certifi -- 2025.1.31
2025-06-02 14:40:19,347 - __main__ - INFO - semver -- 3.0.2
2025-06-02 14:40:19,347 - __main__ - INFO - six -- 1.10.0
2025-06-02 14:40:19,465 - commands.provision_command - INFO - {'ConfigureChrony': (<class 'modules.provision.chrony.chrony.ConfigureChrony'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/chrony/chrony.py'), 'ConfigureOs': (<class 'modules.provision.configure_os.os_config.ConfigureOs'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/configure_os/os_config.py'), 'ConfigureNetwork': (<class 'modules.provision.network.network.ConfigureNetwork'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/network/network.py'), 'InstallNodeAgent': (<class 'modules.provision.node_agent.node_agent.InstallNodeAgent'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/node_agent/node_agent.py'), 'ConfigureNodeExporter': (<class 'modules.provision.node_exporter.node_exporter.ConfigureNodeExporter'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/node_exporter/node_exporter.py'), 'RebootNode': (<class 'modules.provision.reboot_node.reboot_node.RebootNode'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/reboot_node/reboot_node.py'), 'ConfigureSystemd': (<class 'modules.provision.systemd.systemd.ConfigureSystemd'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/systemd/systemd.py'), 'Preprovision': (<class 'modules.provision.update_os.update.Preprovision'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/update_os/update.py'), 'CreateYugabyteUser': (<class 'modules.provision.yugabyte.yugabyte.CreateYugabyteUser'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/yugabyte/yugabyte.py')}
2025-06-02 14:40:19,465 - commands.provision_command - INFO - initialized
2025-06-02 14:40:19,526 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): 35.184.240.7:443
/data/2024.2.2.2-b2/devops/pexvenv/lib64/python3.9/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '35.184.240.7'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings
  warnings.warn(
2025-06-02 14:40:19,851 - urllib3.connectionpool - DEBUG - https://35.184.240.7:443 "GET /api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers?name=onPrem HTTP/1.1" 200 979
2025-06-02 14:40:19,852 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): 35.184.240.7:443
/data/2024.2.2.2-b2/devops/pexvenv/lib64/python3.9/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '35.184.240.7'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings
  warnings.warn(
2025-06-02 14:40:20,156 - urllib3.connectionpool - DEBUG - https://35.184.240.7:443 "GET /api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/instance_types/n2 HTTP/1.1" 200 255
2025-06-02 14:40:20,168 - commands.provision_command - INFO - /tmp/tmpfjuv5ns7
2025-06-02 14:40:20,168 - commands.provision_command - INFO - /tmp/tmpwm86wo2u
2025-06-02 14:40:30,648 - commands.provision_command - INFO - Output: 200 OK
User yugabyte already exists
User yugabyte added to systemd-journal group
el8 not detected, skipping changing selinux context
*                -       core            unlimited
*                -       data            unlimited
*                -       fsize            unlimited
*                -       sigpending            119934
*                -       memlock            64
*                -       rss            unlimited
*                -       nofile            1048576
*                -       msgqueue            819200
*                -       stack            8192
*                -       cpu            unlimited
*                -       nproc            12000
*                -       locks            unlimited
File /etc/security/limits.d/20-nproc.conf created.
el8 not detected, not updating file limits
Systemd file limits configured.
vm.swappiness = 0
kernel.core_pattern = /data/home/yugabyte/cores/core_%p_%t_%E
vm.max_map_count = 262144
Kernel settings configured.
OS Configuration applied successfully.
Node exporter setup is complete.
Start network configuration
Nothing to be done
End network configuration
Using node agent port 9070.
* Starting YB Node Agent install.
* Downloading YB Node Agent build package.
* Getting Linux/amd64 package
* Creating Node Agent Directory.
* Changing directory to node agent.
* Creating Sub Directories.
* Downloaded Version - 2024.2.2.2-b2
* Extracting the build package
   â€¢ Completed Node Agent Configuration
   â€¢ Checking for existing Node Agent with IP 10.128.15.231
   â€¢ Node Agent is already registered with IP 10.128.15.231
   â€¢ Node Agent Configuration Successful
Source ~/.bashrc to make node-agent available in the PATH.
You can install a systemd service on linux machines by running node-agent-installer.sh -c install_service --user yugabyte (Requires sudo access).
Using node agent port 9070.
* Starting YB Node Agent install_service.
success
* Installing Node Agent Systemd Service
  [Unit]
  Description=YB Anywhere Node Agent
  After=network-online.target
  # Disable restart limits, using RestartSec to rate limit restarts.
  StartLimitInterval=0

  [Service]
  User=yugabyte
  WorkingDirectory=/data/home/yugabyte/node-agent
  LimitCORE=infinity
  LimitNOFILE=1048576
  LimitNPROC=12000
  ExecStart=/data/home/yugabyte/node-agent/pkg/bin/node-agent server start
  Restart=always
  RestartSec=2

  [Install]
  WantedBy=multi-user.target
* Starting the systemd service
* Started the systemd service
* Run 'systemctl status yb-node-agent' to check the status of the yb-node-agent
* Run 'sudo systemctl stop yb-node-agent' to stop the yb-node-agent service
HTTP GET request successful. Processing response...
rpm command exists, testing operating system version
Operating system is NOT based on el7.
This script is intended for CentOS 7 derivatives only.
{
"results":[

]}
Pre-flight checks successful

2025-06-02 14:40:30,648 - commands.provision_command - INFO - Error: + json_results='{
"results":[
'
+ SUDO_ACCESS=false
++ id -u
+ '[' 0 = 0 ']'
+ SUDO_ACCESS=true
+ [[ OSFamily.REDHAT == suse ]]
+ [[ rhel == ubuntu ]]
+ [[ OSFamily.REDHAT == almaLinux ]]
+ [[ rhel == RedHat ]]
+ [[ rhel == RedHat ]]
+ [[ -n '' ]]
+ [[ -n '' ]]
+ [[ -n '' ]]
+ [[ rhel == ubuntu ]]
+ [[ OSFamily.REDHAT == RedHat ]]
+ [[ OSFamily.REDHAT == RedHat ]]
+ [[ rhel == oraclelinux ]]
+ [[ rhel == oraclelinux ]]
+ '[' -n '' ']'
+ '[' -n '' ']'
+ chronyc makestep
+ id yugabyte
+ echo 'User yugabyte already exists'
+ '[' -n '' ']'
+ groups yugabyte
+ grep -q '\bsystemd-journal\b'
+ usermod -aG systemd-journal yugabyte
+ echo 'User yugabyte added to systemd-journal group'
++ grep -oP '(?<=^PLATFORM_ID=).+' /etc/os-release
++ tr -d '"'
+ platform_id=platform:el9
+ [[ platform:el9 == \p\l\a\t\f\o\r\m\:\e\l\8 ]]
+ echo 'el8 not detected, skipping changing selinux context'
+ installer_dir=/data/2024.2.2.2-b2/scripts/ynp/../../bin
+ mkdir -p /data/home/yugabyte/.install
+ cp /data/2024.2.2.2-b2/scripts/ynp/../../bin/node-agent-installer.sh /data/home/yugabyte/.install
+ chown -R yugabyte:yugabyte /data/home/yugabyte/.install
+ chmod 750 /data/home/yugabyte
+ systemd_dir=/etc/systemd/system
+ loginctl enable-linger yugabyte
++ id -u yugabyte
+ su - yugabyte -c 'export XDG_RUNTIME_DIR=/run/user/1019'
++ id -u yugabyte
+ echo 'export XDG_RUNTIME_DIR=/run/user/1019'
+ systemd_dir=/data/home/yugabyte/.config/systemd/user
+ su - yugabyte -c 'mkdir -p /data/home/yugabyte/.config/systemd/user'
++ mktemp
+ tmp_file=/tmp/tmp.LchHMD1JYb
+ echo '[Unit]
Description=Yugabyte tserver service
Requires=network-online.target
After=network.target network-online.target multi-user.target
# Disable restart limits, using RestartSec to rate limit restarts
StartLimitInterval=0
Wants=yb-ysql-cgroup.service

[Path]
PathExists=/data/home/yugabyte/tserver/bin/yb-tserver
PathExists=/data/home/yugabyte/tserver/conf/server.conf

[Service]
# Start


ExecStartPre=/data/home/yugabyte/bin/clock-sync.sh
ExecStart=/data/home/yugabyte/tserver/bin/yb-tserver --flagfile /data/home/yugabyte/tserver/conf/server.conf
Restart=always
RestartSec=5
# Stop -> SIGTERM - 10s - SIGKILL (if not stopped) [matches existing cron behavior]
KillMode=process
TimeoutStopFailureMode=terminate
KillSignal=SIGTERM
TimeoutStopSec=10
FinalKillSignal=SIGKILL
# Logs
StandardOutput=syslog
StandardError=syslog
# ulimit
LimitCORE=infinity
LimitNOFILE=1048576
LimitNPROC=12000
# Allow tserver to move postgres into its own cgroup
Delegate=true

[Install]
WantedBy=default.target'
+ chown yugabyte:yugabyte /tmp/tmp.LchHMD1JYb
+ su - yugabyte -c 'mv /tmp/tmp.LchHMD1JYb /data/home/yugabyte/.config/systemd/user/yb-tserver.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-tserver.service'
++ mktemp
+ tmp_file=/tmp/tmp.w9cmyzxuCq
+ echo '[Unit]
Description=Yugabyte master service
Requires=network-online.target
After=network.target network-online.target multi-user.target
# Disable restart limits, using RestartSec to rate limit restarts
StartLimitInterval=0

[Path]
PathExists=/data/home/yugabyte/master/bin/yb-master
PathExists=/data/home/yugabyte/master/conf/server.conf

[Service]
# Start


ExecStartPre=/data/home/yugabyte/bin/clock-sync.sh
ExecStart=/data/home/yugabyte/master/bin/yb-master --flagfile /data/home/yugabyte/master/conf/server.conf
Restart=always
RestartSec=5
# Stop -> SIGTERM - 10s - SIGKILL (if not stopped) [matches existing cron behavior]
KillMode=process
TimeoutStopFailureMode=terminate
KillSignal=SIGTERM
TimeoutStopSec=10
FinalKillSignal=SIGKILL
# Logs
StandardOutput=syslog
StandardError=syslog
# ulimit
LimitCORE=infinity
LimitNOFILE=1048576
LimitNPROC=12000

[Install]
WantedBy=default.target'
+ chown yugabyte:yugabyte /tmp/tmp.w9cmyzxuCq
+ su - yugabyte -c 'mv /tmp/tmp.w9cmyzxuCq /data/home/yugabyte/.config/systemd/user/yb-master.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-master.service'
++ mktemp
+ tmp_file=/tmp/tmp.OKdiA7lfxX
++ basename /tmp/tmpwm86wo2u
++ chronyc tracking
++ chronyc tracking
++ echo ''
++ awk '/Reference ID/ {print $4}'
++ echo ''
++ awk '/System time/ {print $4}'
++ echo ''
++ awk '/Root dispersion/ {print $4}'
++ echo ''
++ awk '/Root delay/ {print $4}'
++ python -c 'print( +  + (.5 * ))'
  File "<string>", line 1
    print( +  + (.5 * ))
                      ^
SyntaxError: invalid syntax
++ ntpq -c 'rv 0 clock'
++ awk '-F[=,]' '/offset/ {print $3}'
/tmp/tmpwm86wo2u: line 339: ntpq: command not found
++ ntpq -p
/tmp/tmpwm86wo2u: line 339: ntpq: command not found
++ awk '$1 ~ "^*" {print $9}'
++ python -c 'print( * 1000)'
Traceback (most recent call last):
  File "<string>", line 1, in <module>
TypeError: print() argument after * must be an iterable, not int
++ timedatectl status
++ grep 'System clock synchronized'
++ awk '{print $4}'
++ systemctl show --no-pager
++ grep ActiveState
++ cut -d= -f2
+ echo '#!/bin/bash

SCRIPT_NAME=tmpwm86wo2u

################### Config ###################
is_acceptable_clock_skew_wait_enabled=True  # Whether check clock skew
acceptable_clock_skew_sec=0.5  # In seconds
max_tries=120  # Maximum number of tries before returning failure
retry_wait_time_s=1  # How long waits before retry in seconds

if [[  != true &&  != True ]]; then
  echo Wait' for clock skew to go below the acceptable threshold is disabled. Returning 'success.
  exit 0
fi

command_exists() {
  command -v  >/dev/null 2>&1
}

readonly PYTHON_EXECUTABLES=('\''python'\'' '\''python3'\'' '\''python3.11'\'' '\''python3.10'\'' '\''python3.9'\'' '\''python3.8'\'' '\''python3.7'\'' '\''python3.6'\'' '\''python3.12'\'' '\''python2'\'')
PYTHON_EXECUTABLE=
set_python_executable() {
  for py_executable in ; do
    if which  > /dev/null 2>&1; then
      PYTHON_EXECUTABLE=
      export PYTHON_EXECUTABLE
      return
    fi
  done
}

check_clock_sync_chrony() {
  # if chrond is restarted, tracking will return all 0'\''s
  set_python_executable
  chrony_tracking=Reference' ID : A9FEA9FE '(metadata.google.internal)' Stratum : 3 Ref time '(UTC)' : Mon Jun 02 14:40:06 2025 System time : 0.000000000 seconds slow of NTP time Last offset : -0.000005786 seconds RMS offset : 0.000005786 seconds Frequency : 5.010 ppm slow Residual freq : -0.001 ppm Skew : 47.114 ppm Root delay : 0.000106680 seconds Root dispersion : 0.000730765 seconds Update interval : 2.0 seconds Leap status : 'Normal
  if [[ 0 -ne 0 ]]; then 
    echo Reference' ID : A9FEA9FE '(metadata.google.internal)' Stratum : 3 Ref time '(UTC)' : Mon Jun 02 14:40:06 2025 System time : 0.000000000 seconds slow of NTP time Last offset : -0.000005786 seconds RMS offset : 0.000005786 seconds Frequency : 5.010 ppm slow Residual freq : -0.001 ppm Skew : 47.114 ppm Root delay : 0.000106680 seconds Root dispersion : 0.000730915 seconds Update interval : 2.0 seconds Leap status : Normal failed to 'execute
    return 1
  fi
  if [[  == 00000000 ]]; then
    echo chrony' is not 'initialized
    return 1
  fi
  local skew=
  local dispersion=
  local delay=
  local clock_error=
  if [[ -z python ]]; then
    clock_error=
  else
   clock_error=
  fi
  
  if awk '\''BEGIN{exit !('\'''\'' < '\'''\'')}'\''; then
    echo Clock' skew is within acceptable limits: 'ms
    return 0
  else
    echo Clock' skew exceeds acceptable limits: 'ms
    return 1
  fi
}

check_clock_sync_ntpd() {
  #local skew=
  local skew=
  local acceptable_skew_ms=

  if [[ -z  ]]; then
    echo ntpd' is not 'initialized
    return 1
  fi
  
  if awk '\''BEGIN{exit !('\'''\'' < '\'''\'')}'\''; then
    echo Clock' skew is within acceptable limits: 'ms
    return 0
  else
    echo Clock' skew exceeds acceptable limits: 'ms
    return 1
  fi
}

check_clock_sync_timesyncd() {
  synchronized=no
  if [[  == yes ]]; then
    echo timesyncd' reports clock is 'synchronized
    return 0
  else
    echo timesyncd' clock is not 'synchronized
    return 1
  fi
}

systemd_loaded() {
  active=
  if [[  == active ]]; then
    return 0
  fi
  return 1
}

iter=0
while true; do
  # If chrony is available, use it for clock sync.
  if command_exists chronyc; then
    check_clock_sync_chrony
    res=0
  # If ntpd is available, use it for clock sync.
  elif command_exists ntpd; then
    check_clock_sync_ntpd
    res=0
  elif systemd_loaded systemd-timesyncd; then 
    check_clock_sync_timesyncd
    res=0
  else
    echo Chrony,' NTPd, and timesyncd are not available, but 'required.
    exit 1
  fi
  ((iter++))
  if [  -eq 0 ]; then
    echo Success!' Clock skew is within acceptable 'limits.
    exit 0
  fi
  if [  -ge  ]; then
    echo Failure!' Maximum number of tries 'reached.
    exit 1
  fi
  sleep 
done'
+ chown yugabyte:yugabyte /tmp/tmp.OKdiA7lfxX
+ su - yugabyte -c 'mv /tmp/tmp.OKdiA7lfxX /data/home/yugabyte/.config/systemd/user/clock-sync.sh.j2'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/clock-sync.sh.j2'
++ mktemp
+ tmp_file=/tmp/tmp.io2pCfoK8i
+ echo '[Unit]
Description=Yugabyte IP Bind Check
Requires=network-online.target
After=network.target network-online.target multi-user.target
Before=yb-controller.service yb-tserver.service yb-master.service
# Disable restart limits, using RestartSec to rate limit restarts
StartLimitInterval=0

[Path]
PathExists=/data/home/yugabyte/controller/bin/yb-controller-server
PathExists=/data/home/yugabyte/controller/conf/server.conf

[Service]
# Start
ExecStart=/data/home/yugabyte/controller/bin/yb-controller-server     --flagfile /data/home/yugabyte/controller/conf/server.conf     --only_bind --logtostderr
Type=oneshot
KillMode=control-group
KillSignal=SIGTERM
TimeoutStopSec=10
# Logs
StandardOutput=syslog
StandardError=syslog

[Install]
WantedBy=default.target'
+ chown yugabyte:yugabyte /tmp/tmp.io2pCfoK8i
+ su - yugabyte -c 'mv /tmp/tmp.io2pCfoK8i /data/home/yugabyte/.config/systemd/user/yb-bind_check.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-bind_check.service'
++ mktemp
+ tmp_file=/tmp/tmp.8xlVNfKU67
+ echo '[Unit]
Description=Yugabyte clean cores

[Service]


Type=oneshot
WorkingDirectory=/data/home/yugabyte/bin
ExecStart=/bin/bash /data/home/yugabyte/bin/clean_cores.sh

[Install]
WantedBy=multi-user.target'
+ chown yugabyte:yugabyte /tmp/tmp.8xlVNfKU67
+ su - yugabyte -c 'mv /tmp/tmp.8xlVNfKU67 /data/home/yugabyte/.config/systemd/user/yb-clean_cores.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-clean_cores.service'
++ mktemp
+ tmp_file=/tmp/tmp.IqouSanWcM
+ echo '[Unit]
Description=Yugabyte clean cores

[Timer]


Unit=yb-clean_cores.service
# Run every 5 minutes
OnCalendar=*:0/5

[Install]
WantedBy=timers.target'
+ chown yugabyte:yugabyte /tmp/tmp.IqouSanWcM
+ su - yugabyte -c 'mv /tmp/tmp.IqouSanWcM /data/home/yugabyte/.config/systemd/user/yb-clean_cores.timer'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-clean_cores.timer'
++ mktemp
+ tmp_file=/tmp/tmp.aI1C9xFVvI
+ echo '[Unit]
Description=Yugabyte collect metrics

[Service]


Type=oneshot
WorkingDirectory=/data/home/yugabyte/bin
ExecStart=/bin/bash /data/home/yugabyte/bin/collect_metrics_wrapper.sh

[Install]'
+ chown yugabyte:yugabyte /tmp/tmp.aI1C9xFVvI
+ su - yugabyte -c 'mv /tmp/tmp.aI1C9xFVvI /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.service'
++ mktemp
+ tmp_file=/tmp/tmp.EMDmE0hAtq
+ echo '[Unit]
Description=Yugabyte collect metrics

[Timer]


Unit=yb-collect_metrics.service
# Run every 1 minute
OnCalendar=*:0/1:0

[Install]
WantedBy=timers.target'
+ chown yugabyte:yugabyte /tmp/tmp.EMDmE0hAtq
+ su - yugabyte -c 'mv /tmp/tmp.EMDmE0hAtq /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.timer'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.timer'
++ mktemp
+ tmp_file=/tmp/tmp.9RSojZVgkH
+ echo '[Unit]
Description=Yugabyte Controller
Requires=network-online.target
After=network.target network-online.target multi-user.target
# Disable restart limits, using RestartSec to rate limit restarts
StartLimitInterval=0

[Path]
PathExists=/data/home/yugabyte/controller/bin/yb-controller-server
PathExists=/data/home/yugabyte/controller/conf/server.conf

[Service]
# Start


ExecStart=/data/home/yugabyte/controller/bin/yb-controller-server     --flagfile /data/home/yugabyte/controller/conf/server.conf
Restart=always
RestartSec=5
# Stop -> SIGTERM - 10s - SIGKILL (if not stopped) [matches existing cron behavior]
KillMode=control-group
TimeoutStopFailureMode=terminate
KillSignal=SIGTERM
TimeoutStopSec=10
FinalKillSignal=SIGKILL
# Logs
StandardOutput=syslog
StandardError=syslog
# ulimit
LimitCORE=infinity
LimitNOFILE=1048576
LimitNPROC=12000

[Install]
WantedBy=default.target'
+ chown yugabyte:yugabyte /tmp/tmp.9RSojZVgkH
+ su - yugabyte -c 'mv /tmp/tmp.9RSojZVgkH /data/home/yugabyte/.config/systemd/user/yb-controller.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-controller.service'
++ mktemp
+ tmp_file=/tmp/tmp.8YGt9x727v
+ echo '[Unit]
Description=Yugabyte logs

[Service]


Type=oneshot
WorkingDirectory=/data/home/yugabyte/bin
ExecStart=/bin/bash /data/home/yugabyte/bin/zip_purge_yb_logs.sh

[Install]
WantedBy=multi-user.target'
+ chown yugabyte:yugabyte /tmp/tmp.8YGt9x727v
+ su - yugabyte -c 'mv /tmp/tmp.8YGt9x727v /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.service'
++ mktemp
+ tmp_file=/tmp/tmp.CMcaFYyafU
+ echo '[Unit]
Description=Yugabyte logs

[Timer]


Unit=yb-zip_purge_yb_logs.service
# Run every 5 minutes
OnCalendar=*:0/5

[Install]
WantedBy=timers.target'
+ chown yugabyte:yugabyte /tmp/tmp.CMcaFYyafU
+ su - yugabyte -c 'mv /tmp/tmp.CMcaFYyafU /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.timer'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.timer'
+ grep -qP '^\*\s+-\s+core\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       core            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+data\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       data            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+fsize\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       fsize            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+sigpending\s+119934$' /etc/security/limits.conf
+ echo '*                -       sigpending            119934'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+memlock\s+64$' /etc/security/limits.conf
+ echo '*                -       memlock            64'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+rss\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       rss            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+nofile\s+1048576$' /etc/security/limits.conf
+ echo '*                -       nofile            1048576'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+msgqueue\s+819200$' /etc/security/limits.conf
+ echo '*                -       msgqueue            819200'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+stack\s+8192$' /etc/security/limits.conf
+ echo '*                -       stack            8192'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+cpu\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       cpu            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+nproc\s+12000$' /etc/security/limits.conf
+ echo '*                -       nproc            12000'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+locks\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       locks            unlimited'
+ tee -a /etc/security/limits.conf
+ test '!' -f /etc/security/limits.d/20-nproc.conf
+ touch /etc/security/limits.d/20-nproc.conf
+ echo 'File /etc/security/limits.d/20-nproc.conf created.'
+ sed -i 's/*          soft    nproc     4096/*          soft    nproc     \
12000/' /etc/security/limits.d/20-nproc.conf
++ grep -oP '(?<=^PLATFORM_ID=).+' /etc/os-release
++ tr -d '"'
+ platform_id=platform:el9
+ [[ platform:el9 == \p\l\a\t\f\o\r\m\:\e\l\8 ]]
+ echo 'el8 not detected, not updating file limits'
+ echo 'Systemd file limits configured.'
+ systemctl daemon-reload
+ ulimit -n 1048576
+ ulimit -u 12000
+ grep -q '^vm.swappiness=' /etc/sysctl.conf
+ echo vm.swappiness=0
+ sysctl -w vm.swappiness=0
+ grep -q '^kernel.core_pattern=' /etc/sysctl.conf
+ echo kernel.core_pattern=/data/home/yugabyte/cores/core_%p_%t_%E
+ sysctl -w kernel.core_pattern=/data/home/yugabyte/cores/core_%p_%t_%E
+ grep -q '^vm.max_map_count=' /etc/sysctl.conf
+ echo vm.max_map_count=262144
+ sysctl -w vm.max_map_count=262144
+ echo 'Kernel settings configured.'
+ echo 'OS Configuration applied successfully.'
+ thirdparty_dir=/data/2024.2.2.2-b2/scripts/ynp/../../thirdparty
+ mkdir -p /opt/prometheus
+ mkdir -p /etc/prometheus
+ mkdir -p /var/log/prometheus
+ mkdir -p /var/run/prometheus
+ mkdir -p /tmp/yugabyte/metrics
+ '[' -f /data/2024.2.2.2-b2/scripts/ynp/../../thirdparty/node_exporter-1.7.0.linux-amd64.tar.gz ']'
+ cp /data/2024.2.2.2-b2/scripts/ynp/../../thirdparty/node_exporter-1.7.0.linux-amd64.tar.gz /opt/prometheus
+ id -u prometheus
++ grep -oP '(?<=^ID=).+' /etc/os-release
++ tr -d '"'
+ os_type=rhel
+ case "$os_type" in
+ useradd --shell /bin/bash --no-create-home -g yugabyte prometheus
+ chown -R prometheus:yugabyte /opt/prometheus
+ chown -R prometheus:yugabyte /etc/prometheus
+ chown -R prometheus:yugabyte /var/log/prometheus
+ chown -R prometheus:yugabyte /var/run/prometheus
+ chown -R yugabyte:yugabyte /tmp/yugabyte/metrics
+ chmod -R 755 /tmp/yugabyte/metrics
+ '[' -f /opt/prometheus/node_exporter-1.7.0.linux-amd64.tar.gz ']'
+ chmod +r /opt/prometheus/node_exporter-1.7.0.linux-amd64.tar.gz
+ su - prometheus -c 'cd /opt/prometheus && tar zxf node_exporter-1.7.0.linux-amd64.tar.gz'
su: warning: cannot change directory to /home/prometheus: No such file or directory
+ file_path=/etc/systemd/system/node_exporter.service
+ cat
+ systemctl daemon-reload
+ systemctl enable node_exporter
Created symlink /etc/systemd/system/multi-user.target.wants/node_exporter.service â†’ /etc/systemd/system/node_exporter.service.
+ systemctl start node_exporter
+ echo 'Node exporter setup is complete.'
+ echo 'Start network configuration'
+ echo 'Nothing to be done'
+ echo 'End network configuration'
+ yba_url=https://35.184.240.7
+ customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
+ token=3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
+ provider_id=2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ skip_tls_verify https://35.184.240.7
++ local yba_url=https://35.184.240.7
++ [[ https://35.184.240.7 == https* ]]
++ echo --insecure
+ tls_verify_option=--insecure
+ readarray -t headers
++ _get_headers 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
++ local token=3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
++ echo 'Accept: application/json'
++ echo 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb'
++ echo 'Content-Type: application/json'
+ header_options=()
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ test -f /tmp/create_provider.json
+ test -f /tmp/update_provider.json
+ test -f /tmp/create_instance.json
+ airgap_flag=
+ installer_dir=/data/home/yugabyte/.install
+ su - yugabyte -c '"/data/home/yugabyte/.install/node-agent-installer.sh" -c install -u https://35.184.240.7 -t 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb --provider_id 2dc133e8-9482-4f77-ab4d-4f617ebbfa28 --instance_type n2 --zone_name us-central1-a --node_name yb-dev-tu-n1 --region_name us-central1 --node_ip 10.128.15.231 --bind_ip 10.128.15.231 --silent --skip_verify_cert '
+ /data/home/yugabyte/.install/node-agent-installer.sh -c install_service --user yugabyte
Created symlink /etc/systemd/system/multi-user.target.wants/yb-node-agent.service â†’ /etc/systemd/system/yb-node-agent.service.
+ test -f /tmp/add_node_to_provider.json
++ _get_nodes_in_provider https://35.184.240.7 cf6d0b42-bae0-49f2-8fa1-28a3f9216a77 2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ local yba_url=https://35.184.240.7
++ local customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
++ local provider_uuid=2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ echo https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
+ get_nodes_in_provider=https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
++ curl -s -w '%{http_code}' -o response.txt -X GET -H 'Accept: application/json' -H 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb' -H 'Content-Type: application/json' --insecure https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
+ response=200
+ http_status=200
+ response_body='[{"nodeUuid":"8dc3bcc1-fb03-42ec-9903-a07c3983e0d0","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n3","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.3","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n3"},"detailsJson":"{\"ip\":\"10.128.0.3\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n3\"}","inUse":true},{"nodeUuid":"716b9c3c-3743-41c7-b2b8-87d92f96bf83","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n2","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.2","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n2"},"detailsJson":"{\"ip\":\"10.128.0.2\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n2\"}","inUse":true},{"nodeUuid":"ed729a1c-89d8-44cf-8031-110cddfdd47f","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n1","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.15.231","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n1"},"detailsJson":"{\"ip\":\"10.128.15.231\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n1\"}","inUse":true}]'
+ matched=false
+ '[' 200 -ge 200 ']'
+ '[' 200 -lt 300 ']'
+ echo 'HTTP GET request successful. Processing response...'
++ echo '[{"nodeUuid":"8dc3bcc1-fb03-42ec-9903-a07c3983e0d0","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n3","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.3","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n3"},"detailsJson":"{\"ip\":\"10.128.0.3\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n3\"}","inUse":true},{"nodeUuid":"716b9c3c-3743-41c7-b2b8-87d92f96bf83","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n2","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.2","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n2"},"detailsJson":"{\"ip\":\"10.128.0.2\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n2\"}","inUse":true},{"nodeUuid":"ed729a1c-89d8-44cf-8031-110cddfdd47f","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n1","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.15.231","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n1"},"detailsJson":"{\"ip\":\"10.128.15.231\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n1\"}","inUse":true}]'
++ sed -n 's/.*"ip":"\([a-zA-Z0-9\.-]*\)".*/\1/p'
+ ips=10.128.15.231
+ for ip in $ips
+ [[ 10.128.15.231 == \1\0\.\1\2\8\.\1\5\.\2\3\1 ]]
+ matched=true
+ break
+ [[ true == false ]]
+ trap cleanup EXIT
+ check_os
+ command -v rpm
+ echo 'rpm command exists, testing operating system version'
++ rpm -E '%{rhel}'
+ rhel_version=9
+ [[ 9 == \7 ]]
+ echo 'Operating system is NOT based on el7.'
+ return 1
+ echo 'This script is intended for CentOS 7 derivatives only.'
+ print_results
+ any_fail=0
+ [[ {
"results":[
 == *\"\r\e\s\u\l\t\"\:\ \"\F\A\I\L\"* ]]
+ json_results+='
]}'
+ echo '{
"results":[

]}'
+ '[' 0 -eq 1 ']'
+ echo 'Pre-flight checks successful'
+ cleanup
+ test -f /tmp/create_provider.json
+ test -f /tmp/update_provider.json
+ test -f /tmp/create_instance.json
+ test -f /tmp/add_node_to_provider.json
+ rm /tmp/add_node_to_provider.json
+ rm -rf /data/home/yugabyte/.install

2025-06-02 14:40:30,648 - commands.provision_command - INFO - Return Code: 0
2025-06-02 14:41:13,173 - commands.provision_command - INFO - Output: 200 OK
System clock synchronized
User yugabyte exists
/data/home/yugabyte has the correct ownership and acceptable permissions
[PASS] Directory /data is owned by yugabyte or has '7' (rwx) permissions.
[PASS] Sufficient disk space available: 196G
Systemd unit yb-tserver.service is configured.
Systemd unit yb-master.service is configured.
Systemd unit clock-sync.sh.j2 is configured.
Systemd unit yb-bind_check.service is configured.
Systemd unit yb-clean_cores.service is configured.
Systemd unit yb-clean_cores.timer is configured.
Systemd unit yb-collect_metrics.service is configured.
Systemd unit yb-collect_metrics.timer is configured.
Systemd unit yb-controller.service is configured.
Systemd unit yb-zip_purge_yb_logs.service is configured.
Systemd unit yb-zip_purge_yb_logs.timer is configured.
[PASS] vm.swappiness is set to 0 (expected: 0)
[PASS] vm.max_map_count is set to 262144 (expected: 262144)
[PASS] kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E
"node_exporter.service" is active
Port 7000 is open on 10.128.15.231
Port 7100 is open on 10.128.15.231
Port 9000 is open on 10.128.15.231
Port 9100 is open on 10.128.15.231
Port 18018 is open on 10.128.15.231
Port 22 is open on 10.128.15.231
Port 5433 is open on 10.128.15.231
Port 9042 is open on 10.128.15.231
Port 9070 is open on 10.128.15.231
Port 9300 is open on 10.128.15.231
Port 12000 is open on 10.128.15.231
Port 13000 is open on 10.128.15.231
"yb-node-agent.service" is active
MemoryCurrent is greater than 0: 9854976
HTTP GET request successful. Processing response...
Node addition to the provider passed: 10.128.15.231
Marker file does not exist: /tmp/.reboot-required
Reboot requirement cleared.
{
"results":[
    {
      "check": "Clock Synchronization",
      "result": "PASS",
      "message": "System clock synchronized"
    },
    {
      "check": "Yugabyte User Existence Check",
      "result": "PASS",
      "message": "User yugabyte exists"
    },
    {
      "check": "Yugabyte Home Directory Permissions Check",
      "result": "PASS",
      "message": "/data/home/yugabyte has the correct ownership and acceptable permissions"
    },
    {
      "check": "/data Ownership/Permission Check",
      "result": "PASS",
      "message": "Directory /data is owned by yugabyte or has '7' (rwx) permissions."
    },
    {
      "check": "/data Free Space Check",
      "result": "PASS",
      "message": "Sufficient disk space available: 196G"
    },
    {
      "check": "Systemd Unit File Check - yb-tserver.service",
      "result": "PASS",
      "message": "Systemd unit yb-tserver.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-master.service",
      "result": "PASS",
      "message": "Systemd unit yb-master.service is configured."
    },
    {
      "check": "Systemd Unit File Check - clock-sync.sh.j2",
      "result": "PASS",
      "message": "Systemd unit clock-sync.sh.j2 is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-bind_check.service",
      "result": "PASS",
      "message": "Systemd unit yb-bind_check.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.service",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.timer",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.service",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.timer",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-controller.service",
      "result": "PASS",
      "message": "Systemd unit yb-controller.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.service",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.timer",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.timer is configured."
    },
    {
      "check": "vm.swappiness",
      "result": "PASS",
      "message": "vm.swappiness is set to 0 (expected: 0)"
    },
    {
      "check": "vm.max_map_count",
      "result": "PASS",
      "message": "vm.max_map_count is set to 262144 (expected: 262144)"
    },
    {
      "check": "kernel.core_pattern",
      "result": "PASS",
      "message": "kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "node_exporter.service is active"
    },
    {
      "check": "Port 7000 Check",
      "result": "PASS",
      "message": "Port 7000 is open on 10.128.15.231"
    },
    {
      "check": "Port 7100 Check",
      "result": "PASS",
      "message": "Port 7100 is open on 10.128.15.231"
    },
    {
      "check": "Port 9000 Check",
      "result": "PASS",
      "message": "Port 9000 is open on 10.128.15.231"
    },
    {
      "check": "Port 9100 Check",
      "result": "PASS",
      "message": "Port 9100 is open on 10.128.15.231"
    },
    {
      "check": "Port 18018 Check",
      "result": "PASS",
      "message": "Port 18018 is open on 10.128.15.231"
    },
    {
      "check": "Port 22 Check",
      "result": "PASS",
      "message": "Port 22 is open on 10.128.15.231"
    },
    {
      "check": "Port 5433 Check",
      "result": "PASS",
      "message": "Port 5433 is open on 10.128.15.231"
    },
    {
      "check": "Port 9042 Check",
      "result": "PASS",
      "message": "Port 9042 is open on 10.128.15.231"
    },
    {
      "check": "Port 9070 Check",
      "result": "PASS",
      "message": "Port 9070 is open on 10.128.15.231"
    },
    {
      "check": "Port 9300 Check",
      "result": "PASS",
      "message": "Port 9300 is open on 10.128.15.231"
    },
    {
      "check": "Port 12000 Check",
      "result": "PASS",
      "message": "Port 12000 is open on 10.128.15.231"
    },
    {
      "check": "Port 13000 Check",
      "result": "PASS",
      "message": "Port 13000 is open on 10.128.15.231"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "yb-node-agent.service is active"
    },
    {
      "check": "Memory Usage Check",
      "result": "PASS",
      "message": "MemoryCurrent is greater than 0: 9854976"
    },
    {
      "check": "Node addition failed",
      "result": "PASS",
      "message": "Node addition to the provider passed: 10.128.15.231"
    },
    {
      "check": "REBOOT REQUIRED",
      "result": "PASS",
      "message": "No Marker file present"
    }
]}
Pre-flight checks successful

2025-06-02 14:41:13,173 - commands.provision_command - INFO - Error: + json_results='{
"results":[
'
+ SUDO_ACCESS=false
++ id -u
+ '[' 0 = 0 ']'
+ SUDO_ACCESS=true
+ '[' true = true ']'
+ chronyc makestep
+ '[' 0 -eq 0 ']'
+ add_result 'Clock Synchronization' PASS 'System clock synchronized'
+ local 'check=Clock Synchronization'
+ local result=PASS
+ local 'message=System clock synchronized'
+ '[' 14 -gt 20 ']'
+ json_results+='    {
'
+ json_results+='      "check": "Clock Synchronization",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "System clock synchronized"
'
+ json_results+='    }'
+ echo 'System clock synchronized'
+ id yugabyte
+ echo 'User yugabyte exists'
+ add_result 'Yugabyte User Existence Check' PASS 'User yugabyte exists'
+ local 'check=Yugabyte User Existence Check'
+ local result=PASS
+ local 'message=User yugabyte exists'
+ '[' 134 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Yugabyte User Existence Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "User yugabyte exists"
'
+ json_results+='    }'
+ '[' -d /data/home/yugabyte ']'
++ stat -c %U /data/home/yugabyte
+ owner=yugabyte
++ stat -c %G /data/home/yugabyte
+ group=yugabyte
++ stat -c %a /data/home/yugabyte
+ permissions=750
+ '[' yugabyte '!=' yugabyte ']'
+ '[' yugabyte '!=' yugabyte ']'
+ '[' 750 -lt 711 ']'
+ echo '/data/home/yugabyte has the correct ownership and acceptable permissions'
+ add_result 'Yugabyte Home Directory Permissions Check' PASS '/data/home/yugabyte has the correct ownership and acceptable permissions'
+ local 'check=Yugabyte Home Directory Permissions Check'
+ local result=PASS
+ local 'message=/data/home/yugabyte has the correct ownership and acceptable permissions'
+ '[' 259 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Yugabyte Home Directory Permissions Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "/data/home/yugabyte has the correct ownership and acceptable permissions"
'
+ json_results+='    }'
+ yb_home_dir=/data/home/yugabyte
+ threshold=49
+ mount_points=/data
+ IFS=' '
+ read -ra mount_points_array
+ for mount_point in "${mount_points_array[@]}"
+ is_subdirectory /data /data/home/yugabyte
+ local dir=/data
+ local parent=/data/home/yugabyte
++ realpath -m /data
+ dir=/data
++ realpath -m /data/home/yugabyte
+ parent=/data/home/yugabyte
+ case "$dir/" in
+ return 1
+ '[' -d /data ']'
++ stat -c %U /data
+ owner_check=root
++ stat -c %A /data
++ cut -c 2-4
+ perm_check=rwx
+ '[' root == yugabyte ']'
+ [[ rwx == \r\w\x ]]
+ result=PASS
+ message='Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions.'
+ echo '[PASS] Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions.'
+ add_result '/data Ownership/Permission Check' PASS 'Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions.'
+ local 'check=/data Ownership/Permission Check'
+ local result=PASS
+ local 'message=Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions.'
+ '[' 448 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "/data Ownership/Permission Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions."
'
+ json_results+='    }'
+ '[' -d /data ']'
++ df -BG --output=avail /data
++ tail -n 1
++ tr -d 'G '
+ free_space_gb=196
+ '[' 196 -gt 49 ']'
+ result=PASS
+ message='Sufficient disk space available: 196G'
+ echo '[PASS] Sufficient disk space available: 196G'
+ add_result '/data Free Space Check' PASS 'Sufficient disk space available: 196G'
+ local 'check=/data Free Space Check'
+ local result=PASS
+ local 'message=Sufficient disk space available: 196G'
+ '[' 622 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "/data Free Space Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Sufficient disk space available: 196G"
'
+ json_results+='    }'
+ systemd_dir=/etc/systemd/system
+ systemd_dir=/data/home/yugabyte/.config/systemd/user
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-tserver.service ']'
+ echo 'Systemd unit yb-tserver.service is configured.'
+ add_result 'Systemd Unit File Check - yb-tserver.service' PASS 'Systemd unit yb-tserver.service is configured.'
+ local 'check=Systemd Unit File Check - yb-tserver.service'
+ local result=PASS
+ local 'message=Systemd unit yb-tserver.service is configured.'
+ '[' 757 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-tserver.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-tserver.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-master.service ']'
+ echo 'Systemd unit yb-master.service is configured.'
+ add_result 'Systemd Unit File Check - yb-master.service' PASS 'Systemd unit yb-master.service is configured.'
+ local 'check=Systemd Unit File Check - yb-master.service'
+ local result=PASS
+ local 'message=Systemd unit yb-master.service is configured.'
+ '[' 923 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-master.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-master.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/clock-sync.sh.j2 ']'
+ echo 'Systemd unit clock-sync.sh.j2 is configured.'
+ add_result 'Systemd Unit File Check - clock-sync.sh.j2' PASS 'Systemd unit clock-sync.sh.j2 is configured.'
+ local 'check=Systemd Unit File Check - clock-sync.sh.j2'
+ local result=PASS
+ local 'message=Systemd unit clock-sync.sh.j2 is configured.'
+ '[' 1087 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - clock-sync.sh.j2",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit clock-sync.sh.j2 is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-bind_check.service ']'
+ echo 'Systemd unit yb-bind_check.service is configured.'
+ add_result 'Systemd Unit File Check - yb-bind_check.service' PASS 'Systemd unit yb-bind_check.service is configured.'
+ local 'check=Systemd Unit File Check - yb-bind_check.service'
+ local result=PASS
+ local 'message=Systemd unit yb-bind_check.service is configured.'
+ '[' 1249 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-bind_check.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-bind_check.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-clean_cores.service ']'
+ echo 'Systemd unit yb-clean_cores.service is configured.'
+ add_result 'Systemd Unit File Check - yb-clean_cores.service' PASS 'Systemd unit yb-clean_cores.service is configured.'
+ local 'check=Systemd Unit File Check - yb-clean_cores.service'
+ local result=PASS
+ local 'message=Systemd unit yb-clean_cores.service is configured.'
+ '[' 1421 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-clean_cores.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-clean_cores.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-clean_cores.timer ']'
+ echo 'Systemd unit yb-clean_cores.timer is configured.'
+ add_result 'Systemd Unit File Check - yb-clean_cores.timer' PASS 'Systemd unit yb-clean_cores.timer is configured.'
+ local 'check=Systemd Unit File Check - yb-clean_cores.timer'
+ local result=PASS
+ local 'message=Systemd unit yb-clean_cores.timer is configured.'
+ '[' 1595 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-clean_cores.timer",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-clean_cores.timer is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.service ']'
+ echo 'Systemd unit yb-collect_metrics.service is configured.'
+ add_result 'Systemd Unit File Check - yb-collect_metrics.service' PASS 'Systemd unit yb-collect_metrics.service is configured.'
+ local 'check=Systemd Unit File Check - yb-collect_metrics.service'
+ local result=PASS
+ local 'message=Systemd unit yb-collect_metrics.service is configured.'
+ '[' 1765 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-collect_metrics.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-collect_metrics.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.timer ']'
+ echo 'Systemd unit yb-collect_metrics.timer is configured.'
+ add_result 'Systemd Unit File Check - yb-collect_metrics.timer' PASS 'Systemd unit yb-collect_metrics.timer is configured.'
+ local 'check=Systemd Unit File Check - yb-collect_metrics.timer'
+ local result=PASS
+ local 'message=Systemd unit yb-collect_metrics.timer is configured.'
+ '[' 1947 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-collect_metrics.timer",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-collect_metrics.timer is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-controller.service ']'
+ echo 'Systemd unit yb-controller.service is configured.'
+ add_result 'Systemd Unit File Check - yb-controller.service' PASS 'Systemd unit yb-controller.service is configured.'
+ local 'check=Systemd Unit File Check - yb-controller.service'
+ local result=PASS
+ local 'message=Systemd unit yb-controller.service is configured.'
+ '[' 2125 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-controller.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-controller.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.service ']'
+ echo 'Systemd unit yb-zip_purge_yb_logs.service is configured.'
+ add_result 'Systemd Unit File Check - yb-zip_purge_yb_logs.service' PASS 'Systemd unit yb-zip_purge_yb_logs.service is configured.'
+ local 'check=Systemd Unit File Check - yb-zip_purge_yb_logs.service'
+ local result=PASS
+ local 'message=Systemd unit yb-zip_purge_yb_logs.service is configured.'
+ '[' 2297 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-zip_purge_yb_logs.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.timer ']'
+ echo 'Systemd unit yb-zip_purge_yb_logs.timer is configured.'
+ add_result 'Systemd Unit File Check - yb-zip_purge_yb_logs.timer' PASS 'Systemd unit yb-zip_purge_yb_logs.timer is configured.'
+ local 'check=Systemd Unit File Check - yb-zip_purge_yb_logs.timer'
+ local result=PASS
+ local 'message=Systemd unit yb-zip_purge_yb_logs.timer is configured.'
+ '[' 2483 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.timer",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-zip_purge_yb_logs.timer is configured."
'
+ json_results+='    }'
++ grep -oP '(?<=^PLATFORM_ID=).+' /etc/os-release
++ tr -d '"'
+ platform_id=platform:el9
+ [[ platform:el9 == \p\l\a\t\f\o\r\m\:\e\l\8 ]]
+ verify_sysctl vm.swappiness 0
+ local param=vm.swappiness
+ local expected_value=0
+ local current_value
++ sysctl -n vm.swappiness
+ current_value=0
+ '[' 0 -eq 0 ']'
+ echo '[PASS] vm.swappiness is set to 0 (expected: 0)'
+ add_result vm.swappiness PASS 'vm.swappiness is set to 0 (expected: 0)'
+ local check=vm.swappiness
+ local result=PASS
+ local 'message=vm.swappiness is set to 0 (expected: 0)'
+ '[' 2665 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "vm.swappiness",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "vm.swappiness is set to 0 (expected: 0)"
'
+ json_results+='    }'
+ verify_sysctl vm.max_map_count 262144
+ local param=vm.max_map_count
+ local expected_value=262144
+ local current_value
++ sysctl -n vm.max_map_count
+ current_value=262144
+ '[' 262144 -eq 262144 ']'
+ echo '[PASS] vm.max_map_count is set to 262144 (expected: 262144)'
+ add_result vm.max_map_count PASS 'vm.max_map_count is set to 262144 (expected: 262144)'
+ local check=vm.max_map_count
+ local result=PASS
+ local 'message=vm.max_map_count is set to 262144 (expected: 262144)'
+ '[' 2793 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "vm.max_map_count",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "vm.max_map_count is set to 262144 (expected: 262144)"
'
+ json_results+='    }'
++ sysctl -n kernel.core_pattern
+ kernel_core_pattern_value=/data/home/yugabyte/cores/core_%p_%t_%E
+ '[' /data/home/yugabyte/cores/core_%p_%t_%E == /data/home/yugabyte/cores/core_%p_%t_%E ']'
+ echo '[PASS] kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E'
+ add_result kernel.core_pattern PASS 'kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E'
+ local check=kernel.core_pattern
+ local result=PASS
+ local 'message=kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E'
+ '[' 2937 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "kernel.core_pattern",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E"
'
+ json_results+='    }'
++ grep -oP '(?<=^PLATFORM_ID=).+' /etc/os-release
++ tr -d '"'
+ platform_id=platform:el9
+ [[ platform:el9 == \p\l\a\t\f\o\r\m\:\e\l\8 ]]
+ service_name=node_exporter.service
++ systemctl show -p ActiveState node_exporter.service
++ grep ActiveState
++ awk -F= '{print $2}'
+ status=active
+ '[' active = active ']'
+ echo '"node_exporter.service" is active'
+ add_result 'Service Status Check' PASS 'node_exporter.service is active'
+ local 'check=Service Status Check'
+ local result=PASS
+ local 'message=node_exporter.service is active'
+ '[' 3101 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Service Status Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "node_exporter.service is active"
'
+ json_results+='    }'
+ ports=(7000 7100 9000 9100 18018 22 5433 9042 9070 9300 12000 13000)
+ vm_ip=10.128.15.231
+ for port in "${ports[@]}"
++ start_server 7000
++ local port=7000
++ echo 5617
++ python3 -m http.server 7000
+ server_pid=5617
+ sleep 2
+ check_port 10.128.15.231 7000
+ local ip=10.128.15.231
+ local port=7000
+ '[' 0 -eq 0 ']'
+ echo 'Port 7000 is open on 10.128.15.231'
+ add_result 'Port 7000 Check' PASS 'Port 7000 is open on 10.128.15.231'
+ local 'check=Port 7000 Check'
+ local result=PASS
+ local 'message=Port 7000 is open on 10.128.15.231'
+ '[' 3228 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 7000 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 7000 is open on 10.128.15.231"
'
+ json_results+='    }'
+ ps -p 5617
+ force_kill 5617
+ local pid=5617
+ kill 5617
+ sleep 2
+ ps -p 5617
+ for port in "${ports[@]}"
++ start_server 7100
++ local port=7100
++ echo 5625
++ python3 -m http.server 7100
+ server_pid=5625
+ sleep 2
+ check_port 10.128.15.231 7100
+ local ip=10.128.15.231
+ local port=7100
+ '[' 0 -eq 0 ']'
+ echo 'Port 7100 is open on 10.128.15.231'
+ add_result 'Port 7100 Check' PASS 'Port 7100 is open on 10.128.15.231'
+ local 'check=Port 7100 Check'
+ local result=PASS
+ local 'message=Port 7100 is open on 10.128.15.231'
+ '[' 3353 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 7100 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 7100 is open on 10.128.15.231"
'
+ json_results+='    }'
+ ps -p 5625
+ force_kill 5625
+ local pid=5625
+ kill 5625
+ sleep 2
+ ps -p 5625
+ for port in "${ports[@]}"
++ start_server 9000
++ local port=9000
++ echo 5633
++ python3 -m http.server 9000
+ server_pid=5633
+ sleep 2
+ check_port 10.128.15.231 9000
+ local ip=10.128.15.231
+ local port=9000
+ '[' 0 -eq 0 ']'
+ echo 'Port 9000 is open on 10.128.15.231'
+ add_result 'Port 9000 Check' PASS 'Port 9000 is open on 10.128.15.231'
+ local 'check=Port 9000 Check'
+ local result=PASS
+ local 'message=Port 9000 is open on 10.128.15.231'
+ '[' 3478 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9000 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9000 is open on 10.128.15.231"
'
+ json_results+='    }'
+ ps -p 5633
+ force_kill 5633
+ local pid=5633
+ kill 5633
+ sleep 2
+ ps -p 5633
+ for port in "${ports[@]}"
++ start_server 9100
++ local port=9100
++ echo 5642
++ python3 -m http.server 9100
+ server_pid=5642
+ sleep 2
+ check_port 10.128.15.231 9100
+ local ip=10.128.15.231
+ local port=9100
+ '[' 0 -eq 0 ']'
+ echo 'Port 9100 is open on 10.128.15.231'
+ add_result 'Port 9100 Check' PASS 'Port 9100 is open on 10.128.15.231'
+ local 'check=Port 9100 Check'
+ local result=PASS
+ local 'message=Port 9100 is open on 10.128.15.231'
+ '[' 3603 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9100 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9100 is open on 10.128.15.231"
'
+ json_results+='    }'
+ ps -p 5642
+ force_kill 5642
+ local pid=5642
+ kill 5642
+ sleep 2
+ ps -p 5642
+ for port in "${ports[@]}"
++ start_server 18018
++ local port=18018
++ echo 5755
++ python3 -m http.server 18018
+ server_pid=5755
+ sleep 2
+ check_port 10.128.15.231 18018
+ local ip=10.128.15.231
+ local port=18018
+ '[' 0 -eq 0 ']'
+ echo 'Port 18018 is open on 10.128.15.231'
+ add_result 'Port 18018 Check' PASS 'Port 18018 is open on 10.128.15.231'
+ local 'check=Port 18018 Check'
+ local result=PASS
+ local 'message=Port 18018 is open on 10.128.15.231'
+ '[' 3728 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 18018 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 18018 is open on 10.128.15.231"
'
+ json_results+='    }'
+ ps -p 5755
+ force_kill 5755
+ local pid=5755
+ kill 5755
+ sleep 2
+ ps -p 5755
+ for port in "${ports[@]}"
++ start_server 22
++ local port=22
++ echo 5765
++ python3 -m http.server 22
+ server_pid=5765
+ sleep 2
+ check_port 10.128.15.231 22
+ local ip=10.128.15.231
+ local port=22
+ '[' 0 -eq 0 ']'
+ echo 'Port 22 is open on 10.128.15.231'
+ add_result 'Port 22 Check' PASS 'Port 22 is open on 10.128.15.231'
+ local 'check=Port 22 Check'
+ local result=PASS
+ local 'message=Port 22 is open on 10.128.15.231'
+ '[' 3855 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 22 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 22 is open on 10.128.15.231"
'
+ json_results+='    }'
+ ps -p 5765
+ for port in "${ports[@]}"
++ start_server 5433
++ local port=5433
++ echo 5771
++ python3 -m http.server 5433
+ server_pid=5771
+ sleep 2
+ check_port 10.128.15.231 5433
+ local ip=10.128.15.231
+ local port=5433
+ '[' 0 -eq 0 ']'
+ echo 'Port 5433 is open on 10.128.15.231'
+ add_result 'Port 5433 Check' PASS 'Port 5433 is open on 10.128.15.231'
+ local 'check=Port 5433 Check'
+ local result=PASS
+ local 'message=Port 5433 is open on 10.128.15.231'
+ '[' 3976 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 5433 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 5433 is open on 10.128.15.231"
'
+ json_results+='    }'
+ ps -p 5771
+ force_kill 5771
+ local pid=5771
+ kill 5771
+ sleep 2
+ ps -p 5771
+ for port in "${ports[@]}"
++ start_server 9042
++ local port=9042
++ echo 5779
++ python3 -m http.server 9042
+ server_pid=5779
+ sleep 2
+ check_port 10.128.15.231 9042
+ local ip=10.128.15.231
+ local port=9042
+ '[' 0 -eq 0 ']'
+ echo 'Port 9042 is open on 10.128.15.231'
+ add_result 'Port 9042 Check' PASS 'Port 9042 is open on 10.128.15.231'
+ local 'check=Port 9042 Check'
+ local result=PASS
+ local 'message=Port 9042 is open on 10.128.15.231'
+ '[' 4101 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9042 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9042 is open on 10.128.15.231"
'
+ json_results+='    }'
+ ps -p 5779
+ force_kill 5779
+ local pid=5779
+ kill 5779
+ sleep 2
+ ps -p 5779
+ for port in "${ports[@]}"
++ start_server 9070
++ local port=9070
++ echo 5789
++ python3 -m http.server 9070
+ server_pid=5789
+ sleep 2
+ check_port 10.128.15.231 9070
+ local ip=10.128.15.231
+ local port=9070
+ '[' 0 -eq 0 ']'
+ echo 'Port 9070 is open on 10.128.15.231'
+ add_result 'Port 9070 Check' PASS 'Port 9070 is open on 10.128.15.231'
+ local 'check=Port 9070 Check'
+ local result=PASS
+ local 'message=Port 9070 is open on 10.128.15.231'
+ '[' 4226 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9070 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9070 is open on 10.128.15.231"
'
+ json_results+='    }'
+ ps -p 5789
+ for port in "${ports[@]}"
++ start_server 9300
++ local port=9300
++ echo 5794
++ python3 -m http.server 9300
+ server_pid=5794
+ sleep 2
+ check_port 10.128.15.231 9300
+ local ip=10.128.15.231
+ local port=9300
+ '[' 0 -eq 0 ']'
+ echo 'Port 9300 is open on 10.128.15.231'
+ add_result 'Port 9300 Check' PASS 'Port 9300 is open on 10.128.15.231'
+ local 'check=Port 9300 Check'
+ local result=PASS
+ local 'message=Port 9300 is open on 10.128.15.231'
+ '[' 4351 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9300 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9300 is open on 10.128.15.231"
'
+ json_results+='    }'
+ ps -p 5794
+ for port in "${ports[@]}"
++ start_server 12000
++ local port=12000
++ echo 5799
++ python3 -m http.server 12000
+ server_pid=5799
+ sleep 2
+ check_port 10.128.15.231 12000
+ local ip=10.128.15.231
+ local port=12000
+ '[' 0 -eq 0 ']'
+ echo 'Port 12000 is open on 10.128.15.231'
+ add_result 'Port 12000 Check' PASS 'Port 12000 is open on 10.128.15.231'
+ local 'check=Port 12000 Check'
+ local result=PASS
+ local 'message=Port 12000 is open on 10.128.15.231'
+ '[' 4476 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 12000 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 12000 is open on 10.128.15.231"
'
+ json_results+='    }'
+ ps -p 5799
+ force_kill 5799
+ local pid=5799
+ kill 5799
+ sleep 2
+ ps -p 5799
+ for port in "${ports[@]}"
++ start_server 13000
++ local port=13000
++ echo 5807
++ python3 -m http.server 13000
+ server_pid=5807
+ sleep 2
+ check_port 10.128.15.231 13000
+ local ip=10.128.15.231
+ local port=13000
+ '[' 0 -eq 0 ']'
+ echo 'Port 13000 is open on 10.128.15.231'
+ add_result 'Port 13000 Check' PASS 'Port 13000 is open on 10.128.15.231'
+ local 'check=Port 13000 Check'
+ local result=PASS
+ local 'message=Port 13000 is open on 10.128.15.231'
+ '[' 4603 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 13000 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 13000 is open on 10.128.15.231"
'
+ json_results+='    }'
+ ps -p 5807
+ force_kill 5807
+ local pid=5807
+ kill 5807
+ sleep 2
+ ps -p 5807
+ service_name=yb-node-agent.service
+ yba_url=https://35.184.240.7
+ customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
+ token=3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
+ provider_id=2dc133e8-9482-4f77-ab4d-4f617ebbfa28
+ provider_name=onPrem
++ skip_tls_verify https://35.184.240.7
++ local yba_url=https://35.184.240.7
++ [[ https://35.184.240.7 == https* ]]
++ echo --insecure
+ tls_verify_option=--insecure
+ readarray -t headers
++ _get_headers 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
++ local token=3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
++ echo 'Accept: application/json'
++ echo 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb'
++ echo 'Content-Type: application/json'
+ header_options=()
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
++ systemctl show -p ActiveState yb-node-agent.service
++ grep ActiveState
++ awk -F= '{print $2}'
+ status=active
+ '[' active = active ']'
+ echo '"yb-node-agent.service" is active'
+ add_result 'Service Status Check' PASS 'yb-node-agent.service is active'
+ local 'check=Service Status Check'
+ local result=PASS
+ local 'message=yb-node-agent.service is active'
+ '[' 4730 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Service Status Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "yb-node-agent.service is active"
'
+ json_results+='    }'
++ systemctl show -p MemoryCurrent yb-node-agent.service
++ grep MemoryCurrent
++ awk -F= '{print $2}'
+ memory=9854976
+ '[' 9854976 -gt 0 ']'
+ echo 'MemoryCurrent is greater than 0: 9854976'
+ add_result 'Memory Usage Check' PASS 'MemoryCurrent is greater than 0: 9854976'
+ local 'check=Memory Usage Check'
+ local result=PASS
+ local 'message=MemoryCurrent is greater than 0: 9854976'
+ '[' 4857 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Memory Usage Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "MemoryCurrent is greater than 0: 9854976"
'
+ json_results+='    }'
+ '[' -z 2dc133e8-9482-4f77-ab4d-4f617ebbfa28 ']'
++ _get_nodes_in_provider https://35.184.240.7 cf6d0b42-bae0-49f2-8fa1-28a3f9216a77 2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ local yba_url=https://35.184.240.7
++ local customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
++ local provider_uuid=2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ echo https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
+ get_nodes_in_provider=https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
++ curl -s -w '%{http_code}' -o response.txt -X GET -H 'Accept: application/json' -H 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb' -H 'Content-Type: application/json' --insecure https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
+ response=200
+ http_status=200
+ response_body='[{"nodeUuid":"8dc3bcc1-fb03-42ec-9903-a07c3983e0d0","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n3","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.3","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n3"},"detailsJson":"{\"ip\":\"10.128.0.3\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n3\"}","inUse":true},{"nodeUuid":"716b9c3c-3743-41c7-b2b8-87d92f96bf83","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n2","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.2","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n2"},"detailsJson":"{\"ip\":\"10.128.0.2\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n2\"}","inUse":true},{"nodeUuid":"ed729a1c-89d8-44cf-8031-110cddfdd47f","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n1","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.15.231","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n1"},"detailsJson":"{\"ip\":\"10.128.15.231\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n1\"}","inUse":true}]'
+ matched=false
+ '[' 200 -ge 200 ']'
+ '[' 200 -lt 300 ']'
+ echo 'HTTP GET request successful. Processing response...'
++ echo '[{"nodeUuid":"8dc3bcc1-fb03-42ec-9903-a07c3983e0d0","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n3","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.3","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n3"},"detailsJson":"{\"ip\":\"10.128.0.3\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n3\"}","inUse":true},{"nodeUuid":"716b9c3c-3743-41c7-b2b8-87d92f96bf83","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n2","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.2","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n2"},"detailsJson":"{\"ip\":\"10.128.0.2\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n2\"}","inUse":true},{"nodeUuid":"ed729a1c-89d8-44cf-8031-110cddfdd47f","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n1","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.15.231","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n1"},"detailsJson":"{\"ip\":\"10.128.15.231\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n1\"}","inUse":true}]'
++ grep -o '"ip":"[a-zA-Z0-9.:_-]*"'
++ cut -d '"' -f4
+ ips='10.128.0.3
10.128.0.2
10.128.15.231'
+ for ip in $ips
+ [[ 10.128.0.3 == \1\0\.\1\2\8\.\1\5\.\2\3\1 ]]
+ for ip in $ips
+ [[ 10.128.0.2 == \1\0\.\1\2\8\.\1\5\.\2\3\1 ]]
+ for ip in $ips
+ [[ 10.128.15.231 == \1\0\.\1\2\8\.\1\5\.\2\3\1 ]]
+ matched=true
+ break
+ [[ true == false ]]
+ echo 'Node addition to the provider passed: 10.128.15.231'
+ add_result 'Node addition failed' PASS 'Node addition to the provider passed: 10.128.15.231'
+ local 'check=Node addition failed'
+ local result=PASS
+ local 'message=Node addition to the provider passed: 10.128.15.231'
+ '[' 4991 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Node addition failed",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Node addition to the provider passed: 10.128.15.231"
'
+ json_results+='    }'
+ check_marker_file
+ MARKER_FILE=/tmp/.reboot-required
+ '[' -f /tmp/.reboot-required ']'
+ echo 'Marker file does not exist: /tmp/.reboot-required'
+ return 1
+ echo 'Reboot requirement cleared.'
+ add_result 'REBOOT REQUIRED' PASS 'No Marker file present'
+ local 'check=REBOOT REQUIRED'
+ local result=PASS
+ local INI file has been created successfully at: /data/2024.2.2.2-b2/scripts/ynp/configs/config.ini
File written successfully and crash-consistent.
'message=No Marker file present'
+ '[' 5138 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "REBOOT REQUIRED",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "No Marker file present"
'
+ json_results+='    }'
+ print_results
+ any_fail=0
+ [[ {
"results":[
    {
      "check": "Clock Synchronization",
      "result": "PASS",
      "message": "System clock synchronized"
    },
    {
      "check": "Yugabyte User Existence Check",
      "result": "PASS",
      "message": "User yugabyte exists"
    },
    {
      "check": "Yugabyte Home Directory Permissions Check",
      "result": "PASS",
      "message": "/data/home/yugabyte has the correct ownership and acceptable permissions"
    },
    {
      "check": "/data Ownership/Permission Check",
      "result": "PASS",
      "message": "Directory /data is owned by yugabyte or has '7' (rwx) permissions."
    },
    {
      "check": "/data Free Space Check",
      "result": "PASS",
      "message": "Sufficient disk space available: 196G"
    },
    {
      "check": "Systemd Unit File Check - yb-tserver.service",
      "result": "PASS",
      "message": "Systemd unit yb-tserver.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-master.service",
      "result": "PASS",
      "message": "Systemd unit yb-master.service is configured."
    },
    {
      "check": "Systemd Unit File Check - clock-sync.sh.j2",
      "result": "PASS",
      "message": "Systemd unit clock-sync.sh.j2 is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-bind_check.service",
      "result": "PASS",
      "message": "Systemd unit yb-bind_check.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.service",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.timer",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.service",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.timer",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-controller.service",
      "result": "PASS",
      "message": "Systemd unit yb-controller.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.service",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.timer",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.timer is configured."
    },
    {
      "check": "vm.swappiness",
      "result": "PASS",
      "message": "vm.swappiness is set to 0 (expected: 0)"
    },
    {
      "check": "vm.max_map_count",
      "result": "PASS",
      "message": "vm.max_map_count is set to 262144 (expected: 262144)"
    },
    {
      "check": "kernel.core_pattern",
      "result": "PASS",
      "message": "kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "node_exporter.service is active"
    },
    {
      "check": "Port 7000 Check",
      "result": "PASS",
      "message": "Port 7000 is open on 10.128.15.231"
    },
    {
      "check": "Port 7100 Check",
      "result": "PASS",
      "message": "Port 7100 is open on 10.128.15.231"
    },
    {
      "check": "Port 9000 Check",
      "result": "PASS",
      "message": "Port 9000 is open on 10.128.15.231"
    },
    {
      "check": "Port 9100 Check",
      "result": "PASS",
      "message": "Port 9100 is open on 10.128.15.231"
    },
    {
      "check": "Port 18018 Check",
      "result": "PASS",
      "message": "Port 18018 is open on 10.128.15.231"
    },
    {
      "check": "Port 22 Check",
      "result": "PASS",
      "message": "Port 22 is open on 10.128.15.231"
    },
    {
      "check": "Port 5433 Check",
      "result": "PASS",
      "message": "Port 5433 is open on 10.128.15.231"
    },
    {
      "check": "Port 9042 Check",
      "result": "PASS",
      "message": "Port 9042 is open on 10.128.15.231"
    },
    {
      "check": "Port 9070 Check",
      "result": "PASS",
      "message": "Port 9070 is open on 10.128.15.231"
    },
    {
      "check": "Port 9300 Check",
      "result": "PASS",
      "message": "Port 9300 is open on 10.128.15.231"
    },
    {
      "check": "Port 12000 Check",
      "result": "PASS",
      "message": "Port 12000 is open on 10.128.15.231"
    },
    {
      "check": "Port 13000 Check",
      "result": "PASS",
      "message": "Port 13000 is open on 10.128.15.231"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "yb-node-agent.service is active"
    },
    {
      "check": "Memory Usage Check",
      "result": "PASS",
      "message": "MemoryCurrent is greater than 0: 9854976"
    },
    {
      "check": "Node addition failed",
      "result": "PASS",
      "message": "Node addition to the provider passed: 10.128.15.231"
    },
    {
      "check": "REBOOT REQUIRED",
      "result": "PASS",
      "message": "No Marker file present"
    } == *\"\r\e\s\u\l\t\"\:\ \"\F\A\I\L\"* ]]
+ json_results+='
]}'
+ echo '{
"results":[
    {
      "check": "Clock Synchronization",
      "result": "PASS",
      "message": "System clock synchronized"
    },
    {
      "check": "Yugabyte User Existence Check",
      "result": "PASS",
      "message": "User yugabyte exists"
    },
    {
      "check": "Yugabyte Home Directory Permissions Check",
      "result": "PASS",
      "message": "/data/home/yugabyte has the correct ownership and acceptable permissions"
    },
    {
      "check": "/data Ownership/Permission Check",
      "result": "PASS",
      "message": "Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions."
    },
    {
      "check": "/data Free Space Check",
      "result": "PASS",
      "message": "Sufficient disk space available: 196G"
    },
    {
      "check": "Systemd Unit File Check - yb-tserver.service",
      "result": "PASS",
      "message": "Systemd unit yb-tserver.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-master.service",
      "result": "PASS",
      "message": "Systemd unit yb-master.service is configured."
    },
    {
      "check": "Systemd Unit File Check - clock-sync.sh.j2",
      "result": "PASS",
      "message": "Systemd unit clock-sync.sh.j2 is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-bind_check.service",
      "result": "PASS",
      "message": "Systemd unit yb-bind_check.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.service",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.timer",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.service",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.timer",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-controller.service",
      "result": "PASS",
      "message": "Systemd unit yb-controller.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.service",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.timer",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.timer is configured."
    },
    {
      "check": "vm.swappiness",
      "result": "PASS",
      "message": "vm.swappiness is set to 0 (expected: 0)"
    },
    {
      "check": "vm.max_map_count",
      "result": "PASS",
      "message": "vm.max_map_count is set to 262144 (expected: 262144)"
    },
    {
      "check": "kernel.core_pattern",
      "result": "PASS",
      "message": "kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "node_exporter.service is active"
    },
    {
      "check": "Port 7000 Check",
      "result": "PASS",
      "message": "Port 7000 is open on 10.128.15.231"
    },
    {
      "check": "Port 7100 Check",
      "result": "PASS",
      "message": "Port 7100 is open on 10.128.15.231"
    },
    {
      "check": "Port 9000 Check",
      "result": "PASS",
      "message": "Port 9000 is open on 10.128.15.231"
    },
    {
      "check": "Port 9100 Check",
      "result": "PASS",
      "message": "Port 9100 is open on 10.128.15.231"
    },
    {
      "check": "Port 18018 Check",
      "result": "PASS",
      "message": "Port 18018 is open on 10.128.15.231"
    },
    {
      "check": "Port 22 Check",
      "result": "PASS",
      "message": "Port 22 is open on 10.128.15.231"
    },
    {
      "check": "Port 5433 Check",
      "result": "PASS",
      "message": "Port 5433 is open on 10.128.15.231"
    },
    {
      "check": "Port 9042 Check",
      "result": "PASS",
      "message": "Port 9042 is open on 10.128.15.231"
    },
    {
      "check": "Port 9070 Check",
      "result": "PASS",
      "message": "Port 9070 is open on 10.128.15.231"
    },
    {
      "check": "Port 9300 Check",
      "result": "PASS",
      "message": "Port 9300 is open on 10.128.15.231"
    },
    {
      "check": "Port 12000 Check",
      "result": "PASS",
      "message": "Port 12000 is open on 10.128.15.231"
    },
    {
      "check": "Port 13000 Check",
      "result": "PASS",
      "message": "Port 13000 is open on 10.128.15.231"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "yb-node-agent.service is active"
    },
    {
      "check": "Memory Usage Check",
      "result": "PASS",
      "message": "MemoryCurrent is greater than 0: 9854976"
    },
    {
      "check": "Node addition failed",
      "result": "PASS",
      "message": "Node addition to the provider passed: 10.128.15.231"
    },
    {
      "check": "REBOOT REQUIRED",
      "result": "PASS",
      "message": "No Marker file present"
    }
]}'
+ '[' 0 -eq 1 ']'
+ echo 'Pre-flight checks successful'

2025-06-02 14:41:13,173 - commands.provision_command - INFO - Return Code: 0
2025-06-02 14:41:13,490 - INFO - Running: /opt/yugabyte/software/active/yb-platform/yugaware/yba-cli/yba_cli-2024.2.2.2-b2-linux-amd64/yba universe node reprovision -H https://35.184.240.7/ -a 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb --name TU --node-name yb-dev-TU-n1

Waiting for node yb-dev-TU-n1 operation Reprovision to be completed
The node yb-dev-TU-n1 operation Reprovision has been completed
Node Name      Node UUID                              IP              State     Is Master process running   Is Tserver process running   Master state
yb-dev-TU-n1   ed729a1c-89d8-44cf-8031-110cddfdd47f   10.128.15.231   Stopped   false                       false                        -
2025-06-02 14:43:30,959 - INFO - Running: /opt/yugabyte/software/active/yb-platform/yugaware/yba-cli/yba_cli-2024.2.2.2-b2-linux-amd64/yba universe node start -H https://35.184.240.7/ -a 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb --name TU --node-name yb-dev-TU-n1

Waiting for node yb-dev-TU-n1 operation Start to be completed
The node yb-dev-TU-n1 operation Start has been completed
Node Name      Node UUID                              IP              State     Is Master process running   Is Tserver process running   Master state
yb-dev-TU-n1   ed729a1c-89d8-44cf-8031-110cddfdd47f   10.128.15.231   Live      true                        true                         
2025-06-02 14:45:58,437 - INFO - âœ… Rehydration complete for node yb-dev-TU-n1
2025-06-02 14:45:58,438 - INFO - Waiting 60 seconds before processing next node...
2025-06-02 14:46:58,496 - INFO - Running: gcloud compute instances list --filter networkInterfaces[0].networkIP=10.128.0.2 --format json --project yuga-rjw
2025-06-02 14:47:00,301 - INFO - Processing node yb-dev-TU-n2 (IP: 10.128.0.2)
2025-06-02 14:47:00,301 - INFO - Running: /opt/yugabyte/software/active/yb-platform/yugaware/yba-cli/yba_cli-2024.2.2.2-b2-linux-amd64/yba universe node stop -H https://35.184.240.7/ -a 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb --name TU --node-name yb-dev-TU-n2

Waiting for node yb-dev-TU-n2 operation Stop to be completed
The node yb-dev-TU-n2 operation Stop has been completed
Node Name      Node UUID                              IP           State     Is Master process running   Is Tserver process running   Master state
yb-dev-TU-n2   716b9c3c-3743-41c7-b2b8-87d92f96bf83   10.128.0.2   Stopped   false                       false                        -
2025-06-02 14:48:19,771 - INFO - âœ… Stopped node yb-dev-TU-n2 using YBA CLI
2025-06-02 14:48:19,771 - INFO - Running: gcloud compute instances stop yb-tu-dev-n1 --zone us-central1-a --project yuga-rjw
Stopping instance(s) yb-tu-dev-n1...done.                                                                                                                                                                                             
Updated [https://compute.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-a/instances/yb-tu-dev-n1].
2025-06-02 14:48:41,034 - INFO - Running: gcloud compute instances describe yb-tu-dev-n1 --zone us-central1-a --project yuga-rjw --format=json
2025-06-02 14:48:42,529 - INFO - Running: gcloud compute instances describe yb-tu-dev-n1 --zone us-central1-a --project yuga-rjw --format=json
2025-06-02 14:48:44,001 - INFO - Running: gcloud compute disks create yb-tu-dev-n1-boot-1748875724 --image rhel-9-v20240415 --image-project rhel-cloud --size 150GB --type pd-balanced --zone us-central1-a --project yuga-rjw
Created [https://www.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-a/disks/yb-tu-dev-n1-boot-1748875724].
WARNING: Some requests generated warnings:
 - Disk size: '150 GB' is larger than image size: '20 GB'. You might need to resize the root repartition manually if the operating system does not support automatic resizing. See https://cloud.google.com/compute/docs/disks/add-persistent-disk#resize_pd for details.
 - The resource 'projects/rhel-cloud/global/images/rhel-9-v20240415' is deprecated. A suggested replacement is 'projects/rhel-cloud/global/images/rhel-9-v20240515'.

NAME                          ZONE           SIZE_GB  TYPE         STATUS
yb-tu-dev-n1-boot-1748875724  us-central1-a  150      pd-balanced  READY
2025-06-02 14:48:47,251 - INFO - Running: gcloud compute instances detach-disk yb-tu-dev-n1 --disk yb-tu-dev-n1 --zone us-central1-a --project yuga-rjw
Updated [https://www.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-a/instances/yb-tu-dev-n1].
2025-06-02 14:48:50,311 - INFO - Running: gcloud compute instances attach-disk yb-tu-dev-n1 --disk yb-tu-dev-n1-boot-1748875724 --zone us-central1-a --project yuga-rjw --boot
Updated [https://www.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-a/instances/yb-tu-dev-n1].
2025-06-02 14:48:55,049 - INFO - Running: gcloud compute instances start yb-tu-dev-n1 --zone us-central1-a --project yuga-rjw
Starting instance(s) yb-tu-dev-n1...done.                                                                                                                                                                                             
Updated [https://compute.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-a/instances/yb-tu-dev-n1].
Instance internal IP is 10.128.0.2
Instance external IP is 35.188.85.36
2025-06-02 14:49:11,120 - INFO - Running: gcloud compute instances describe yb-tu-dev-n1 --zone us-central1-a --project yuga-rjw --format=json
2025-06-02 14:49:42,623 - INFO - Running: gcloud compute ssh weiwang@yb-tu-dev-n1 --zone us-central1-a --project yuga-rjw --internal-ip --command lsblk -o NAME,FSTYPE,UUID | grep xfs | grep -v sda
2025-06-02 14:49:45,446 - INFO - Running: gcloud compute ssh weiwang@yb-tu-dev-n1 --zone us-central1-a --project yuga-rjw --internal-ip --command sudo mkdir -p /data && sudo mount UUID=db39bcda-8bf8-4f05-8003-e7824f7a4da5 /data && sudo chmod 777 /data && grep -q UUID=db39bcda-8bf8-4f05-8003-e7824f7a4da5 /etc/fstab || echo 'UUID=db39bcda-8bf8-4f05-8003-e7824f7a4da5 /data xfs defaults,nofail 0 2' | sudo tee -a /etc/fstab
UUID=db39bcda-8bf8-4f05-8003-e7824f7a4da5 /data xfs defaults,nofail 0 2
2025-06-02 14:49:48,018 - INFO - Running: gcloud compute ssh weiwang@yb-tu-dev-n1 --zone us-central1-a --project yuga-rjw --internal-ip --command echo 'ðŸ›  Reprovisioning yugabyte user and running node-agent-provision.sh...' && sudo pkill -u yugabyte || true && id yugabyte && sudo userdel -r yugabyte || true && getent group yugabyte && (getent passwd | awk -F: '$4 == "$(getent group yugabyte | cut -d: -f3)"' | grep -q . || sudo groupdel yugabyte) || true && sudo useradd -m -d /data/home/yugabyte -s /bin/bash yugabyte && sudo mkdir -p /data/home/yugabyte && sudo chown -R yugabyte:yugabyte /data/home/yugabyte && sudo chmod 755 /data/home/yugabyte && cd /data/2024.2.2.2-b2/scripts && sudo ./node-agent-provision.sh
ðŸ›  Reprovisioning yugabyte user and running node-agent-provision.sh...
uid=1017(yugabyte) gid=1018(yugabyte) groups=1018(yugabyte),4(adm),39(video),1000(google-sudoers)
useradd: warning: the home directory /data/home/yugabyte already exists.
useradd: Not copying any file from skel directory into it.
[2025-06-02_14_49_54 common.sh:626 activate_pex] Using pex virtualenv python executable now.
2025-06-02 14:49:54,590 - root - INFO - Logging Setup Done
2025-06-02 14:49:54,592 - __main__ - DEBUG - YNP config {'logging': {'directory': './logs', 'file': 'app.log', 'level': 'DEBUG'},
 'yba': {'api_key': '3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb',
         'customer_uuid': 'cf6d0b42-bae0-49f2-8fa1-28a3f9216a77',
         'instance_type': {'cores': 4,
                           'memory_size': 16,
                           'mount_points': ['/data'],
                           'name': 'n2',
                           'volume_size': 100},
         'node_external_fqdn': '10.128.0.2',
         'node_name': 'yb-dev-tu-n1',
         'provider': {'name': 'onPrem',
                      'region': {'name': 'us-central1',
                                 'zone': {'name': 'us-central1-a'}}},
         'url': 'https://35.184.240.7'},
 'ynp': {'chrony_servers': ['0.pool.ntp.org', '1.pool.ntp.org'],
         'is_airgap': False,
         'is_install_node_agent': True,
         'node_ip': '10.128.0.2',
         'tmp_directory': '/tmp',
         'use_system_level_systemd': False,
         'yb_home_dir': '/data/home/yugabyte',
         'yb_user_id': 1994}}
2025-06-02 14:49:54,592 - __main__ - INFO - Config here: {'Preprovision': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875794', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp'}, 'ConfigureChrony': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875794', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'chrony_servers': '"0.pool.ntp.org, 1.pool.ntp.org"'}, 'CreateYugabyteUser': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875794', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'yb_user_id': '1994', 'yb_user_password': '', 'mount_points': '/data'}, 'ConfigureSystemd': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875794', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'user_name': 'yugabyte', 'use_system_level_systemd': 'False', 'service_files': '"yb-tserver.service, yb-master.service, clock-sync.sh.j2, yb-bind_check.service, yb-clean_cores.service, yb-clean_cores.timer, yb-collect_metrics.service, yb-collect_metrics.timer, yb-controller.service, yb-zip_purge_yb_logs.service, yb-zip_purge_yb_logs.timer"'}, 'ConfigureOs': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875794', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'message': '"Configure limits and sysctl parameters"', 'fd_limit': '1048576', 'nproc_limit': '12000', 'vm_swappiness': '0', 'kernel_core_pattern': '/data/home/yugabyte/cores/core_%p_%t_%E', 'vm_max_map_count': '262144', 'mount_points': '/data', 'limits': {'core': 'unlimited', 'data': 'unlimited', 'fsize': 'unlimited', 'sigpending': '119934', 'memlock': '64', 'rss': 'unlimited', 'nofile': '1048576', 'msgqueue': '819200', 'stack': '8192', 'cpu': 'unlimited', 'nproc': '12000', 'locks': 'unlimited'}}, 'ConfigureOs.limits': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875794', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'core': 'unlimited', 'data': 'unlimited', 'fsize': 'unlimited', 'sigpending': '119934', 'memlock': '64', 'rss': 'unlimited', 'nofile': '1048576', 'msgqueue': '819200', 'stack': '8192', 'cpu': 'unlimited', 'nproc': '12000', 'locks': 'unlimited'}, 'ConfigureNodeExporter': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875794', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'prometheus_user': 'prometheus'}, 'ConfigureNetwork': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875794', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'ip_address': '10.128.0.2', 'ports': '7000 7100 9000 9100 18018 22 5433 9042 9070 9300 12000 13000'}, 'InstallNodeAgent': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875794', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'url': 'https://35.184.240.7', 'customer_uuid': 'cf6d0b42-bae0-49f2-8fa1-28a3f9216a77', 'api_key': '3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb', 'node_name': 'yb-dev-tu-n1', 'node_external_fqdn': '10.128.0.2', 'provider_name': 'onPrem', 'provider_region_name': 'us-central1', 'provider_region_zone_name': 'us-central1-a', 'instance_type_name': 'n2', 'instance_type_cores': '4', 'instance_type_memory_size': '16', 'instance_type_volume_size': '100', 'instance_type_mount_points': "['/data']", 'node_ip': '10.128.0.2', 'bind_ip': '10.128.0.2'}, 'RebootNode': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875794', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp'}, 'DEFAULT': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748875794', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp'}}
2025-06-02 14:49:54,592 - __main__ - INFO - Python Version: 3.9.18 (main, Jan  4 2024, 00:00:00) 
[GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]
2025-06-02 14:49:54,603 - __main__ - INFO - Jinja2 -- 3.0.3
2025-06-02 14:49:54,608 - __main__ - INFO - MarkupSafe -- 3.0.2
2025-06-02 14:49:54,614 - __main__ - INFO - ansible-vault -- 2.1.0
2025-06-02 14:49:54,618 - __main__ - INFO - setuptools -- 78.1.0
2025-06-02 14:49:54,622 - __main__ - INFO - ansible -- 2.9.27
2025-06-02 14:49:54,632 - __main__ - INFO - PyYAML -- 6.0.2
2025-06-02 14:49:54,635 - __main__ - INFO - cryptography -- 44.0.2
2025-06-02 14:49:54,637 - __main__ - INFO - cffi -- 1.17.1
2025-06-02 14:49:54,639 - __main__ - INFO - pycparser -- 2.22
2025-06-02 14:49:54,644 - __main__ - INFO - distro -- 1.5.0
2025-06-02 14:49:54,645 - __main__ - INFO - mitogen -- 0.2.9
2025-06-02 14:49:54,647 - __main__ - INFO - requests -- 2.32.3
2025-06-02 14:49:54,651 - __main__ - INFO - charset-normalizer -- 3.4.1
2025-06-02 14:49:54,656 - __main__ - INFO - idna -- 3.10
2025-06-02 14:49:54,658 - __main__ - INFO - urllib3 -- 2.3.0
2025-06-02 14:49:54,661 - __main__ - INFO - certifi -- 2025.1.31
2025-06-02 14:49:54,662 - __main__ - INFO - semver -- 3.0.2
2025-06-02 14:49:54,664 - __main__ - INFO - six -- 1.10.0
2025-06-02 14:49:54,665 - __main__ - INFO - Jinja2 -- 3.0.3
2025-06-02 14:49:54,666 - __main__ - INFO - MarkupSafe -- 3.0.2
2025-06-02 14:49:54,666 - __main__ - INFO - ansible-vault -- 2.1.0
2025-06-02 14:49:54,667 - __main__ - INFO - setuptools -- 78.1.0
2025-06-02 14:49:54,668 - __main__ - INFO - ansible -- 2.9.27
2025-06-02 14:49:54,668 - __main__ - INFO - PyYAML -- 6.0.2
2025-06-02 14:49:54,669 - __main__ - INFO - cryptography -- 44.0.2
2025-06-02 14:49:54,670 - __main__ - INFO - cffi -- 1.17.1
2025-06-02 14:49:54,670 - __main__ - INFO - pycparser -- 2.22
2025-06-02 14:49:54,671 - __main__ - INFO - distro -- 1.5.0
2025-06-02 14:49:54,671 - __main__ - INFO - mitogen -- 0.2.9
2025-06-02 14:49:54,672 - __main__ - INFO - requests -- 2.32.3
2025-06-02 14:49:54,674 - __main__ - INFO - charset-normalizer -- 3.4.1
2025-06-02 14:49:54,675 - __main__ - INFO - idna -- 3.10
2025-06-02 14:49:54,675 - __main__ - INFO - urllib3 -- 2.3.0
2025-06-02 14:49:54,676 - __main__ - INFO - certifi -- 2025.1.31
2025-06-02 14:49:54,677 - __main__ - INFO - semver -- 3.0.2
2025-06-02 14:49:54,677 - __main__ - INFO - six -- 1.10.0
2025-06-02 14:49:55,070 - commands.provision_command - INFO - {'ConfigureChrony': (<class 'modules.provision.chrony.chrony.ConfigureChrony'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/chrony/chrony.py'), 'ConfigureOs': (<class 'modules.provision.configure_os.os_config.ConfigureOs'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/configure_os/os_config.py'), 'ConfigureNetwork': (<class 'modules.provision.network.network.ConfigureNetwork'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/network/network.py'), 'InstallNodeAgent': (<class 'modules.provision.node_agent.node_agent.InstallNodeAgent'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/node_agent/node_agent.py'), 'ConfigureNodeExporter': (<class 'modules.provision.node_exporter.node_exporter.ConfigureNodeExporter'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/node_exporter/node_exporter.py'), 'RebootNode': (<class 'modules.provision.reboot_node.reboot_node.RebootNode'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/reboot_node/reboot_node.py'), 'ConfigureSystemd': (<class 'modules.provision.systemd.systemd.ConfigureSystemd'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/systemd/systemd.py'), 'Preprovision': (<class 'modules.provision.update_os.update.Preprovision'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/update_os/update.py'), 'CreateYugabyteUser': (<class 'modules.provision.yugabyte.yugabyte.CreateYugabyteUser'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/yugabyte/yugabyte.py')}
2025-06-02 14:49:55,071 - commands.provision_command - INFO - initialized
2025-06-02 14:49:55,370 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): 35.184.240.7:443
/data/2024.2.2.2-b2/devops/pexvenv/lib64/python3.9/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '35.184.240.7'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings
  warnings.warn(
2025-06-02 14:49:55,703 - urllib3.connectionpool - DEBUG - https://35.184.240.7:443 "GET /api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers?name=onPrem HTTP/1.1" 200 979
2025-06-02 14:49:55,704 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): 35.184.240.7:443
/data/2024.2.2.2-b2/devops/pexvenv/lib64/python3.9/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '35.184.240.7'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings
  warnings.warn(
2025-06-02 14:49:56,014 - urllib3.connectionpool - DEBUG - https://35.184.240.7:443 "GET /api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/instance_types/n2 HTTP/1.1" 200 255
2025-06-02 14:49:56,034 - commands.provision_command - INFO - /tmp/tmpjwg59vv5
2025-06-02 14:49:56,034 - commands.provision_command - INFO - /tmp/tmp64pc6r_1
2025-06-02 14:50:09,593 - commands.provision_command - INFO - Output: 200 OK
User yugabyte already exists
User yugabyte added to systemd-journal group
el8 not detected, skipping changing selinux context
*                -       core            unlimited
*                -       data            unlimited
*                -       fsize            unlimited
*                -       sigpending            119934
*                -       memlock            64
*                -       rss            unlimited
*                -       nofile            1048576
*                -       msgqueue            819200
*                -       stack            8192
*                -       cpu            unlimited
*                -       nproc            12000
*                -       locks            unlimited
File /etc/security/limits.d/20-nproc.conf created.
el8 not detected, not updating file limits
Systemd file limits configured.
vm.swappiness = 0
kernel.core_pattern = /data/home/yugabyte/cores/core_%p_%t_%E
vm.max_map_count = 262144
Kernel settings configured.
OS Configuration applied successfully.
Node exporter setup is complete.
Start network configuration
Nothing to be done
End network configuration
Using node agent port 9070.
* Starting YB Node Agent install.
* Downloading YB Node Agent build package.
* Getting Linux/amd64 package
* Creating Node Agent Directory.
* Changing directory to node agent.
* Creating Sub Directories.
* Downloaded Version - 2024.2.2.2-b2
* Extracting the build package
   â€¢ Completed Node Agent Configuration
   â€¢ Checking for existing Node Agent with IP 10.128.0.2
   â€¢ Node Agent is already registered with IP 10.128.0.2
   â€¢ Node Agent Configuration Successful
Source ~/.bashrc to make node-agent available in the PATH.
You can install a systemd service on linux machines by running node-agent-installer.sh -c install_service --user yugabyte (Requires sudo access).
Using node agent port 9070.
* Starting YB Node Agent install_service.
success
* Installing Node Agent Systemd Service
  [Unit]
  Description=YB Anywhere Node Agent
  After=network-online.target
  # Disable restart limits, using RestartSec to rate limit restarts.
  StartLimitInterval=0

  [Service]
  User=yugabyte
  WorkingDirectory=/data/home/yugabyte/node-agent
  LimitCORE=infinity
  LimitNOFILE=1048576
  LimitNPROC=12000
  ExecStart=/data/home/yugabyte/node-agent/pkg/bin/node-agent server start
  Restart=always
  RestartSec=2

  [Install]
  WantedBy=multi-user.target
* Starting the systemd service
* Started the systemd service
* Run 'systemctl status yb-node-agent' to check the status of the yb-node-agent
* Run 'sudo systemctl stop yb-node-agent' to stop the yb-node-agent service
HTTP GET request successful. Processing response...
HTTP GET request successful. Processing response...
Match found: Zone Code = us-central1-a, UUID = 559eb98c-4f80-4304-ac69-a89a36fb2a4f
Error: POST request failed with HTTP status 400

2025-06-02 14:50:09,593 - commands.provision_command - INFO - Error: + json_results='{
"results":[
'
+ SUDO_ACCESS=false
++ id -u
+ '[' 0 = 0 ']'
+ SUDO_ACCESS=true
+ [[ OSFamily.REDHAT == suse ]]
+ [[ rhel == ubuntu ]]
+ [[ OSFamily.REDHAT == almaLinux ]]
+ [[ rhel == RedHat ]]
+ [[ rhel == RedHat ]]
+ [[ -n '' ]]
+ [[ -n '' ]]
+ [[ -n '' ]]
+ [[ rhel == ubuntu ]]
+ [[ OSFamily.REDHAT == RedHat ]]
+ [[ OSFamily.REDHAT == RedHat ]]
+ [[ rhel == oraclelinux ]]
+ [[ rhel == oraclelinux ]]
+ '[' -n '' ']'
+ '[' -n '' ']'
+ chronyc makestep
+ id yugabyte
+ echo 'User yugabyte already exists'
+ '[' -n '' ']'
+ groups yugabyte
+ grep -q '\bsystemd-journal\b'
+ usermod -aG systemd-journal yugabyte
+ echo 'User yugabyte added to systemd-journal group'
++ grep -oP '(?<=^PLATFORM_ID=).+' /etc/os-release
++ tr -d '"'
+ platform_id=platform:el9
+ [[ platform:el9 == \p\l\a\t\f\o\r\m\:\e\l\8 ]]
+ echo 'el8 not detected, skipping changing selinux context'
+ installer_dir=/data/2024.2.2.2-b2/scripts/ynp/../../bin
+ mkdir -p /data/home/yugabyte/.install
+ cp /data/2024.2.2.2-b2/scripts/ynp/../../bin/node-agent-installer.sh /data/home/yugabyte/.install
+ chown -R yugabyte:yugabyte /data/home/yugabyte/.install
+ chmod 750 /data/home/yugabyte
+ systemd_dir=/etc/systemd/system
+ loginctl enable-linger yugabyte
++ id -u yugabyte
+ su - yugabyte -c 'export XDG_RUNTIME_DIR=/run/user/1019'
++ id -u yugabyte
+ echo 'export XDG_RUNTIME_DIR=/run/user/1019'
+ systemd_dir=/data/home/yugabyte/.config/systemd/user
+ su - yugabyte -c 'mkdir -p /data/home/yugabyte/.config/systemd/user'
++ mktemp
+ tmp_file=/tmp/tmp.bjDxYGTfeW
+ echo '[Unit]
Description=Yugabyte tserver service
Requires=network-online.target
After=network.target network-online.target multi-user.target
# Disable restart limits, using RestartSec to rate limit restarts
StartLimitInterval=0
Wants=yb-ysql-cgroup.service

[Path]
PathExists=/data/home/yugabyte/tserver/bin/yb-tserver
PathExists=/data/home/yugabyte/tserver/conf/server.conf

[Service]
# Start


ExecStartPre=/data/home/yugabyte/bin/clock-sync.sh
ExecStart=/data/home/yugabyte/tserver/bin/yb-tserver --flagfile /data/home/yugabyte/tserver/conf/server.conf
Restart=always
RestartSec=5
# Stop -> SIGTERM - 10s - SIGKILL (if not stopped) [matches existing cron behavior]
KillMode=process
TimeoutStopFailureMode=terminate
KillSignal=SIGTERM
TimeoutStopSec=10
FinalKillSignal=SIGKILL
# Logs
StandardOutput=syslog
StandardError=syslog
# ulimit
LimitCORE=infinity
LimitNOFILE=1048576
LimitNPROC=12000
# Allow tserver to move postgres into its own cgroup
Delegate=true

[Install]
WantedBy=default.target'
+ chown yugabyte:yugabyte /tmp/tmp.bjDxYGTfeW
+ su - yugabyte -c 'mv /tmp/tmp.bjDxYGTfeW /data/home/yugabyte/.config/systemd/user/yb-tserver.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-tserver.service'
++ mktemp
+ tmp_file=/tmp/tmp.TMxdqmbMiu
+ echo '[Unit]
Description=Yugabyte master service
Requires=network-online.target
After=network.target network-online.target multi-user.target
# Disable restart limits, using RestartSec to rate limit restarts
StartLimitInterval=0

[Path]
PathExists=/data/home/yugabyte/master/bin/yb-master
PathExists=/data/home/yugabyte/master/conf/server.conf

[Service]
# Start


ExecStartPre=/data/home/yugabyte/bin/clock-sync.sh
ExecStart=/data/home/yugabyte/master/bin/yb-master --flagfile /data/home/yugabyte/master/conf/server.conf
Restart=always
RestartSec=5
# Stop -> SIGTERM - 10s - SIGKILL (if not stopped) [matches existing cron behavior]
KillMode=process
TimeoutStopFailureMode=terminate
KillSignal=SIGTERM
TimeoutStopSec=10
FinalKillSignal=SIGKILL
# Logs
StandardOutput=syslog
StandardError=syslog
# ulimit
LimitCORE=infinity
LimitNOFILE=1048576
LimitNPROC=12000

[Install]
WantedBy=default.target'
+ chown yugabyte:yugabyte /tmp/tmp.TMxdqmbMiu
+ su - yugabyte -c 'mv /tmp/tmp.TMxdqmbMiu /data/home/yugabyte/.config/systemd/user/yb-master.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-master.service'
++ mktemp
+ tmp_file=/tmp/tmp.2v0yWWp2mQ
++ basename /tmp/tmp64pc6r_1
++ chronyc tracking
++ chronyc tracking
++ echo ''
++ awk '/Reference ID/ {print $4}'
++ echo ''
++ awk '/System time/ {print $4}'
++ echo ''
++ awk '/Root dispersion/ {print $4}'
++ echo ''
++ awk '/Root delay/ {print $4}'
++ python -c 'print( +  + (.5 * ))'
  File "<string>", line 1
    print( +  + (.5 * ))
                      ^
SyntaxError: invalid syntax
++ ntpq -c 'rv 0 clock'
++ awk '-F[=,]' '/offset/ {print $3}'
/tmp/tmp64pc6r_1: line 339: ntpq: command not found
++ ntpq -p
++ awk '$1 ~ "^*" {print $9}'
/tmp/tmp64pc6r_1: line 339: ntpq: command not found
++ python -c 'print( * 1000)'
Traceback (most recent call last):
  File "<string>", line 1, in <module>
TypeError: print() argument after * must be an iterable, not int
++ timedatectl status
++ grep 'System clock synchronized'
++ awk '{print $4}'
++ systemctl show --no-pager
++ grep ActiveState
++ cut -d= -f2
+ echo '#!/bin/bash

SCRIPT_NAME=tmp64pc6r_1

################### Config ###################
is_acceptable_clock_skew_wait_enabled=True  # Whether check clock skew
acceptable_clock_skew_sec=0.5  # In seconds
max_tries=120  # Maximum number of tries before returning failure
retry_wait_time_s=1  # How long waits before retry in seconds

if [[  != true &&  != True ]]; then
  echo Wait' for clock skew to go below the acceptable threshold is disabled. Returning 'success.
  exit 0
fi

command_exists() {
  command -v  >/dev/null 2>&1
}

readonly PYTHON_EXECUTABLES=('\''python'\'' '\''python3'\'' '\''python3.11'\'' '\''python3.10'\'' '\''python3.9'\'' '\''python3.8'\'' '\''python3.7'\'' '\''python3.6'\'' '\''python3.12'\'' '\''python2'\'')
PYTHON_EXECUTABLE=
set_python_executable() {
  for py_executable in ; do
    if which  > /dev/null 2>&1; then
      PYTHON_EXECUTABLE=
      export PYTHON_EXECUTABLE
      return
    fi
  done
}

check_clock_sync_chrony() {
  # if chrond is restarted, tracking will return all 0'\''s
  set_python_executable
  chrony_tracking=Reference' ID : A9FEA9FE '(metadata.google.internal)' Stratum : 3 Ref time '(UTC)' : Mon Jun 02 14:49:39 2025 System time : 0.000000000 seconds fast of NTP time Last offset : +0.000012354 seconds RMS offset : 0.000012354 seconds Frequency : 65.482 ppm slow Residual freq : +0.064 ppm Skew : 58.754 ppm Root delay : 0.000196759 seconds Root dispersion : 0.001091068 seconds Update interval : 2.0 seconds Leap status : 'Normal
  if [[ 0 -ne 0 ]]; then 
    echo Reference' ID : A9FEA9FE '(metadata.google.internal)' Stratum : 3 Ref time '(UTC)' : Mon Jun 02 14:49:39 2025 System time : 0.000000000 seconds fast of NTP time Last offset : +0.000012354 seconds RMS offset : 0.000012354 seconds Frequency : 65.482 ppm slow Residual freq : +0.064 ppm Skew : 58.754 ppm Root delay : 0.000196759 seconds Root dispersion : 0.001091356 seconds Update interval : 2.0 seconds Leap status : Normal failed to 'execute
    return 1
  fi
  if [[  == 00000000 ]]; then
    echo chrony' is not 'initialized
    return 1
  fi
  local skew=
  local dispersion=
  local delay=
  local clock_error=
  if [[ -z python ]]; then
    clock_error=
  else
   clock_error=
  fi
  
  if awk '\''BEGIN{exit !('\'''\'' < '\'''\'')}'\''; then
    echo Clock' skew is within acceptable limits: 'ms
    return 0
  else
    echo Clock' skew exceeds acceptable limits: 'ms
    return 1
  fi
}

check_clock_sync_ntpd() {
  #local skew=
  local skew=
  local acceptable_skew_ms=

  if [[ -z  ]]; then
    echo ntpd' is not 'initialized
    return 1
  fi
  
  if awk '\''BEGIN{exit !('\'''\'' < '\'''\'')}'\''; then
    echo Clock' skew is within acceptable limits: 'ms
    return 0
  else
    echo Clock' skew exceeds acceptable limits: 'ms
    return 1
  fi
}

check_clock_sync_timesyncd() {
  synchronized=no
  if [[  == yes ]]; then
    echo timesyncd' reports clock is 'synchronized
    return 0
  else
    echo timesyncd' clock is not 'synchronized
    return 1
  fi
}

systemd_loaded() {
  active=
  if [[  == active ]]; then
    return 0
  fi
  return 1
}

iter=0
while true; do
  # If chrony is available, use it for clock sync.
  if command_exists chronyc; then
    check_clock_sync_chrony
    res=0
  # If ntpd is available, use it for clock sync.
  elif command_exists ntpd; then
    check_clock_sync_ntpd
    res=0
  elif systemd_loaded systemd-timesyncd; then 
    check_clock_sync_timesyncd
    res=0
  else
    echo Chrony,' NTPd, and timesyncd are not available, but 'required.
    exit 1
  fi
  ((iter++))
  if [  -eq 0 ]; then
    echo Success!' Clock skew is within acceptable 'limits.
    exit 0
  fi
  if [  -ge  ]; then
    echo Failure!' Maximum number of tries 'reached.
    exit 1
  fi
  sleep 
done'
+ chown yugabyte:yugabyte /tmp/tmp.2v0yWWp2mQ
+ su - yugabyte -c 'mv /tmp/tmp.2v0yWWp2mQ /data/home/yugabyte/.config/systemd/user/clock-sync.sh.j2'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/clock-sync.sh.j2'
++ mktemp
+ tmp_file=/tmp/tmp.BK2wAtVtTh
+ echo '[Unit]
Description=Yugabyte IP Bind Check
Requires=network-online.target
After=network.target network-online.target multi-user.target
Before=yb-controller.service yb-tserver.service yb-master.service
# Disable restart limits, using RestartSec to rate limit restarts
StartLimitInterval=0

[Path]
PathExists=/data/home/yugabyte/controller/bin/yb-controller-server
PathExists=/data/home/yugabyte/controller/conf/server.conf

[Service]
# Start
ExecStart=/data/home/yugabyte/controller/bin/yb-controller-server     --flagfile /data/home/yugabyte/controller/conf/server.conf     --only_bind --logtostderr
Type=oneshot
KillMode=control-group
KillSignal=SIGTERM
TimeoutStopSec=10
# Logs
StandardOutput=syslog
StandardError=syslog

[Install]
WantedBy=default.target'
+ chown yugabyte:yugabyte /tmp/tmp.BK2wAtVtTh
+ su - yugabyte -c 'mv /tmp/tmp.BK2wAtVtTh /data/home/yugabyte/.config/systemd/user/yb-bind_check.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-bind_check.service'
++ mktemp
+ tmp_file=/tmp/tmp.AC13znG8Hc
+ echo '[Unit]
Description=Yugabyte clean cores

[Service]


Type=oneshot
WorkingDirectory=/data/home/yugabyte/bin
ExecStart=/bin/bash /data/home/yugabyte/bin/clean_cores.sh

[Install]
WantedBy=multi-user.target'
+ chown yugabyte:yugabyte /tmp/tmp.AC13znG8Hc
+ su - yugabyte -c 'mv /tmp/tmp.AC13znG8Hc /data/home/yugabyte/.config/systemd/user/yb-clean_cores.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-clean_cores.service'
++ mktemp
+ tmp_file=/tmp/tmp.CdwH06b6Up
+ echo '[Unit]
Description=Yugabyte clean cores

[Timer]


Unit=yb-clean_cores.service
# Run every 5 minutes
OnCalendar=*:0/5

[Install]
WantedBy=timers.target'
+ chown yugabyte:yugabyte /tmp/tmp.CdwH06b6Up
+ su - yugabyte -c 'mv /tmp/tmp.CdwH06b6Up /data/home/yugabyte/.config/systemd/user/yb-clean_cores.timer'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-clean_cores.timer'
++ mktemp
+ tmp_file=/tmp/tmp.bjTyZtC3Vo
+ echo '[Unit]
Description=Yugabyte collect metrics

[Service]


Type=oneshot
WorkingDirectory=/data/home/yugabyte/bin
ExecStart=/bin/bash /data/home/yugabyte/bin/collect_metrics_wrapper.sh

[Install]'
+ chown yugabyte:yugabyte /tmp/tmp.bjTyZtC3Vo
+ su - yugabyte -c 'mv /tmp/tmp.bjTyZtC3Vo /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.service'
++ mktemp
+ tmp_file=/tmp/tmp.dCvidmJkgp
+ echo '[Unit]
Description=Yugabyte collect metrics

[Timer]


Unit=yb-collect_metrics.service
# Run every 1 minute
OnCalendar=*:0/1:0

[Install]
WantedBy=timers.target'
+ chown yugabyte:yugabyte /tmp/tmp.dCvidmJkgp
+ su - yugabyte -c 'mv /tmp/tmp.dCvidmJkgp /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.timer'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.timer'
++ mktemp
+ tmp_file=/tmp/tmp.sNuAKN18Ct
+ echo '[Unit]
Description=Yugabyte Controller
Requires=network-online.target
After=network.target network-online.target multi-user.target
# Disable restart limits, using RestartSec to rate limit restarts
StartLimitInterval=0

[Path]
PathExists=/data/home/yugabyte/controller/bin/yb-controller-server
PathExists=/data/home/yugabyte/controller/conf/server.conf

[Service]
# Start


ExecStart=/data/home/yugabyte/controller/bin/yb-controller-server     --flagfile /data/home/yugabyte/controller/conf/server.conf
Restart=always
RestartSec=5
# Stop -> SIGTERM - 10s - SIGKILL (if not stopped) [matches existing cron behavior]
KillMode=control-group
TimeoutStopFailureMode=terminate
KillSignal=SIGTERM
TimeoutStopSec=10
FinalKillSignal=SIGKILL
# Logs
StandardOutput=syslog
StandardError=syslog
# ulimit
LimitCORE=infinity
LimitNOFILE=1048576
LimitNPROC=12000

[Install]
WantedBy=default.target'
+ chown yugabyte:yugabyte /tmp/tmp.sNuAKN18Ct
+ su - yugabyte -c 'mv /tmp/tmp.sNuAKN18Ct /data/home/yugabyte/.config/systemd/user/yb-controller.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-controller.service'
++ mktemp
+ tmp_file=/tmp/tmp.7BQypIs0N5
+ echo '[Unit]
Description=Yugabyte logs

[Service]


Type=oneshot
WorkingDirectory=/data/home/yugabyte/bin
ExecStart=/bin/bash /data/home/yugabyte/bin/zip_purge_yb_logs.sh

[Install]
WantedBy=multi-user.target'
+ chown yugabyte:yugabyte /tmp/tmp.7BQypIs0N5
+ su - yugabyte -c 'mv /tmp/tmp.7BQypIs0N5 /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.service'
++ mktemp
+ tmp_file=/tmp/tmp.saV1vbvBbW
+ echo '[Unit]
Description=Yugabyte logs

[Timer]


Unit=yb-zip_purge_yb_logs.service
# Run every 5 minutes
OnCalendar=*:0/5

[Install]
WantedBy=timers.target'
+ chown yugabyte:yugabyte /tmp/tmp.saV1vbvBbW
+ su - yugabyte -c 'mv /tmp/tmp.saV1vbvBbW /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.timer'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.timer'
+ grep -qP '^\*\s+-\s+core\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       core            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+data\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       data            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+fsize\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       fsize            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+sigpending\s+119934$' /etc/security/limits.conf
+ echo '*                -       sigpending            119934'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+memlock\s+64$' /etc/security/limits.conf
+ echo '*                -       memlock            64'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+rss\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       rss            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+nofile\s+1048576$' /etc/security/limits.conf
+ echo '*                -       nofile            1048576'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+msgqueue\s+819200$' /etc/security/limits.conf
+ echo '*                -       msgqueue            819200'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+stack\s+8192$' /etc/security/limits.conf
+ echo '*                -       stack            8192'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+cpu\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       cpu            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+nproc\s+12000$' /etc/security/limits.conf
+ echo '*                -       nproc            12000'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+locks\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       locks            unlimited'
+ tee -a /etc/security/limits.conf
+ test '!' -f /etc/security/limits.d/20-nproc.conf
+ touch /etc/security/limits.d/20-nproc.conf
+ echo 'File /etc/security/limits.d/20-nproc.conf created.'
+ sed -i 's/*          soft    nproc     4096/*          soft    nproc     \
12000/' /etc/security/limits.d/20-nproc.conf
++ grep -oP '(?<=^PLATFORM_ID=).+' /etc/os-release
++ tr -d '"'
+ platform_id=platform:el9
+ [[ platform:el9 == \p\l\a\t\f\o\r\m\:\e\l\8 ]]
+ echo 'el8 not detected, not updating file limits'
+ echo 'Systemd file limits configured.'
+ systemctl daemon-reload
+ ulimit -n 1048576
+ ulimit -u 12000
+ grep -q '^vm.swappiness=' /etc/sysctl.conf
+ echo vm.swappiness=0
+ sysctl -w vm.swappiness=0
+ grep -q '^kernel.core_pattern=' /etc/sysctl.conf
+ echo kernel.core_pattern=/data/home/yugabyte/cores/core_%p_%t_%E
+ sysctl -w kernel.core_pattern=/data/home/yugabyte/cores/core_%p_%t_%E
+ grep -q '^vm.max_map_count=' /etc/sysctl.conf
+ echo vm.max_map_count=262144
+ sysctl -w vm.max_map_count=262144
+ echo 'Kernel settings configured.'
+ echo 'OS Configuration applied successfully.'
+ thirdparty_dir=/data/2024.2.2.2-b2/scripts/ynp/../../thirdparty
+ mkdir -p /opt/prometheus
+ mkdir -p /etc/prometheus
+ mkdir -p /var/log/prometheus
+ mkdir -p /var/run/prometheus
+ mkdir -p /tmp/yugabyte/metrics
+ '[' -f /data/2024.2.2.2-b2/scripts/ynp/../../thirdparty/node_exporter-1.7.0.linux-amd64.tar.gz ']'
+ cp /data/2024.2.2.2-b2/scripts/ynp/../../thirdparty/node_exporter-1.7.0.linux-amd64.tar.gz /opt/prometheus
+ id -u prometheus
++ grep -oP '(?<=^ID=).+' /etc/os-release
++ tr -d '"'
+ os_type=rhel
+ case "$os_type" in
+ useradd --shell /bin/bash --no-create-home -g yugabyte prometheus
+ chown -R prometheus:yugabyte /opt/prometheus
+ chown -R prometheus:yugabyte /etc/prometheus
+ chown -R prometheus:yugabyte /var/log/prometheus
+ chown -R prometheus:yugabyte /var/run/prometheus
+ chown -R yugabyte:yugabyte /tmp/yugabyte/metrics
+ chmod -R 755 /tmp/yugabyte/metrics
+ '[' -f /opt/prometheus/node_exporter-1.7.0.linux-amd64.tar.gz ']'
+ chmod +r /opt/prometheus/node_exporter-1.7.0.linux-amd64.tar.gz
+ su - prometheus -c 'cd /opt/prometheus && tar zxf node_exporter-1.7.0.linux-amd64.tar.gz'
su: warning: cannot change directory to /home/prometheus: No such file or directory
+ file_path=/etc/systemd/system/node_exporter.service
+ cat
+ systemctl daemon-reload
+ systemctl enable node_exporter
Created symlink /etc/systemd/system/multi-user.target.wants/node_exporter.service â†’ /etc/systemd/system/node_exporter.service.
+ systemctl start node_exporter
+ echo 'Node exporter setup is complete.'
+ echo 'Start network configuration'
+ echo 'Nothing to be done'
+ echo 'End network configuration'
+ yba_url=https://35.184.240.7
+ customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
+ token=3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
+ provider_id=2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ skip_tls_verify https://35.184.240.7
++ local yba_url=https://35.184.240.7
++ [[ https://35.184.240.7 == https* ]]
++ echo --insecure
+ tls_verify_option=--insecure
+ readarray -t headers
++ _get_headers 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
++ local token=3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
++ echo 'Accept: application/json'
++ echo 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb'
++ echo 'Content-Type: application/json'
+ header_options=()
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ test -f /tmp/create_provider.json
+ test -f /tmp/update_provider.json
+ test -f /tmp/create_instance.json
+ airgap_flag=
+ installer_dir=/data/home/yugabyte/.install
+ su - yugabyte -c '"/data/home/yugabyte/.install/node-agent-installer.sh" -c install -u https://35.184.240.7 -t 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb --provider_id 2dc133e8-9482-4f77-ab4d-4f617ebbfa28 --instance_type n2 --zone_name us-central1-a --node_name yb-dev-tu-n1 --region_name us-central1 --node_ip 10.128.0.2 --bind_ip 10.128.0.2 --silent --skip_verify_cert '
+ /data/home/yugabyte/.install/node-agent-installer.sh -c install_service --user yugabyte
Created symlink /etc/systemd/system/multi-user.target.wants/yb-node-agent.service â†’ /etc/systemd/system/yb-node-agent.service.
+ test -f /tmp/add_node_to_provider.json
++ _get_nodes_in_provider https://35.184.240.7 cf6d0b42-bae0-49f2-8fa1-28a3f9216a77 2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ local yba_url=https://35.184.240.7
++ local customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
++ local provider_uuid=2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ echo https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
+ get_nodes_in_provider=https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
++ curl -s -w '%{http_code}' -o response.txt -X GET -H 'Accept: application/json' -H 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb' -H 'Content-Type: application/json' --insecure https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
+ response=200
+ http_status=200
+ response_body='[{"nodeUuid":"8dc3bcc1-fb03-42ec-9903-a07c3983e0d0","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n3","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.3","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n3"},"detailsJson":"{\"ip\":\"10.128.0.3\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n3\"}","inUse":true},{"nodeUuid":"716b9c3c-3743-41c7-b2b8-87d92f96bf83","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n2","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.2","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n2"},"detailsJson":"{\"ip\":\"10.128.0.2\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n2\"}","inUse":true},{"nodeUuid":"ed729a1c-89d8-44cf-8031-110cddfdd47f","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n1","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.15.231","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n1"},"detailsJson":"{\"ip\":\"10.128.15.231\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n1\"}","inUse":true}]'
+ matched=false
+ '[' 200 -ge 200 ']'
+ '[' 200 -lt 300 ']'
+ echo 'HTTP GET request successful. Processing response...'
++ echo '[{"nodeUuid":"8dc3bcc1-fb03-42ec-9903-a07c3983e0d0","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n3","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.3","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n3"},"detailsJson":"{\"ip\":\"10.128.0.3\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n3\"}","inUse":true},{"nodeUuid":"716b9c3c-3743-41c7-b2b8-87d92f96bf83","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n2","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.2","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n2"},"detailsJson":"{\"ip\":\"10.128.0.2\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n2\"}","inUse":true},{"nodeUuid":"ed729a1c-89d8-44cf-8031-110cddfdd47f","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n1","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.15.231","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n1"},"detailsJson":"{\"ip\":\"10.128.15.231\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n1\"}","inUse":true}]'
++ sed -n 's/.*"ip":"\([a-zA-Z0-9\.-]*\)".*/\1/p'
+ ips=10.128.15.231
+ for ip in $ips
+ [[ 10.128.15.231 == \1\0\.\1\2\8\.\0\.\2 ]]
+ [[ false == false ]]
++ _get_provider_url https://35.184.240.7 cf6d0b42-bae0-49f2-8fa1-28a3f9216a77 2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ local yba_url=https://35.184.240.7
++ local customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
++ local provider_uuid=2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ echo https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28
+ get_provider_endpoint=https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ curl -s -w '%{http_code}' -o response.txt -X GET -H 'Accept: application/json' -H 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb' -H 'Content-Type: application/json' --insecure https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28
+ response=200
+ http_status=200
+ response_body='{"uuid":"2dc133e8-9482-4f77-ab4d-4f617ebbfa28","code":"onprem","name":"onPrem","active":true,"customerUUID":"cf6d0b42-bae0-49f2-8fa1-28a3f9216a77","config":{"YB_HOME_DIR":"/data/home/yugabyte"},"details":{"sshPort":22,"airGapInstall":false,"passwordlessSudoAccess":true,"provisionInstanceScript":"","installNodeExporter":true,"nodeExporterPort":9300,"nodeExporterUser":"prometheus","skipProvisioning":true,"setUpChrony":false,"ntpServers":[],"showSetUpChrony":false,"cloudInfo":{"onprem":{"ybHomeDir":"/data/home/yugabyte"}},"enableNodeAgent":true},"regions":[{"uuid":"435ead87-b36e-4139-bc89-34943671398e","code":"us-central1","name":"us-central1","longitude":0.0,"latitude":0.0,"zones":[{"uuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","code":"us-central1-a","name":"us-central1-a","active":true,"details":{"cloudInfo":{}}}],"active":true,"details":{"cloudInfo":{}}}],"imageBundles":[],"allAccessKeys":[],"version":3,"usabilityState":"READY","airGapInstall":false,"sshPort":22}'
+ '[' 200 -ge 200 ']'
+ '[' 200 -lt 300 ']'
+ echo 'HTTP GET request successful. Processing response...'
+ zone_uuid=
+ echo '{"uuid":"2dc133e8-9482-4f77-ab4d-4f617ebbfa28","code":"onprem","name":"onPrem","active":true,"customerUUID":"cf6d0b42-bae0-49f2-8fa1-28a3f9216a77","config":{"YB_HOME_DIR":"/data/home/yugabyte"},"details":{"sshPort":22,"airGapInstall":false,"passwordlessSudoAccess":true,"provisionInstanceScript":"","installNodeExporter":true,"nodeExporterPort":9300,"nodeExporterUser":"prometheus","skipProvisioning":true,"setUpChrony":false,"ntpServers":[],"showSetUpChrony":false,"cloudInfo":{"onprem":{"ybHomeDir":"/data/home/yugabyte"}},"enableNodeAgent":true},"regions":[{"uuid":"435ead87-b36e-4139-bc89-34943671398e","code":"us-central1","name":"us-central1","longitude":0.0,"latitude":0.0,"zones":[{"uuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","code":"us-central1-a","name":"us-central1-a","active":true,"details":{"cloudInfo":{}}}],"active":true,"details":{"cloudInfo":{}}}],"imageBundles":[],"allAccessKeys":[],"version":3,"usabilityState":"READY","airGapInstall":false,"sshPort":22}'
+ read -r zone
++ grep -oP '"zones":\s*\[[^]]*\]' temp_response.txt
++ grep -oP '{[^}]*}'
++ echo '{"uuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","code":"us-central1-a","name":"us-central1-a","active":true,"details":{"cloudInfo":{}'
++ grep -oP '"code":\s*"\K[^"]+'
+ zone_code=us-central1-a
++ echo '{"uuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","code":"us-central1-a","name":"us-central1-a","active":true,"details":{"cloudInfo":{}'
++ grep -oP '"uuid":\s*"\K[^"]+'
+ uuid=559eb98c-4f80-4304-ac69-a89a36fb2a4f
+ '[' us-central1-a == us-central1-a ']'
+ zone_uuid=559eb98c-4f80-4304-ac69-a89a36fb2a4f
+ echo 'Match found: Zone Code = us-central1-a, UUID = 559eb98c-4f80-4304-ac69-a89a36fb2a4f'
+ break
+ '[' -z 559eb98c-4f80-4304-ac69-a89a36fb2a4f ']'
++ cat /tmp/add_node_to_provider.json
+ add_node_data='{
    "nodes": [
        {
            "instanceType": "n2",
            "ip": "10.128.0.2",
            "region": "us-central1",
            "zone": "us-central1-a",
            "nodeName": "yb-dev-tu-n1",
            "instanceName": "n2"
        }
    ]
}'
++ _add_node_to_provider https://35.184.240.7 cf6d0b42-bae0-49f2-8fa1-28a3f9216a77 559eb98c-4f80-4304-ac69-a89a36fb2a4f
++ local yba_url=https://35.184.240.7
++ local customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
++ local zone_uuid=559eb98c-4f80-4304-ac69-a89a36fb2a4f
++ echo https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/zones/559eb98c-4f80-4304-ac69-a89a36fb2a4f/nodes
+ add_node_url=https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/zones/559eb98c-4f80-4304-ac69-a89a36fb2a4f/nodes
++ curl -s -w '%{http_code}' -o response.txt -X POST -H 'Accept: application/json' -H 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb' -H 'Content-Type: application/json' -d '{
    "nodes": [
        {
            "instanceType": "n2",
            "ip": "10.128.0.2",
            "region": "us-central1",
            "zone": "us-central1-a",
            "nodeName": "yb-dev-tu-n1",
            "instanceName": "n2"
        }
    ]
}' --insecure https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/zones/559eb98c-4f80-4304-ac69-a89a36fb2a4f/nodes
+ response=400
+ http_status=400
+ '[' 400 -ge 200 ']'
+ '[' 400 -lt 300 ']'
+ echo 'Error: POST request failed with HTTP status 400'
+ exit 1

2025-06-02 14:50:09,594 - commands.provision_command - INFO - Return Code: 1
2025-06-02 14:50:52,211 - commands.provision_command - INFO - Output: 200 OK
System clock synchronized
User yugabyte exists
/data/home/yugabyte has the correct ownership and acceptable permissions
[PASS] Directory /data is owned by yugabyte or has '7' (rwx) permissions.
[PASS] Sufficient disk space available: 196G
Systemd unit yb-tserver.service is configured.
Systemd unit yb-master.service is configured.
Systemd unit clock-sync.sh.j2 is configured.
Systemd unit yb-bind_check.service is configured.
Systemd unit yb-clean_cores.service is configured.
Systemd unit yb-clean_cores.timer is configured.
Systemd unit yb-collect_metrics.service is configured.
Systemd unit yb-collect_metrics.timer is configured.
Systemd unit yb-controller.service is configured.
Systemd unit yb-zip_purge_yb_logs.service is configured.
Systemd unit yb-zip_purge_yb_logs.timer is configured.
[PASS] vm.swappiness is set to 0 (expected: 0)
[PASS] vm.max_map_count is set to 262144 (expected: 262144)
[PASS] kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E
"node_exporter.service" is active
Port 7000 is open on 10.128.0.2
Port 7100 is open on 10.128.0.2
Port 9000 is open on 10.128.0.2
Port 9100 is open on 10.128.0.2
Port 18018 is open on 10.128.0.2
Port 22 is open on 10.128.0.2
Port 5433 is open on 10.128.0.2
Port 9042 is open on 10.128.0.2
Port 9070 is open on 10.128.0.2
Port 9300 is open on 10.128.0.2
Port 12000 is open on 10.128.0.2
Port 13000 is open on 10.128.0.2
"yb-node-agent.service" is active
MemoryCurrent is greater than 0: 10305536
HTTP GET request successful. Processing response...
Node addition to the provider passed: 10.128.0.2
Marker file does not exist: /tmp/.reboot-required
Reboot requirement cleared.
{
"results":[
    {
      "check": "Clock Synchronization",
      "result": "PASS",
      "message": "System clock synchronized"
    },
    {
      "check": "Yugabyte User Existence Check",
      "result": "PASS",
      "message": "User yugabyte exists"
    },
    {
      "check": "Yugabyte Home Directory Permissions Check",
      "result": "PASS",
      "message": "/data/home/yugabyte has the correct ownership and acceptable permissions"
    },
    {
      "check": "/data Ownership/Permission Check",
      "result": "PASS",
      "message": "Directory /data is owned by yugabyte or has '7' (rwx) permissions."
    },
    {
      "check": "/data Free Space Check",
      "result": "PASS",
      "message": "Sufficient disk space available: 196G"
    },
    {
      "check": "Systemd Unit File Check - yb-tserver.service",
      "result": "PASS",
      "message": "Systemd unit yb-tserver.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-master.service",
      "result": "PASS",
      "message": "Systemd unit yb-master.service is configured."
    },
    {
      "check": "Systemd Unit File Check - clock-sync.sh.j2",
      "result": "PASS",
      "message": "Systemd unit clock-sync.sh.j2 is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-bind_check.service",
      "result": "PASS",
      "message": "Systemd unit yb-bind_check.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.service",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.timer",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.service",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.timer",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-controller.service",
      "result": "PASS",
      "message": "Systemd unit yb-controller.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.service",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.timer",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.timer is configured."
    },
    {
      "check": "vm.swappiness",
      "result": "PASS",
      "message": "vm.swappiness is set to 0 (expected: 0)"
    },
    {
      "check": "vm.max_map_count",
      "result": "PASS",
      "message": "vm.max_map_count is set to 262144 (expected: 262144)"
    },
    {
      "check": "kernel.core_pattern",
      "result": "PASS",
      "message": "kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "node_exporter.service is active"
    },
    {
      "check": "Port 7000 Check",
      "result": "PASS",
      "message": "Port 7000 is open on 10.128.0.2"
    },
    {
      "check": "Port 7100 Check",
      "result": "PASS",
      "message": "Port 7100 is open on 10.128.0.2"
    },
    {
      "check": "Port 9000 Check",
      "result": "PASS",
      "message": "Port 9000 is open on 10.128.0.2"
    },
    {
      "check": "Port 9100 Check",
      "result": "PASS",
      "message": "Port 9100 is open on 10.128.0.2"
    },
    {
      "check": "Port 18018 Check",
      "result": "PASS",
      "message": "Port 18018 is open on 10.128.0.2"
    },
    {
      "check": "Port 22 Check",
      "result": "PASS",
      "message": "Port 22 is open on 10.128.0.2"
    },
    {
      "check": "Port 5433 Check",
      "result": "PASS",
      "message": "Port 5433 is open on 10.128.0.2"
    },
    {
      "check": "Port 9042 Check",
      "result": "PASS",
      "message": "Port 9042 is open on 10.128.0.2"
    },
    {
      "check": "Port 9070 Check",
      "result": "PASS",
      "message": "Port 9070 is open on 10.128.0.2"
    },
    {
      "check": "Port 9300 Check",
      "result": "PASS",
      "message": "Port 9300 is open on 10.128.0.2"
    },
    {
      "check": "Port 12000 Check",
      "result": "PASS",
      "message": "Port 12000 is open on 10.128.0.2"
    },
    {
      "check": "Port 13000 Check",
      "result": "PASS",
      "message": "Port 13000 is open on 10.128.0.2"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "yb-node-agent.service is active"
    },
    {
      "check": "Memory Usage Check",
      "result": "PASS",
      "message": "MemoryCurrent is greater than 0: 10305536"
    },
    {
      "check": "Node addition failed",
      "result": "PASS",
      "message": "Node addition to the provider passed: 10.128.0.2"
    },
    {
      "check": "REBOOT REQUIRED",
      "result": "PASS",
      "message": "No Marker file present"
    }
]}
Pre-flight checks successful

2025-06-02 14:50:52,211 - commands.provision_command - INFO - Error: + json_results='{
"results":[
'
+ SUDO_ACCESS=false
++ id -u
+ '[' 0 = 0 ']'
+ SUDO_ACCESS=true
+ '[' true = true ']'
+ chronyc makestep
+ '[' 0 -eq 0 ']'
+ add_result 'Clock Synchronization' PASS 'System clock synchronized'
+ local 'check=Clock Synchronization'
+ local result=PASS
+ local 'message=System clock synchronized'
+ '[' 14 -gt 20 ']'
+ json_results+='    {
'
+ json_results+='      "check": "Clock Synchronization",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "System clock synchronized"
'
+ json_results+='    }'
+ echo 'System clock synchronized'
+ id yugabyte
+ echo 'User yugabyte exists'
+ add_result 'Yugabyte User Existence Check' PASS 'User yugabyte exists'
+ local 'check=Yugabyte User Existence Check'
+ local result=PASS
+ local 'message=User yugabyte exists'
+ '[' 134 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Yugabyte User Existence Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "User yugabyte exists"
'
+ json_results+='    }'
+ '[' -d /data/home/yugabyte ']'
++ stat -c %U /data/home/yugabyte
+ owner=yugabyte
++ stat -c %G /data/home/yugabyte
+ group=yugabyte
++ stat -c %a /data/home/yugabyte
+ permissions=750
+ '[' yugabyte '!=' yugabyte ']'
+ '[' yugabyte '!=' yugabyte ']'
+ '[' 750 -lt 711 ']'
+ echo '/data/home/yugabyte has the correct ownership and acceptable permissions'
+ add_result 'Yugabyte Home Directory Permissions Check' PASS '/data/home/yugabyte has the correct ownership and acceptable permissions'
+ local 'check=Yugabyte Home Directory Permissions Check'
+ local result=PASS
+ local 'message=/data/home/yugabyte has the correct ownership and acceptable permissions'
+ '[' 259 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Yugabyte Home Directory Permissions Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "/data/home/yugabyte has the correct ownership and acceptable permissions"
'
+ json_results+='    }'
+ yb_home_dir=/data/home/yugabyte
+ threshold=49
+ mount_points=/data
+ IFS=' '
+ read -ra mount_points_array
+ for mount_point in "${mount_points_array[@]}"
+ is_subdirectory /data /data/home/yugabyte
+ local dir=/data
+ local parent=/data/home/yugabyte
++ realpath -m /data
+ dir=/data
++ realpath -m /data/home/yugabyte
+ parent=/data/home/yugabyte
+ case "$dir/" in
+ return 1
+ '[' -d /data ']'
++ stat -c %U /data
+ owner_check=root
++ stat -c %A /data
++ cut -c 2-4
+ perm_check=rwx
+ '[' root == yugabyte ']'
+ [[ rwx == \r\w\x ]]
+ result=PASS
+ message='Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions.'
+ echo '[PASS] Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions.'
+ add_result '/data Ownership/Permission Check' PASS 'Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions.'
+ local 'check=/data Ownership/Permission Check'
+ local result=PASS
+ local 'message=Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions.'
+ '[' 448 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "/data Ownership/Permission Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions."
'
+ json_results+='    }'
+ '[' -d /data ']'
++ df -BG --output=avail /data
++ tail -n 1
++ tr -d 'G '
+ free_space_gb=196
+ '[' 196 -gt 49 ']'
+ result=PASS
+ message='Sufficient disk space available: 196G'
+ echo '[PASS] Sufficient disk space available: 196G'
+ add_result '/data Free Space Check' PASS 'Sufficient disk space available: 196G'
+ local 'check=/data Free Space Check'
+ local result=PASS
+ local 'message=Sufficient disk space available: 196G'
+ '[' 622 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "/data Free Space Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Sufficient disk space available: 196G"
'
+ json_results+='    }'
+ systemd_dir=/etc/systemd/system
+ systemd_dir=/data/home/yugabyte/.config/systemd/user
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-tserver.service ']'
+ echo 'Systemd unit yb-tserver.service is configured.'
+ add_result 'Systemd Unit File Check - yb-tserver.service' PASS 'Systemd unit yb-tserver.service is configured.'
+ local 'check=Systemd Unit File Check - yb-tserver.service'
+ local result=PASS
+ local 'message=Systemd unit yb-tserver.service is configured.'
+ '[' 757 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-tserver.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-tserver.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-master.service ']'
+ echo 'Systemd unit yb-master.service is configured.'
+ add_result 'Systemd Unit File Check - yb-master.service' PASS 'Systemd unit yb-master.service is configured.'
+ local 'check=Systemd Unit File Check - yb-master.service'
+ local result=PASS
+ local 'message=Systemd unit yb-master.service is configured.'
+ '[' 923 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-master.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-master.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/clock-sync.sh.j2 ']'
+ echo 'Systemd unit clock-sync.sh.j2 is configured.'
+ add_result 'Systemd Unit File Check - clock-sync.sh.j2' PASS 'Systemd unit clock-sync.sh.j2 is configured.'
+ local 'check=Systemd Unit File Check - clock-sync.sh.j2'
+ local result=PASS
+ local 'message=Systemd unit clock-sync.sh.j2 is configured.'
+ '[' 1087 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - clock-sync.sh.j2",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit clock-sync.sh.j2 is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-bind_check.service ']'
+ echo 'Systemd unit yb-bind_check.service is configured.'
+ add_result 'Systemd Unit File Check - yb-bind_check.service' PASS 'Systemd unit yb-bind_check.service is configured.'
+ local 'check=Systemd Unit File Check - yb-bind_check.service'
+ local result=PASS
+ local 'message=Systemd unit yb-bind_check.service is configured.'
+ '[' 1249 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-bind_check.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-bind_check.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-clean_cores.service ']'
+ echo 'Systemd unit yb-clean_cores.service is configured.'
+ add_result 'Systemd Unit File Check - yb-clean_cores.service' PASS 'Systemd unit yb-clean_cores.service is configured.'
+ local 'check=Systemd Unit File Check - yb-clean_cores.service'
+ local result=PASS
+ local 'message=Systemd unit yb-clean_cores.service is configured.'
+ '[' 1421 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-clean_cores.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-clean_cores.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-clean_cores.timer ']'
+ echo 'Systemd unit yb-clean_cores.timer is configured.'
+ add_result 'Systemd Unit File Check - yb-clean_cores.timer' PASS 'Systemd unit yb-clean_cores.timer is configured.'
+ local 'check=Systemd Unit File Check - yb-clean_cores.timer'
+ local result=PASS
+ local 'message=Systemd unit yb-clean_cores.timer is configured.'
+ '[' 1595 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-clean_cores.timer",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-clean_cores.timer is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.service ']'
+ echo 'Systemd unit yb-collect_metrics.service is configured.'
+ add_result 'Systemd Unit File Check - yb-collect_metrics.service' PASS 'Systemd unit yb-collect_metrics.service is configured.'
+ local 'check=Systemd Unit File Check - yb-collect_metrics.service'
+ local result=PASS
+ local 'message=Systemd unit yb-collect_metrics.service is configured.'
+ '[' 1765 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-collect_metrics.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-collect_metrics.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.timer ']'
+ echo 'Systemd unit yb-collect_metrics.timer is configured.'
+ add_result 'Systemd Unit File Check - yb-collect_metrics.timer' PASS 'Systemd unit yb-collect_metrics.timer is configured.'
+ local 'check=Systemd Unit File Check - yb-collect_metrics.timer'
+ local result=PASS
+ local 'message=Systemd unit yb-collect_metrics.timer is configured.'
+ '[' 1947 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-collect_metrics.timer",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-collect_metrics.timer is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-controller.service ']'
+ echo 'Systemd unit yb-controller.service is configured.'
+ add_result 'Systemd Unit File Check - yb-controller.service' PASS 'Systemd unit yb-controller.service is configured.'
+ local 'check=Systemd Unit File Check - yb-controller.service'
+ local result=PASS
+ local 'message=Systemd unit yb-controller.service is configured.'
+ '[' 2125 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-controller.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-controller.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.service ']'
+ echo 'Systemd unit yb-zip_purge_yb_logs.service is configured.'
+ add_result 'Systemd Unit File Check - yb-zip_purge_yb_logs.service' PASS 'Systemd unit yb-zip_purge_yb_logs.service is configured.'
+ local 'check=Systemd Unit File Check - yb-zip_purge_yb_logs.service'
+ local result=PASS
+ local 'message=Systemd unit yb-zip_purge_yb_logs.service is configured.'
+ '[' 2297 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-zip_purge_yb_logs.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.timer ']'
+ echo 'Systemd unit yb-zip_purge_yb_logs.timer is configured.'
+ add_result 'Systemd Unit File Check - yb-zip_purge_yb_logs.timer' PASS 'Systemd unit yb-zip_purge_yb_logs.timer is configured.'
+ local 'check=Systemd Unit File Check - yb-zip_purge_yb_logs.timer'
+ local result=PASS
+ local 'message=Systemd unit yb-zip_purge_yb_logs.timer is configured.'
+ '[' 2483 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.timer",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-zip_purge_yb_logs.timer is configured."
'
+ json_results+='    }'
++ grep -oP '(?<=^PLATFORM_ID=).+' /etc/os-release
++ tr -d '"'
+ platform_id=platform:el9
+ [[ platform:el9 == \p\l\a\t\f\o\r\m\:\e\l\8 ]]
+ verify_sysctl vm.swappiness 0
+ local param=vm.swappiness
+ local expected_value=0
+ local current_value
++ sysctl -n vm.swappiness
+ current_value=0
+ '[' 0 -eq 0 ']'
+ echo '[PASS] vm.swappiness is set to 0 (expected: 0)'
+ add_result vm.swappiness PASS 'vm.swappiness is set to 0 (expected: 0)'
+ local check=vm.swappiness
+ local result=PASS
+ local 'message=vm.swappiness is set to 0 (expected: 0)'
+ '[' 2665 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "vm.swappiness",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "vm.swappiness is set to 0 (expected: 0)"
'
+ json_results+='    }'
+ verify_sysctl vm.max_map_count 262144
+ local param=vm.max_map_count
+ local expected_value=262144
+ local current_value
++ sysctl -n vm.max_map_count
+ current_value=262144
+ '[' 262144 -eq 262144 ']'
+ echo '[PASS] vm.max_map_count is set to 262144 (expected: 262144)'
+ add_result vm.max_map_count PASS 'vm.max_map_count is set to 262144 (expected: 262144)'
+ local check=vm.max_map_count
+ local result=PASS
+ local 'message=vm.max_map_count is set to 262144 (expected: 262144)'
+ '[' 2793 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "vm.max_map_count",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "vm.max_map_count is set to 262144 (expected: 262144)"
'
+ json_results+='    }'
++ sysctl -n kernel.core_pattern
+ kernel_core_pattern_value=/data/home/yugabyte/cores/core_%p_%t_%E
+ '[' /data/home/yugabyte/cores/core_%p_%t_%E == /data/home/yugabyte/cores/core_%p_%t_%E ']'
+ echo '[PASS] kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E'
+ add_result kernel.core_pattern PASS 'kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E'
+ local check=kernel.core_pattern
+ local result=PASS
+ local 'message=kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E'
+ '[' 2937 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "kernel.core_pattern",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E"
'
+ json_results+='    }'
++ grep -oP '(?<=^PLATFORM_ID=).+' /etc/os-release
++ tr -d '"'
+ platform_id=platform:el9
+ [[ platform:el9 == \p\l\a\t\f\o\r\m\:\e\l\8 ]]
+ service_name=node_exporter.service
++ systemctl show -p ActiveState node_exporter.service
++ grep ActiveState
++ awk -F= '{print $2}'
+ status=active
+ '[' active = active ']'
+ echo '"node_exporter.service" is active'
+ add_result 'Service Status Check' PASS 'node_exporter.service is active'
+ local 'check=Service Status Check'
+ local result=PASS
+ local 'message=node_exporter.service is active'
+ '[' 3101 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Service Status Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "node_exporter.service is active"
'
+ json_results+='    }'
+ ports=(7000 7100 9000 9100 18018 22 5433 9042 9070 9300 12000 13000)
+ vm_ip=10.128.0.2
+ for port in "${ports[@]}"
++ start_server 7000
++ local port=7000
++ echo 5616
++ python3 -m http.server 7000
+ server_pid=5616
+ sleep 2
+ check_port 10.128.0.2 7000
+ local ip=10.128.0.2
+ local port=7000
+ '[' 0 -eq 0 ']'
+ echo 'Port 7000 is open on 10.128.0.2'
+ add_result 'Port 7000 Check' PASS 'Port 7000 is open on 10.128.0.2'
+ local 'check=Port 7000 Check'
+ local result=PASS
+ local 'message=Port 7000 is open on 10.128.0.2'
+ '[' 3228 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 7000 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 7000 is open on 10.128.0.2"
'
+ json_results+='    }'
+ ps -p 5616
+ force_kill 5616
+ local pid=5616
+ kill 5616
+ sleep 2
+ ps -p 5616
+ for port in "${ports[@]}"
++ start_server 7100
++ local port=7100
++ echo 5624
++ python3 -m http.server 7100
+ server_pid=5624
+ sleep 2
+ check_port 10.128.0.2 7100
+ local ip=10.128.0.2
+ local port=7100
+ '[' 0 -eq 0 ']'
+ echo 'Port 7100 is open on 10.128.0.2'
+ add_result 'Port 7100 Check' PASS 'Port 7100 is open on 10.128.0.2'
+ local 'check=Port 7100 Check'
+ local result=PASS
+ local 'message=Port 7100 is open on 10.128.0.2'
+ '[' 3350 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 7100 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 7100 is open on 10.128.0.2"
'
+ json_results+='    }'
+ ps -p 5624
+ force_kill 5624
+ local pid=5624
+ kill 5624
+ sleep 2
+ ps -p 5624
+ for port in "${ports[@]}"
++ start_server 9000
++ local port=9000
++ echo 5952
++ python3 -m http.server 9000
+ server_pid=5952
+ sleep 2
+ check_port 10.128.0.2 9000
+ local ip=10.128.0.2
+ local port=9000
+ '[' 0 -eq 0 ']'
+ echo 'Port 9000 is open on 10.128.0.2'
+ add_result 'Port 9000 Check' PASS 'Port 9000 is open on 10.128.0.2'
+ local 'check=Port 9000 Check'
+ local result=PASS
+ local 'message=Port 9000 is open on 10.128.0.2'
+ '[' 3472 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9000 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9000 is open on 10.128.0.2"
'
+ json_results+='    }'
+ ps -p 5952
+ force_kill 5952
+ local pid=5952
+ kill 5952
+ sleep 2
+ ps -p 5952
+ for port in "${ports[@]}"
++ start_server 9100
++ local port=9100
++ echo 5961
++ python3 -m http.server 9100
+ server_pid=5961
+ sleep 2
+ check_port 10.128.0.2 9100
+ local ip=10.128.0.2
+ local port=9100
+ '[' 0 -eq 0 ']'
+ echo 'Port 9100 is open on 10.128.0.2'
+ add_result 'Port 9100 Check' PASS 'Port 9100 is open on 10.128.0.2'
+ local 'check=Port 9100 Check'
+ local result=PASS
+ local 'message=Port 9100 is open on 10.128.0.2'
+ '[' 3594 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9100 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9100 is open on 10.128.0.2"
'
+ json_results+='    }'
+ ps -p 5961
+ force_kill 5961
+ local pid=5961
+ kill 5961
+ sleep 2
+ ps -p 5961
+ for port in "${ports[@]}"
++ start_server 18018
++ local port=18018
++ echo 6075
++ python3 -m http.server 18018
+ server_pid=6075
+ sleep 2
+ check_port 10.128.0.2 18018
+ local ip=10.128.0.2
+ local port=18018
+ '[' 0 -eq 0 ']'
+ echo 'Port 18018 is open on 10.128.0.2'
+ add_result 'Port 18018 Check' PASS 'Port 18018 is open on 10.128.0.2'
+ local 'check=Port 18018 Check'
+ local result=PASS
+ local 'message=Port 18018 is open on 10.128.0.2'
+ '[' 3716 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 18018 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 18018 is open on 10.128.0.2"
'
+ json_results+='    }'
+ ps -p 6075
+ force_kill 6075
+ local pid=6075
+ kill 6075
+ sleep 2
+ ps -p 6075
+ for port in "${ports[@]}"
++ start_server 22
++ local port=22
++ echo 6083
++ python3 -m http.server 22
+ server_pid=6083
+ sleep 2
+ check_port 10.128.0.2 22
+ local ip=10.128.0.2
+ local port=22
+ '[' 0 -eq 0 ']'
+ echo 'Port 22 is open on 10.128.0.2'
+ add_result 'Port 22 Check' PASS 'Port 22 is open on 10.128.0.2'
+ local 'check=Port 22 Check'
+ local result=PASS
+ local 'message=Port 22 is open on 10.128.0.2'
+ '[' 3840 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 22 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 22 is open on 10.128.0.2"
'
+ json_results+='    }'
+ ps -p 6083
+ for port in "${ports[@]}"
++ start_server 5433
++ local port=5433
++ echo 6089
++ python3 -m http.server 5433
+ server_pid=6089
+ sleep 2
+ check_port 10.128.0.2 5433
+ local ip=10.128.0.2
+ local port=5433
+ '[' 0 -eq 0 ']'
+ echo 'Port 5433 is open on 10.128.0.2'
+ add_result 'Port 5433 Check' PASS 'Port 5433 is open on 10.128.0.2'
+ local 'check=Port 5433 Check'
+ local result=PASS
+ local 'message=Port 5433 is open on 10.128.0.2'
+ '[' 3958 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 5433 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 5433 is open on 10.128.0.2"
'
+ json_results+='    }'
+ ps -p 6089
+ force_kill 6089
+ local pid=6089
+ kill 6089
+ sleep 2
+ ps -p 6089
+ for port in "${ports[@]}"
++ start_server 9042
++ local port=9042
++ echo 6097
++ python3 -m http.server 9042
+ server_pid=6097
+ sleep 2
+ check_port 10.128.0.2 9042
+ local ip=10.128.0.2
+ local port=9042
+ '[' 0 -eq 0 ']'
+ echo 'Port 9042 is open on 10.128.0.2'
+ add_result 'Port 9042 Check' PASS 'Port 9042 is open on 10.128.0.2'
+ local 'check=Port 9042 Check'
+ local result=PASS
+ local 'message=Port 9042 is open on 10.128.0.2'
+ '[' 4080 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9042 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9042 is open on 10.128.0.2"
'
+ json_results+='    }'
+ ps -p 6097
+ force_kill 6097
+ local pid=6097
+ kill 6097
+ sleep 2
+ ps -p 6097
+ for port in "${ports[@]}"
++ start_server 9070
++ local port=9070
++ echo 6107
++ python3 -m http.server 9070
+ server_pid=6107
+ sleep 2
+ check_port 10.128.0.2 9070
+ local ip=10.128.0.2
+ local port=9070
+ '[' 0 -eq 0 ']'
+ echo 'Port 9070 is open on 10.128.0.2'
+ add_result 'Port 9070 Check' PASS 'Port 9070 is open on 10.128.0.2'
+ local 'check=Port 9070 Check'
+ local result=PASS
+ local 'message=Port 9070 is open on 10.128.0.2'
+ '[' 4202 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9070 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9070 is open on 10.128.0.2"
'
+ json_results+='    }'
+ ps -p 6107
+ for port in "${ports[@]}"
++ start_server 9300
++ local port=9300
++ echo 6112
++ python3 -m http.server 9300
+ server_pid=6112
+ sleep 2
+ check_port 10.128.0.2 9300
+ local ip=10.128.0.2
+ local port=9300
+ '[' 0 -eq 0 ']'
+ echo 'Port 9300 is open on 10.128.0.2'
+ add_result 'Port 9300 Check' PASS 'Port 9300 is open on 10.128.0.2'
+ local 'check=Port 9300 Check'
+ local result=PASS
+ local 'message=Port 9300 is open on 10.128.0.2'
+ '[' 4324 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9300 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9300 is open on 10.128.0.2"
'
+ json_results+='    }'
+ ps -p 6112
+ for port in "${ports[@]}"
++ start_server 12000
++ local port=12000
++ echo 6117
++ python3 -m http.server 12000
+ server_pid=6117
+ sleep 2
+ check_port 10.128.0.2 12000
+ local ip=10.128.0.2
+ local port=12000
+ '[' 0 -eq 0 ']'
+ echo 'Port 12000 is open on 10.128.0.2'
+ add_result 'Port 12000 Check' PASS 'Port 12000 is open on 10.128.0.2'
+ local 'check=Port 12000 Check'
+ local result=PASS
+ local 'message=Port 12000 is open on 10.128.0.2'
+ '[' 4446 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 12000 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 12000 is open on 10.128.0.2"
'
+ json_results+='    }'
+ ps -p 6117
+ force_kill 6117
+ local pid=6117
+ kill 6117
+ sleep 2
+ ps -p 6117
+ for port in "${ports[@]}"
++ start_server 13000
++ local port=13000
++ echo 6171
++ python3 -m http.server 13000
+ server_pid=6171
+ sleep 2
+ check_port 10.128.0.2 13000
+ local ip=10.128.0.2
+ local port=13000
+ '[' 0 -eq 0 ']'
+ echo 'Port 13000 is open on 10.128.0.2'
+ add_result 'Port 13000 Check' PASS 'Port 13000 is open on 10.128.0.2'
+ local 'check=Port 13000 Check'
+ local result=PASS
+ local 'message=Port 13000 is open on 10.128.0.2'
+ '[' 4570 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 13000 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 13000 is open on 10.128.0.2"
'
+ json_results+='    }'
+ ps -p 6171
+ force_kill 6171
+ local pid=6171
+ kill 6171
+ sleep 2
+ ps -p 6171
+ service_name=yb-node-agent.service
+ yba_url=https://35.184.240.7
+ customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
+ token=3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
+ provider_id=2dc133e8-9482-4f77-ab4d-4f617ebbfa28
+ provider_name=onPrem
++ skip_tls_verify https://35.184.240.7
++ local yba_url=https://35.184.240.7
++ [[ https://35.184.240.7 == https* ]]
++ echo --insecure
+ tls_verify_option=--insecure
+ readarray -t headers
++ _get_headers 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
++ local token=3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
++ echo 'Accept: application/json'
++ echo 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb'
++ echo 'Content-Type: application/json'
+ header_options=()
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
++ systemctl show -p ActiveState yb-node-agent.service
++ grep ActiveState
++ awk -F= '{print $2}'
+ status=active
+ '[' active = active ']'
+ echo '"yb-node-agent.service" is active'
+ add_result 'Service Status Check' PASS 'yb-node-agent.service is active'
+ local 'check=Service Status Check'
+ local result=PASS
+ local 'message=yb-node-agent.service is active'
+ '[' 4694 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Service Status Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "yb-node-agent.service is active"
'
+ json_results+='    }'
++ systemctl show -p MemoryCurrent yb-node-agent.service
++ grep MemoryCurrent
++ awk -F= '{print $2}'
+ memory=10305536
+ '[' 10305536 -gt 0 ']'
+ echo 'MemoryCurrent is greater than 0: 10305536'
+ add_result 'Memory Usage Check' PASS 'MemoryCurrent is greater than 0: 10305536'
+ local 'check=Memory Usage Check'
+ local result=PASS
+ local 'message=MemoryCurrent is greater than 0: 10305536'
+ '[' 4821 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Memory Usage Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "MemoryCurrent is greater than 0: 10305536"
'
+ json_results+='    }'
+ '[' -z 2dc133e8-9482-4f77-ab4d-4f617ebbfa28 ']'
++ _get_nodes_in_provider https://35.184.240.7 cf6d0b42-bae0-49f2-8fa1-28a3f9216a77 2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ local yba_url=https://35.184.240.7
++ local customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
++ local provider_uuid=2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ echo https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
+ get_nodes_in_provider=https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
++ curl -s -w '%{http_code}' -o response.txt -X GET -H 'Accept: application/json' -H 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb' -H 'Content-Type: application/json' --insecure https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
+ response=200
+ http_status=200
+ response_body='[{"nodeUuid":"8dc3bcc1-fb03-42ec-9903-a07c3983e0d0","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n3","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.3","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n3"},"detailsJson":"{\"ip\":\"10.128.0.3\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n3\"}","inUse":true},{"nodeUuid":"716b9c3c-3743-41c7-b2b8-87d92f96bf83","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n2","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.2","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n2"},"detailsJson":"{\"ip\":\"10.128.0.2\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n2\"}","inUse":true},{"nodeUuid":"ed729a1c-89d8-44cf-8031-110cddfdd47f","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n1","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.15.231","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n1"},"detailsJson":"{\"ip\":\"10.128.15.231\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n1\"}","inUse":true}]'
+ matched=false
+ '[' 200 -ge 200 ']'
+ '[' 200 -lt 300 ']'
+ echo 'HTTP GET request successful. Processing response...'
++ echo '[{"nodeUuid":"8dc3bcc1-fb03-42ec-9903-a07c3983e0d0","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n3","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.3","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n3"},"detailsJson":"{\"ip\":\"10.128.0.3\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n3\"}","inUse":true},{"nodeUuid":"716b9c3c-3743-41c7-b2b8-87d92f96bf83","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n2","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.2","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n2"},"detailsJson":"{\"ip\":\"10.128.0.2\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n2\"}","inUse":true},{"nodeUuid":"ed729a1c-89d8-44cf-8031-110cddfdd47f","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n1","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.15.231","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n1"},"detailsJson":"{\"ip\":\"10.128.15.231\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n1\"}","inUse":true}]'
++ grep -o '"ip":"[a-zA-Z0-9.:_-]*"'
++ cut -d '"' -f4
+ ips='10.128.0.3
10.128.0.2
10.128.15.231'
+ for ip in $ips
+ [[ 10.128.0.3 == \1\0\.\1\2\8\.\0\.\2 ]]
+ for ip in $ips
+ [[ 10.128.0.2 == \1\0\.\1\2\8\.\0\.\2 ]]
+ matched=true
+ break
+ [[ true == false ]]
+ echo 'Node addition to the provider passed: 10.128.0.2'
+ add_result 'Node addition failed' PASS 'Node addition to the provider passed: 10.128.0.2'
+ local 'check=Node addition failed'
+ local result=PASS
+ local 'message=Node addition to the provider passed: 10.128.0.2'
+ '[' 4956 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Node addition failed",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Node addition to the provider passed: 10.128.0.2"
'
+ json_results+='    }'
+ check_marker_file
+ MARKER_FILE=/tmp/.reboot-required
+ '[' -f /tmp/.reboot-required ']'
+ echo 'Marker file does not exist: /tmp/.reboot-required'
+ return 1
+ echo 'Reboot requirement cleared.'
+ add_result 'REBOOT REQUIRED' PASS 'No Marker file present'
+ local 'check=REBOOT REQUIRED'
+ local result=PASS
+ local 'message=No Marker file present'
+ '[' 5100 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "REBOOT REQUIRED",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "No Marker file present"
'
+ json_results+='    }'
+ print_results
+ any_fail=0
+ [[ {
"results":[
    {
      "check": "Clock Synchronization",
      "result": "PASS",
      "message": "System clock synchronized"
    },
    {
      "check": "Yugabyte User Existence Check",
      "result": "PASS",
      "message": "User yugabyte exists"
    },
    {
      "check": "Yugabyte Home Directory Permissions Check",
      "result": "PASS",
      "message": "/data/home/yugabyte has the correct ownership and acceptable permissions"
    },
    {
      "check": "/data Ownership/Permission Check",
      "result": "PASS",
      "message": "Directory /data is owned by yugabyte or has '7' (rwx) permissions."
    },
    {
      "check": "/data Free Space Check",
      "result": "PASS",
      "message": "Sufficient disk space available: 196G"
    },
    {
      "check": "Systemd Unit File Check - yb-tserver.service",
      "result": "PASS",
      "message": "Systemd unit yb-tserver.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-master.service",
      "result": "PASS",
      "message": "Systemd unit yb-master.service is configured."
    },
    {
      "check": "Systemd Unit File Check - clock-sync.sh.j2",
      "result": "PASS",
      "message": "Systemd unit clock-sync.sh.j2 is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-bind_check.service",
      "result": "PASS",
      "message": "Systemd unit yb-bind_check.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.service",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.timer",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.service",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.timer",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-controller.service",
      "result": "PASS",
      "message": "Systemd unit yb-controller.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.service",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.timer",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.timer is configured."
    },
    {
      "check": "vm.swappiness",
      "result": "PASS",
      "message": "vm.swappiness is set to 0 (expected: 0)"
    },
    {
      "check": "vm.max_map_count",
      "result": "PASS",
      "message": "vm.max_map_count is set to 262144 (expected: 262144)"
    },
    {
      "check": "kernel.core_pattern",
      "result": "PASS",
      "message": "kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "node_exporter.service is active"
    },
    {
      "check": "Port 7000 Check",
      "result": "PASS",
      "message": "Port 7000 is open on 10.128.0.2"
    },
    {
      "check": "Port 7100 Check",
      "result": "PASS",
      "message": "Port 7100 is open on 10.128.0.2"
    },
    {
      "check": "Port 9000 Check",
      "result": "PASS",
      "message": "Port 9000 is open on 10.128.0.2"
    },
    {
      "check": "Port 9100 Check",
      "result": "PASS",
      "message": "Port 9100 is open on 10.128.0.2"
    },
    {
      "check": "Port 18018 Check",
      "result": "PASS",
      "message": "Port 18018 is open on 10.128.0.2"
    },
    {
      "check": "Port 22 Check",
      "result": "PASS",
      "message": "Port 22 is open on 10.128.0.2"
    },
    {
      "check": "Port 5433 Check",
      "result": "PASS",
      "message": "Port 5433 is open on 10.128.0.2"
    },
    {
      "check": "Port 9042 Check",
      "result": "PASS",
      "message": "Port 9042 is open on 10.128.0.2"
    },
    {
      "check": "Port 9070 Check",
      "result": "PASS",
      "message": "Port 9070 is open on 10.128.0.2"
    },
    {
      "check": "Port 9300 Check",
      "result": "PASS",
      "message": "Port 9300 is open on 10.128.0.2"
    },
    {
      "check": "Port 12000 Check",
      "result": "PASS",
      "message": "Port 12000 is open on 10.128.0.2"
    },
    {
      "check": "Port 13000 Check",
      "result": "PASS",
      "message": "Port 13000 is open on 10.128.0.2"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "yb-node-agent.service is active"
    },
    {
      "check": "Memory Usage Check",
      "result": "PASS",
      "message": "MemoryCurrent is greater than 0: 10305536"
    },
    {
      "check": "Node addition failed",
      "result": "PASS",
      "message": "Node addition to the provider passed: 10.128.0.2"
    },
    {
      "check": "REBOOT REQUIRED",
      "result": "PASS",
      "message": "No Marker file present"
    } == *\"\r\e\s\u\l\t\"\:\ \"\F\A\I\L\"* ]]
+ json_results+='
]}'
+ echo '{
"results":[
    {
      "check": "Clock Synchronization",
      "result": "PASS",
      "message": "System clock synchronized"
    },
    {
      "check": "Yugabyte User Existence Check",
      "result": "PASS",
      "message": "User yugabyte exists"
    },
    {
      "check": "Yugabyte Home Directory Permissions Check",
      "result": "PASS",
      "message": "/data/home/yugabyte has the correct ownership and acceptable permissions"
    },
    {
      "check": "/data Ownership/Permission Check",
      "result": "PASS",
      "message": "Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions."
    },
    {
      "check": "/data Free Space Check",
      "result": "PASS",
      "message": "Sufficient disk space available: 196G"
    },
    {
      "check": "Systemd Unit File Check - yb-tserver.service",
      "result": "PASS",
      "message": "Systemd unit yb-tserver.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-master.service",
      "result": "PASS",
      "message": "Systemd unit yb-master.service is configured."
    },
    {
      "check": "Systemd Unit File Check - clock-sync.sh.j2",
      "result": "PASS",
      "message": "Systemd unit clock-sync.sh.j2 is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-bind_check.service",
      "result": "PASS",
      "message": "Systemd unit yb-bind_check.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.service",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.timer",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.service",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.timer",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-controller.service",
      "result": "PASS",
      "message": "Systemd unit yb-controller.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.service",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.timer",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.timer is configured."
    },
    {
      "check": "vm.swappiness",
      "result": "PASS",
      "message": "vm.swappiness is set to 0 (expected: 0)"
    },
    {
      "check": "vm.max_map_count",
      "result": "PASS",
      "message": "vm.max_map_count is set to 262144 (expected: 262144)"
    },
    {
      "check": "kernel.core_pattern",
      "result": "PASS",
      "message": "kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "node_exporter.service is active"
    },
    {
      "check": "Port 7000 Check",
      "result": "PASS",
      "message": "Port 7000 is open on 10.128.0.2"
    },
    {
      "check": "Port 7100 Check",
      "result": "PASS",
      "message": "Port 7100 is open on 10.128.0.2"
    },
    {
      "check": "Port 9000 Check",
      "result": "PASS",
      "message": "Port 9000 is open on 10.128.0.2"
    },
    {
      "check": "Port 9100 Check",
      "result": "PASS",
      "message": "Port 9100 is open on 10.128.0.2"
    },
    {
      "check": "Port 18018 Check",
      "result": "PASS",
      "message": "Port 18018 is open on 10.128.0.2"
    },
    {
      "check": "Port 22 Check",
      "result": "PASS",
      "message": "Port 22 is open on 10.128.0.2"
    },
    {
      "check": "Port 5433 Check",
      "result": "PASS",
      "message": "Port 5433 is open on 10.128.0.2"
    },
    {
      "check": "Port 9042 Check",
      "result": "PASS",
      "message": "Port 9042 is open on 10.128.0.2"
    },
    {
      "check": "Port 9070 Check",
      "result": "PASS",
      "message": "Port 9070 is open on 10.128.0.2"
    },
    {
      "check": "Port 9300 Check",
      "result": "PASS",
      "message": "Port 9300 is open on 10.128.0.2"
    },
    {
      "check": "Port 12000 Check",
      "result": "PASS",
      "message": "Port 12000 is open on 10.128.0.2"
    },
    {
      "check": "Port 13000 Check",
      "result": "PASS",
      "message": "Port 13000 is open on 10.128.0.2"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "yb-node-agent.service is active"
    },
    {
      "check": "Memory Usage Check",
      "result": "PASS",
      "message": "MemoryCurrent is greater than 0: 10305536"
    },
    {
      "check": "Node addition failed",
      "result": "PASS",
      "message": "Node addition to the provider passed: 10.128.0.2"
    },
    {
      "check": "REBOOT REQUIRED",
      "result": "PASS",
      "message": "No Marker file present"
    }
]}'
+ '[' 0 -eq 1 ']'
+ echo 'Pre-flight checks successful'

2025-06-02 14:50:52,211 - commands.provision_command - INFO - Return Code: 0
INI file has been created successfully at: /data/2024.2.2.2-b2/scripts/ynp/configs/config.ini
File written successfully and crash-consistent.
2025-06-02 14:50:52,548 - INFO - Running: /opt/yugabyte/software/active/yb-platform/yugaware/yba-cli/yba_cli-2024.2.2.2-b2-linux-amd64/yba universe node reprovision -H https://35.184.240.7/ -a 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb --name TU --node-name yb-dev-TU-n2

Waiting for node yb-dev-TU-n2 operation Reprovision to be completed
The node yb-dev-TU-n2 operation Reprovision has been completed
Node Name      Node UUID                              IP           State     Is Master process running   Is Tserver process running   Master state
yb-dev-TU-n2   716b9c3c-3743-41c7-b2b8-87d92f96bf83   10.128.0.2   Stopped   false                       false                        -
2025-06-02 14:53:24,020 - INFO - Running: /opt/yugabyte/software/active/yb-platform/yugaware/yba-cli/yba_cli-2024.2.2.2-b2-linux-amd64/yba universe node start -H https://35.184.240.7/ -a 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb --name TU --node-name yb-dev-TU-n2

Waiting for node yb-dev-TU-n2 operation Start to be completed
The node yb-dev-TU-n2 operation Start has been completed
Node Name      Node UUID                              IP           State     Is Master process running   Is Tserver process running   Master state
yb-dev-TU-n2   716b9c3c-3743-41c7-b2b8-87d92f96bf83   10.128.0.2   Live      true                        true                         
2025-06-02 14:55:55,509 - INFO - âœ… Rehydration complete for node yb-dev-TU-n2
2025-06-02 14:55:55,509 - INFO - Waiting 60 seconds before processing next node...
2025-06-02 14:56:55,553 - INFO - Running: gcloud compute instances list --filter networkInterfaces[0].networkIP=10.128.0.3 --format json --project yuga-rjw
2025-06-02 14:56:57,346 - INFO - Processing node yb-dev-TU-n3 (IP: 10.128.0.3)
2025-06-02 14:56:57,346 - INFO - Running: /opt/yugabyte/software/active/yb-platform/yugaware/yba-cli/yba_cli-2024.2.2.2-b2-linux-amd64/yba universe node stop -H https://35.184.240.7/ -a 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb --name TU --node-name yb-dev-TU-n3

Waiting for node yb-dev-TU-n3 operation Stop to be completed
The node yb-dev-TU-n3 operation Stop has been completed
Node Name      Node UUID                              IP           State     Is Master process running   Is Tserver process running   Master state
yb-dev-TU-n3   8dc3bcc1-fb03-42ec-9903-a07c3983e0d0   10.128.0.3   Stopped   false                       false                        -
2025-06-02 14:58:18,803 - INFO - âœ… Stopped node yb-dev-TU-n3 using YBA CLI
2025-06-02 14:58:18,804 - INFO - Running: gcloud compute instances stop yb-tu-dev-n4 --zone us-central1-b --project yuga-rjw
Stopping instance(s) yb-tu-dev-n4...done.                                                                                                                                                                                             
Updated [https://compute.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-b/instances/yb-tu-dev-n4].
2025-06-02 14:58:41,372 - INFO - Running: gcloud compute instances describe yb-tu-dev-n4 --zone us-central1-b --project yuga-rjw --format=json
2025-06-02 14:58:42,967 - INFO - Running: gcloud compute instances describe yb-tu-dev-n4 --zone us-central1-b --project yuga-rjw --format=json
2025-06-02 14:58:44,527 - INFO - Running: gcloud compute disks create yb-tu-dev-n4-boot-1748876324 --image rhel-9-v20240415 --image-project rhel-cloud --size 150GB --type pd-balanced --zone us-central1-b --project yuga-rjw
Created [https://www.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-b/disks/yb-tu-dev-n4-boot-1748876324].
WARNING: Some requests generated warnings:
 - Disk size: '150 GB' is larger than image size: '20 GB'. You might need to resize the root repartition manually if the operating system does not support automatic resizing. See https://cloud.google.com/compute/docs/disks/add-persistent-disk#resize_pd for details.
 - The resource 'projects/rhel-cloud/global/images/rhel-9-v20240415' is deprecated. A suggested replacement is 'projects/rhel-cloud/global/images/rhel-9-v20240515'.

NAME                          ZONE           SIZE_GB  TYPE         STATUS
yb-tu-dev-n4-boot-1748876324  us-central1-b  150      pd-balanced  READY
2025-06-02 14:58:47,791 - INFO - Running: gcloud compute instances detach-disk yb-tu-dev-n4 --disk yb-tu-dev-n4 --zone us-central1-b --project yuga-rjw
Updated [https://www.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-b/instances/yb-tu-dev-n4].
2025-06-02 14:58:50,908 - INFO - Running: gcloud compute instances attach-disk yb-tu-dev-n4 --disk yb-tu-dev-n4-boot-1748876324 --zone us-central1-b --project yuga-rjw --boot
Updated [https://www.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-b/instances/yb-tu-dev-n4].
2025-06-02 14:58:54,345 - INFO - Running: gcloud compute instances start yb-tu-dev-n4 --zone us-central1-b --project yuga-rjw
Starting instance(s) yb-tu-dev-n4...done.                                                                                                                                                                                             
Updated [https://compute.googleapis.com/compute/v1/projects/yuga-rjw/zones/us-central1-b/instances/yb-tu-dev-n4].
Instance internal IP is 10.128.0.3
Instance external IP is 34.9.22.199
2025-06-02 14:59:08,141 - INFO - Running: gcloud compute instances describe yb-tu-dev-n4 --zone us-central1-b --project yuga-rjw --format=json
2025-06-02 14:59:39,636 - INFO - Running: gcloud compute ssh weiwang@yb-tu-dev-n4 --zone us-central1-b --project yuga-rjw --internal-ip --command lsblk -o NAME,FSTYPE,UUID | grep xfs | grep -v sda
2025-06-02 14:59:42,526 - INFO - Running: gcloud compute ssh weiwang@yb-tu-dev-n4 --zone us-central1-b --project yuga-rjw --internal-ip --command sudo mkdir -p /data && sudo mount UUID=34386954-9c54-480e-8936-12a9a1f1d44e /data && sudo chmod 777 /data && grep -q UUID=34386954-9c54-480e-8936-12a9a1f1d44e /etc/fstab || echo 'UUID=34386954-9c54-480e-8936-12a9a1f1d44e /data xfs defaults,nofail 0 2' | sudo tee -a /etc/fstab
UUID=34386954-9c54-480e-8936-12a9a1f1d44e /data xfs defaults,nofail 0 2
2025-06-02 14:59:44,918 - INFO - Running: gcloud compute ssh weiwang@yb-tu-dev-n4 --zone us-central1-b --project yuga-rjw --internal-ip --command echo 'ðŸ›  Reprovisioning yugabyte user and running node-agent-provision.sh...' && sudo pkill -u yugabyte || true && id yugabyte && sudo userdel -r yugabyte || true && getent group yugabyte && (getent passwd | awk -F: '$4 == "$(getent group yugabyte | cut -d: -f3)"' | grep -q . || sudo groupdel yugabyte) || true && sudo useradd -m -d /data/home/yugabyte -s /bin/bash yugabyte && sudo mkdir -p /data/home/yugabyte && sudo chown -R yugabyte:yugabyte /data/home/yugabyte && sudo chmod 755 /data/home/yugabyte && cd /data/2024.2.2.2-b2/scripts && sudo ./node-agent-provision.sh
ðŸ›  Reprovisioning yugabyte user and running node-agent-provision.sh...
uid=1017(yugabyte) gid=1018(yugabyte) groups=1018(yugabyte),4(adm),39(video),1000(google-sudoers)
useradd: warning: the home directory /data/home/yugabyte already exists.
useradd: Not copying any file from skel directory into it.
[2025-06-02_14_59_47 common.sh:626 activate_pex] Using pex virtualenv python executable now.
2025-06-02 14:59:47,872 - root - INFO - Logging Setup Done
2025-06-02 14:59:47,874 - __main__ - DEBUG - YNP config {'logging': {'directory': './logs', 'file': 'app.log', 'level': 'DEBUG'},
 'yba': {'api_key': '3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb',
         'customer_uuid': 'cf6d0b42-bae0-49f2-8fa1-28a3f9216a77',
         'instance_type': {'cores': 4,
                           'memory_size': 16,
                           'mount_points': ['/data'],
                           'name': 'n2',
                           'volume_size': 100},
         'node_external_fqdn': '10.128.0.3',
         'node_name': 'yb-dev-tu-n1',
         'provider': {'name': 'onPrem',
                      'region': {'name': 'us-central1',
                                 'zone': {'name': 'us-central1-a'}}},
         'url': 'https://35.184.240.7'},
 'ynp': {'chrony_servers': ['0.pool.ntp.org', '1.pool.ntp.org'],
         'is_airgap': False,
         'is_install_node_agent': True,
         'node_ip': '10.128.0.3',
         'tmp_directory': '/tmp',
         'use_system_level_systemd': False,
         'yb_home_dir': '/data/home/yugabyte',
         'yb_user_id': 1994}}
2025-06-02 14:59:47,874 - __main__ - INFO - Config here: {'Preprovision': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748876387', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp'}, 'ConfigureChrony': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748876387', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'chrony_servers': '"0.pool.ntp.org, 1.pool.ntp.org"'}, 'CreateYugabyteUser': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748876387', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'yb_user_id': '1994', 'yb_user_password': '', 'mount_points': '/data'}, 'ConfigureSystemd': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748876387', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'user_name': 'yugabyte', 'use_system_level_systemd': 'False', 'service_files': '"yb-tserver.service, yb-master.service, clock-sync.sh.j2, yb-bind_check.service, yb-clean_cores.service, yb-clean_cores.timer, yb-collect_metrics.service, yb-collect_metrics.timer, yb-controller.service, yb-zip_purge_yb_logs.service, yb-zip_purge_yb_logs.timer"'}, 'ConfigureOs': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748876387', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'message': '"Configure limits and sysctl parameters"', 'fd_limit': '1048576', 'nproc_limit': '12000', 'vm_swappiness': '0', 'kernel_core_pattern': '/data/home/yugabyte/cores/core_%p_%t_%E', 'vm_max_map_count': '262144', 'mount_points': '/data', 'limits': {'core': 'unlimited', 'data': 'unlimited', 'fsize': 'unlimited', 'sigpending': '119934', 'memlock': '64', 'rss': 'unlimited', 'nofile': '1048576', 'msgqueue': '819200', 'stack': '8192', 'cpu': 'unlimited', 'nproc': '12000', 'locks': 'unlimited'}}, 'ConfigureOs.limits': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748876387', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'core': 'unlimited', 'data': 'unlimited', 'fsize': 'unlimited', 'sigpending': '119934', 'memlock': '64', 'rss': 'unlimited', 'nofile': '1048576', 'msgqueue': '819200', 'stack': '8192', 'cpu': 'unlimited', 'nproc': '12000', 'locks': 'unlimited'}, 'ConfigureNodeExporter': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748876387', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'prometheus_user': 'prometheus'}, 'ConfigureNetwork': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748876387', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'ip_address': '10.128.0.3', 'ports': '7000 7100 9000 9100 18018 22 5433 9042 9070 9300 12000 13000'}, 'InstallNodeAgent': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748876387', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp', 'url': 'https://35.184.240.7', 'customer_uuid': 'cf6d0b42-bae0-49f2-8fa1-28a3f9216a77', 'api_key': '3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb', 'node_name': 'yb-dev-tu-n1', 'node_external_fqdn': '10.128.0.3', 'provider_name': 'onPrem', 'provider_region_name': 'us-central1', 'provider_region_zone_name': 'us-central1-a', 'instance_type_name': 'n2', 'instance_type_cores': '4', 'instance_type_memory_size': '16', 'instance_type_volume_size': '100', 'instance_type_mount_points': "['/data']", 'node_ip': '10.128.0.3', 'bind_ip': '10.128.0.3'}, 'RebootNode': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748876387', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp'}, 'DEFAULT': {'version': '1.0.0', 'loglevel': 'DEBUG', 'logdir': './logs', 'logfile': 'app.log', 'is_airgap': 'False', 'ynp_dir': '/data/2024.2.2.2-b2/scripts/ynp', 'execution_start_time': '1748876387', 'yb_home_dir': '/data/home/yugabyte', 'yb_user': 'yugabyte', 'is_install_node_agent': 'True', 'tmp_directory': '/tmp'}}
2025-06-02 14:59:47,874 - __main__ - INFO - Python Version: 3.9.18 (main, Jan  4 2024, 00:00:00) 
[GCC 11.4.1 20230605 (Red Hat 11.4.1-2)]
2025-06-02 14:59:47,882 - __main__ - INFO - Jinja2 -- 3.0.3
2025-06-02 14:59:47,885 - __main__ - INFO - MarkupSafe -- 3.0.2
2025-06-02 14:59:47,888 - __main__ - INFO - ansible-vault -- 2.1.0
2025-06-02 14:59:47,890 - __main__ - INFO - setuptools -- 78.1.0
2025-06-02 14:59:47,893 - __main__ - INFO - ansible -- 2.9.27
2025-06-02 14:59:47,895 - __main__ - INFO - PyYAML -- 6.0.2
2025-06-02 14:59:47,897 - __main__ - INFO - cryptography -- 44.0.2
2025-06-02 14:59:47,898 - __main__ - INFO - cffi -- 1.17.1
2025-06-02 14:59:47,899 - __main__ - INFO - pycparser -- 2.22
2025-06-02 14:59:47,901 - __main__ - INFO - distro -- 1.5.0
2025-06-02 14:59:47,902 - __main__ - INFO - mitogen -- 0.2.9
2025-06-02 14:59:47,990 - __main__ - INFO - requests -- 2.32.3
2025-06-02 14:59:47,994 - __main__ - INFO - charset-normalizer -- 3.4.1
2025-06-02 14:59:47,996 - __main__ - INFO - idna -- 3.10
2025-06-02 14:59:47,999 - __main__ - INFO - urllib3 -- 2.3.0
2025-06-02 14:59:48,001 - __main__ - INFO - certifi -- 2025.1.31
2025-06-02 14:59:48,003 - __main__ - INFO - semver -- 3.0.2
2025-06-02 14:59:48,004 - __main__ - INFO - six -- 1.10.0
2025-06-02 14:59:48,005 - __main__ - INFO - Jinja2 -- 3.0.3
2025-06-02 14:59:48,005 - __main__ - INFO - MarkupSafe -- 3.0.2
2025-06-02 14:59:48,006 - __main__ - INFO - ansible-vault -- 2.1.0
2025-06-02 14:59:48,007 - __main__ - INFO - setuptools -- 78.1.0
2025-06-02 14:59:48,008 - __main__ - INFO - ansible -- 2.9.27
2025-06-02 14:59:48,008 - __main__ - INFO - PyYAML -- 6.0.2
2025-06-02 14:59:48,009 - __main__ - INFO - cryptography -- 44.0.2
2025-06-02 14:59:48,009 - __main__ - INFO - cffi -- 1.17.1
2025-06-02 14:59:48,010 - __main__ - INFO - pycparser -- 2.22
2025-06-02 14:59:48,011 - __main__ - INFO - distro -- 1.5.0
2025-06-02 14:59:48,011 - __main__ - INFO - mitogen -- 0.2.9
2025-06-02 14:59:48,012 - __main__ - INFO - requests -- 2.32.3
2025-06-02 14:59:48,014 - __main__ - INFO - charset-normalizer -- 3.4.1
2025-06-02 14:59:48,014 - __main__ - INFO - idna -- 3.10
2025-06-02 14:59:48,015 - __main__ - INFO - urllib3 -- 2.3.0
2025-06-02 14:59:48,016 - __main__ - INFO - certifi -- 2025.1.31
2025-06-02 14:59:48,016 - __main__ - INFO - semver -- 3.0.2
2025-06-02 14:59:48,017 - __main__ - INFO - six -- 1.10.0
2025-06-02 14:59:48,174 - commands.provision_command - INFO - {'ConfigureChrony': (<class 'modules.provision.chrony.chrony.ConfigureChrony'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/chrony/chrony.py'), 'ConfigureOs': (<class 'modules.provision.configure_os.os_config.ConfigureOs'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/configure_os/os_config.py'), 'ConfigureNetwork': (<class 'modules.provision.network.network.ConfigureNetwork'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/network/network.py'), 'InstallNodeAgent': (<class 'modules.provision.node_agent.node_agent.InstallNodeAgent'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/node_agent/node_agent.py'), 'ConfigureNodeExporter': (<class 'modules.provision.node_exporter.node_exporter.ConfigureNodeExporter'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/node_exporter/node_exporter.py'), 'RebootNode': (<class 'modules.provision.reboot_node.reboot_node.RebootNode'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/reboot_node/reboot_node.py'), 'ConfigureSystemd': (<class 'modules.provision.systemd.systemd.ConfigureSystemd'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/systemd/systemd.py'), 'Preprovision': (<class 'modules.provision.update_os.update.Preprovision'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/update_os/update.py'), 'CreateYugabyteUser': (<class 'modules.provision.yugabyte.yugabyte.CreateYugabyteUser'>, '/data/2024.2.2.2-b2/scripts/ynp/modules/provision/yugabyte/yugabyte.py')}
2025-06-02 14:59:48,174 - commands.provision_command - INFO - initialized
2025-06-02 14:59:48,250 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): 35.184.240.7:443
/data/2024.2.2.2-b2/devops/pexvenv/lib64/python3.9/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '35.184.240.7'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings
  warnings.warn(
2025-06-02 14:59:48,583 - urllib3.connectionpool - DEBUG - https://35.184.240.7:443 "GET /api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers?name=onPrem HTTP/1.1" 200 979
2025-06-02 14:59:48,585 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): 35.184.240.7:443
/data/2024.2.2.2-b2/devops/pexvenv/lib64/python3.9/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host '35.184.240.7'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings
  warnings.warn(
2025-06-02 14:59:48,899 - urllib3.connectionpool - DEBUG - https://35.184.240.7:443 "GET /api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/instance_types/n2 HTTP/1.1" 200 255
2025-06-02 14:59:48,914 - commands.provision_command - INFO - /tmp/tmpo519v_t0
2025-06-02 14:59:48,915 - commands.provision_command - INFO - /tmp/tmp_k82nr59
2025-06-02 15:00:01,807 - commands.provision_command - INFO - Output: 200 OK
User yugabyte already exists
User yugabyte added to systemd-journal group
el8 not detected, skipping changing selinux context
*                -       core            unlimited
*                -       data            unlimited
*                -       fsize            unlimited
*                -       sigpending            119934
*                -       memlock            64
*                -       rss            unlimited
*                -       nofile            1048576
*                -       msgqueue            819200
*                -       stack            8192
*                -       cpu            unlimited
*                -       nproc            12000
*                -       locks            unlimited
File /etc/security/limits.d/20-nproc.conf created.
el8 not detected, not updating file limits
Systemd file limits configured.
vm.swappiness = 0
kernel.core_pattern = /data/home/yugabyte/cores/core_%p_%t_%E
vm.max_map_count = 262144
Kernel settings configured.
OS Configuration applied successfully.
Node exporter setup is complete.
Start network configuration
Nothing to be done
End network configuration
Using node agent port 9070.
* Starting YB Node Agent install.
* Downloading YB Node Agent build package.
* Getting Linux/amd64 package
* Creating Node Agent Directory.
* Changing directory to node agent.
* Creating Sub Directories.
* Downloaded Version - 2024.2.2.2-b2
* Extracting the build package
   â€¢ Completed Node Agent Configuration
   â€¢ Checking for existing Node Agent with IP 10.128.0.3
   â€¢ Node Agent is already registered with IP 10.128.0.3
   â€¢ Node Agent Configuration Successful
Source ~/.bashrc to make node-agent available in the PATH.
You can install a systemd service on linux machines by running node-agent-installer.sh -c install_service --user yugabyte (Requires sudo access).
Using node agent port 9070.
* Starting YB Node Agent install_service.
success
* Installing Node Agent Systemd Service
  [Unit]
  Description=YB Anywhere Node Agent
  After=network-online.target
  # Disable restart limits, using RestartSec to rate limit restarts.
  StartLimitInterval=0

  [Service]
  User=yugabyte
  WorkingDirectory=/data/home/yugabyte/node-agent
  LimitCORE=infinity
  LimitNOFILE=1048576
  LimitNPROC=12000
  ExecStart=/data/home/yugabyte/node-agent/pkg/bin/node-agent server start
  Restart=always
  RestartSec=2

  [Install]
  WantedBy=multi-user.target
* Starting the systemd service
* Started the systemd service
* Run 'systemctl status yb-node-agent' to check the status of the yb-node-agent
* Run 'sudo systemctl stop yb-node-agent' to stop the yb-node-agent service
HTTP GET request successful. Processing response...
HTTP GET request successful. Processing response...
Match found: Zone Code = us-central1-a, UUID = 559eb98c-4f80-4304-ac69-a89a36fb2a4f
Error: POST request failed with HTTP status 400

2025-06-02 15:00:01,807 - commands.provision_command - INFO - Error: + json_results='{
"results":[
'
+ SUDO_ACCESS=false
++ id -u
+ '[' 0 = 0 ']'
+ SUDO_ACCESS=true
+ [[ OSFamily.REDHAT == suse ]]
+ [[ rhel == ubuntu ]]
+ [[ OSFamily.REDHAT == almaLinux ]]
+ [[ rhel == RedHat ]]
+ [[ rhel == RedHat ]]
+ [[ -n '' ]]
+ [[ -n '' ]]
+ [[ -n '' ]]
+ [[ rhel == ubuntu ]]
+ [[ OSFamily.REDHAT == RedHat ]]
+ [[ OSFamily.REDHAT == RedHat ]]
+ [[ rhel == oraclelinux ]]
+ [[ rhel == oraclelinux ]]
+ '[' -n '' ']'
+ '[' -n '' ']'
+ chronyc makestep
+ id yugabyte
+ echo 'User yugabyte already exists'
+ '[' -n '' ']'
+ groups yugabyte
+ grep -q '\bsystemd-journal\b'
+ usermod -aG systemd-journal yugabyte
+ echo 'User yugabyte added to systemd-journal group'
++ grep -oP '(?<=^PLATFORM_ID=).+' /etc/os-release
++ tr -d '"'
+ platform_id=platform:el9
+ [[ platform:el9 == \p\l\a\t\f\o\r\m\:\e\l\8 ]]
+ echo 'el8 not detected, skipping changing selinux context'
+ installer_dir=/data/2024.2.2.2-b2/scripts/ynp/../../bin
+ mkdir -p /data/home/yugabyte/.install
+ cp /data/2024.2.2.2-b2/scripts/ynp/../../bin/node-agent-installer.sh /data/home/yugabyte/.install
+ chown -R yugabyte:yugabyte /data/home/yugabyte/.install
+ chmod 750 /data/home/yugabyte
+ systemd_dir=/etc/systemd/system
+ loginctl enable-linger yugabyte
++ id -u yugabyte
+ su - yugabyte -c 'export XDG_RUNTIME_DIR=/run/user/1019'
++ id -u yugabyte
+ echo 'export XDG_RUNTIME_DIR=/run/user/1019'
+ systemd_dir=/data/home/yugabyte/.config/systemd/user
+ su - yugabyte -c 'mkdir -p /data/home/yugabyte/.config/systemd/user'
++ mktemp
+ tmp_file=/tmp/tmp.gEC22b87Ev
+ echo '[Unit]
Description=Yugabyte tserver service
Requires=network-online.target
After=network.target network-online.target multi-user.target
# Disable restart limits, using RestartSec to rate limit restarts
StartLimitInterval=0
Wants=yb-ysql-cgroup.service

[Path]
PathExists=/data/home/yugabyte/tserver/bin/yb-tserver
PathExists=/data/home/yugabyte/tserver/conf/server.conf

[Service]
# Start


ExecStartPre=/data/home/yugabyte/bin/clock-sync.sh
ExecStart=/data/home/yugabyte/tserver/bin/yb-tserver --flagfile /data/home/yugabyte/tserver/conf/server.conf
Restart=always
RestartSec=5
# Stop -> SIGTERM - 10s - SIGKILL (if not stopped) [matches existing cron behavior]
KillMode=process
TimeoutStopFailureMode=terminate
KillSignal=SIGTERM
TimeoutStopSec=10
FinalKillSignal=SIGKILL
# Logs
StandardOutput=syslog
StandardError=syslog
# ulimit
LimitCORE=infinity
LimitNOFILE=1048576
LimitNPROC=12000
# Allow tserver to move postgres into its own cgroup
Delegate=true

[Install]
WantedBy=default.target'
+ chown yugabyte:yugabyte /tmp/tmp.gEC22b87Ev
+ su - yugabyte -c 'mv /tmp/tmp.gEC22b87Ev /data/home/yugabyte/.config/systemd/user/yb-tserver.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-tserver.service'
++ mktemp
+ tmp_file=/tmp/tmp.M6DVUQwTQl
+ echo '[Unit]
Description=Yugabyte master service
Requires=network-online.target
After=network.target network-online.target multi-user.target
# Disable restart limits, using RestartSec to rate limit restarts
StartLimitInterval=0

[Path]
PathExists=/data/home/yugabyte/master/bin/yb-master
PathExists=/data/home/yugabyte/master/conf/server.conf

[Service]
# Start


ExecStartPre=/data/home/yugabyte/bin/clock-sync.sh
ExecStart=/data/home/yugabyte/master/bin/yb-master --flagfile /data/home/yugabyte/master/conf/server.conf
Restart=always
RestartSec=5
# Stop -> SIGTERM - 10s - SIGKILL (if not stopped) [matches existing cron behavior]
KillMode=process
TimeoutStopFailureMode=terminate
KillSignal=SIGTERM
TimeoutStopSec=10
FinalKillSignal=SIGKILL
# Logs
StandardOutput=syslog
StandardError=syslog
# ulimit
LimitCORE=infinity
LimitNOFILE=1048576
LimitNPROC=12000

[Install]
WantedBy=default.target'
+ chown yugabyte:yugabyte /tmp/tmp.M6DVUQwTQl
+ su - yugabyte -c 'mv /tmp/tmp.M6DVUQwTQl /data/home/yugabyte/.config/systemd/user/yb-master.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-master.service'
++ mktemp
+ tmp_file=/tmp/tmp.hmITQTE5qZ
++ basename /tmp/tmp_k82nr59
++ chronyc tracking
++ chronyc tracking
++ echo ''
++ awk '/Reference ID/ {print $4}'
++ echo ''
++ awk '/System time/ {print $4}'
++ echo ''
++ awk '/Root dispersion/ {print $4}'
++ echo ''
++ awk '/Root delay/ {print $4}'
++ python -c 'print( +  + (.5 * ))'
  File "<string>", line 1
    print( +  + (.5 * ))
                      ^
SyntaxError: invalid syntax
++ ntpq -c 'rv 0 clock'
++ awk '-F[=,]' '/offset/ {print $3}'
/tmp/tmp_k82nr59: line 339: ntpq: command not found
++ ntpq -p
/tmp/tmp_k82nr59: line 339: ntpq: command not found
++ awk '$1 ~ "^*" {print $9}'
++ python -c 'print( * 1000)'
Traceback (most recent call last):
  File "<string>", line 1, in <module>
TypeError: print() argument after * must be an iterable, not int
++ timedatectl status
++ grep 'System clock synchronized'
++ awk '{print $4}'
++ systemctl show --no-pager
++ grep ActiveState
++ cut -d= -f2
+ echo '#!/bin/bash

SCRIPT_NAME=tmp_k82nr59

################### Config ###################
is_acceptable_clock_skew_wait_enabled=True  # Whether check clock skew
acceptable_clock_skew_sec=0.5  # In seconds
max_tries=120  # Maximum number of tries before returning failure
retry_wait_time_s=1  # How long waits before retry in seconds

if [[  != true &&  != True ]]; then
  echo Wait' for clock skew to go below the acceptable threshold is disabled. Returning 'success.
  exit 0
fi

command_exists() {
  command -v  >/dev/null 2>&1
}

readonly PYTHON_EXECUTABLES=('\''python'\'' '\''python3'\'' '\''python3.11'\'' '\''python3.10'\'' '\''python3.9'\'' '\''python3.8'\'' '\''python3.7'\'' '\''python3.6'\'' '\''python3.12'\'' '\''python2'\'')
PYTHON_EXECUTABLE=
set_python_executable() {
  for py_executable in ; do
    if which  > /dev/null 2>&1; then
      PYTHON_EXECUTABLE=
      export PYTHON_EXECUTABLE
      return
    fi
  done
}

check_clock_sync_chrony() {
  # if chrond is restarted, tracking will return all 0'\''s
  set_python_executable
  chrony_tracking=Reference' ID : A9FEA9FE '(metadata.google.internal)' Stratum : 3 Ref time '(UTC)' : Mon Jun 02 14:59:36 2025 System time : 0.000000000 seconds fast of NTP time Last offset : -0.000151542 seconds RMS offset : 0.000151542 seconds Frequency : 72.881 ppm slow Residual freq : -0.000 ppm Skew : 96.941 ppm Root delay : 0.000113703 seconds Root dispersion : 0.001311587 seconds Update interval : 2.0 seconds Leap status : 'Normal
  if [[ 0 -ne 0 ]]; then 
    echo Reference' ID : A9FEA9FE '(metadata.google.internal)' Stratum : 3 Ref time '(UTC)' : Mon Jun 02 14:59:36 2025 System time : 0.000000000 seconds fast of NTP time Last offset : -0.000151542 seconds RMS offset : 0.000151542 seconds Frequency : 72.881 ppm slow Residual freq : -0.000 ppm Skew : 96.941 ppm Root delay : 0.000113703 seconds Root dispersion : 0.001312011 seconds Update interval : 2.0 seconds Leap status : Normal failed to 'execute
    return 1
  fi
  if [[  == 00000000 ]]; then
    echo chrony' is not 'initialized
    return 1
  fi
  local skew=
  local dispersion=
  local delay=
  local clock_error=
  if [[ -z python ]]; then
    clock_error=
  else
   clock_error=
  fi
  
  if awk '\''BEGIN{exit !('\'''\'' < '\'''\'')}'\''; then
    echo Clock' skew is within acceptable limits: 'ms
    return 0
  else
    echo Clock' skew exceeds acceptable limits: 'ms
    return 1
  fi
}

check_clock_sync_ntpd() {
  #local skew=
  local skew=
  local acceptable_skew_ms=

  if [[ -z  ]]; then
    echo ntpd' is not 'initialized
    return 1
  fi
  
  if awk '\''BEGIN{exit !('\'''\'' < '\'''\'')}'\''; then
    echo Clock' skew is within acceptable limits: 'ms
    return 0
  else
    echo Clock' skew exceeds acceptable limits: 'ms
    return 1
  fi
}

check_clock_sync_timesyncd() {
  synchronized=no
  if [[  == yes ]]; then
    echo timesyncd' reports clock is 'synchronized
    return 0
  else
    echo timesyncd' clock is not 'synchronized
    return 1
  fi
}

systemd_loaded() {
  active=
  if [[  == active ]]; then
    return 0
  fi
  return 1
}

iter=0
while true; do
  # If chrony is available, use it for clock sync.
  if command_exists chronyc; then
    check_clock_sync_chrony
    res=0
  # If ntpd is available, use it for clock sync.
  elif command_exists ntpd; then
    check_clock_sync_ntpd
    res=0
  elif systemd_loaded systemd-timesyncd; then 
    check_clock_sync_timesyncd
    res=0
  else
    echo Chrony,' NTPd, and timesyncd are not available, but 'required.
    exit 1
  fi
  ((iter++))
  if [  -eq 0 ]; then
    echo Success!' Clock skew is within acceptable 'limits.
    exit 0
  fi
  if [  -ge  ]; then
    echo Failure!' Maximum number of tries 'reached.
    exit 1
  fi
  sleep 
done'
+ chown yugabyte:yugabyte /tmp/tmp.hmITQTE5qZ
+ su - yugabyte -c 'mv /tmp/tmp.hmITQTE5qZ /data/home/yugabyte/.config/systemd/user/clock-sync.sh.j2'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/clock-sync.sh.j2'
++ mktemp
+ tmp_file=/tmp/tmp.6s7Xq7VLHV
+ echo '[Unit]
Description=Yugabyte IP Bind Check
Requires=network-online.target
After=network.target network-online.target multi-user.target
Before=yb-controller.service yb-tserver.service yb-master.service
# Disable restart limits, using RestartSec to rate limit restarts
StartLimitInterval=0

[Path]
PathExists=/data/home/yugabyte/controller/bin/yb-controller-server
PathExists=/data/home/yugabyte/controller/conf/server.conf

[Service]
# Start
ExecStart=/data/home/yugabyte/controller/bin/yb-controller-server     --flagfile /data/home/yugabyte/controller/conf/server.conf     --only_bind --logtostderr
Type=oneshot
KillMode=control-group
KillSignal=SIGTERM
TimeoutStopSec=10
# Logs
StandardOutput=syslog
StandardError=syslog

[Install]
WantedBy=default.target'
+ chown yugabyte:yugabyte /tmp/tmp.6s7Xq7VLHV
+ su - yugabyte -c 'mv /tmp/tmp.6s7Xq7VLHV /data/home/yugabyte/.config/systemd/user/yb-bind_check.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-bind_check.service'
++ mktemp
+ tmp_file=/tmp/tmp.AbQOUv8yIs
+ echo '[Unit]
Description=Yugabyte clean cores

[Service]


Type=oneshot
WorkingDirectory=/data/home/yugabyte/bin
ExecStart=/bin/bash /data/home/yugabyte/bin/clean_cores.sh

[Install]
WantedBy=multi-user.target'
+ chown yugabyte:yugabyte /tmp/tmp.AbQOUv8yIs
+ su - yugabyte -c 'mv /tmp/tmp.AbQOUv8yIs /data/home/yugabyte/.config/systemd/user/yb-clean_cores.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-clean_cores.service'
++ mktemp
+ tmp_file=/tmp/tmp.yG0VrC8Vbm
+ echo '[Unit]
Description=Yugabyte clean cores

[Timer]


Unit=yb-clean_cores.service
# Run every 5 minutes
OnCalendar=*:0/5

[Install]
WantedBy=timers.target'
+ chown yugabyte:yugabyte /tmp/tmp.yG0VrC8Vbm
+ su - yugabyte -c 'mv /tmp/tmp.yG0VrC8Vbm /data/home/yugabyte/.config/systemd/user/yb-clean_cores.timer'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-clean_cores.timer'
++ mktemp
+ tmp_file=/tmp/tmp.XoAddOYu2B
+ echo '[Unit]
Description=Yugabyte collect metrics

[Service]


Type=oneshot
WorkingDirectory=/data/home/yugabyte/bin
ExecStart=/bin/bash /data/home/yugabyte/bin/collect_metrics_wrapper.sh

[Install]'
+ chown yugabyte:yugabyte /tmp/tmp.XoAddOYu2B
+ su - yugabyte -c 'mv /tmp/tmp.XoAddOYu2B /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.service'
++ mktemp
+ tmp_file=/tmp/tmp.XxB5QJwVP7
+ echo '[Unit]
Description=Yugabyte collect metrics

[Timer]


Unit=yb-collect_metrics.service
# Run every 1 minute
OnCalendar=*:0/1:0

[Install]
WantedBy=timers.target'
+ chown yugabyte:yugabyte /tmp/tmp.XxB5QJwVP7
+ su - yugabyte -c 'mv /tmp/tmp.XxB5QJwVP7 /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.timer'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.timer'
++ mktemp
+ tmp_file=/tmp/tmp.JIhGYJ3Ywc
+ echo '[Unit]
Description=Yugabyte Controller
Requires=network-online.target
After=network.target network-online.target multi-user.target
# Disable restart limits, using RestartSec to rate limit restarts
StartLimitInterval=0

[Path]
PathExists=/data/home/yugabyte/controller/bin/yb-controller-server
PathExists=/data/home/yugabyte/controller/conf/server.conf

[Service]
# Start


ExecStart=/data/home/yugabyte/controller/bin/yb-controller-server     --flagfile /data/home/yugabyte/controller/conf/server.conf
Restart=always
RestartSec=5
# Stop -> SIGTERM - 10s - SIGKILL (if not stopped) [matches existing cron behavior]
KillMode=control-group
TimeoutStopFailureMode=terminate
KillSignal=SIGTERM
TimeoutStopSec=10
FinalKillSignal=SIGKILL
# Logs
StandardOutput=syslog
StandardError=syslog
# ulimit
LimitCORE=infinity
LimitNOFILE=1048576
LimitNPROC=12000

[Install]
WantedBy=default.target'
+ chown yugabyte:yugabyte /tmp/tmp.JIhGYJ3Ywc
+ su - yugabyte -c 'mv /tmp/tmp.JIhGYJ3Ywc /data/home/yugabyte/.config/systemd/user/yb-controller.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-controller.service'
++ mktemp
+ tmp_file=/tmp/tmp.o5Jk5fxTqk
+ echo '[Unit]
Description=Yugabyte logs

[Service]


Type=oneshot
WorkingDirectory=/data/home/yugabyte/bin
ExecStart=/bin/bash /data/home/yugabyte/bin/zip_purge_yb_logs.sh

[Install]
WantedBy=multi-user.target'
+ chown yugabyte:yugabyte /tmp/tmp.o5Jk5fxTqk
+ su - yugabyte -c 'mv /tmp/tmp.o5Jk5fxTqk /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.service'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.service'
++ mktemp
+ tmp_file=/tmp/tmp.CHS0eIxXVq
+ echo '[Unit]
Description=Yugabyte logs

[Timer]


Unit=yb-zip_purge_yb_logs.service
# Run every 5 minutes
OnCalendar=*:0/5

[Install]
WantedBy=timers.target'
+ chown yugabyte:yugabyte /tmp/tmp.CHS0eIxXVq
+ su - yugabyte -c 'mv /tmp/tmp.CHS0eIxXVq /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.timer'
+ su - yugabyte -c 'chmod 755 /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.timer'
+ grep -qP '^\*\s+-\s+core\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       core            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+data\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       data            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+fsize\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       fsize            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+sigpending\s+119934$' /etc/security/limits.conf
+ echo '*                -       sigpending            119934'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+memlock\s+64$' /etc/security/limits.conf
+ echo '*                -       memlock            64'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+rss\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       rss            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+nofile\s+1048576$' /etc/security/limits.conf
+ echo '*                -       nofile            1048576'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+msgqueue\s+819200$' /etc/security/limits.conf
+ echo '*                -       msgqueue            819200'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+stack\s+8192$' /etc/security/limits.conf
+ echo '*                -       stack            8192'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+cpu\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       cpu            unlimited'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+nproc\s+12000$' /etc/security/limits.conf
+ echo '*                -       nproc            12000'
+ tee -a /etc/security/limits.conf
+ grep -qP '^\*\s+-\s+locks\s+unlimited$' /etc/security/limits.conf
+ echo '*                -       locks            unlimited'
+ tee -a /etc/security/limits.conf
+ test '!' -f /etc/security/limits.d/20-nproc.conf
+ touch /etc/security/limits.d/20-nproc.conf
+ echo 'File /etc/security/limits.d/20-nproc.conf created.'
+ sed -i 's/*          soft    nproc     4096/*          soft    nproc     \
12000/' /etc/security/limits.d/20-nproc.conf
++ grep -oP '(?<=^PLATFORM_ID=).+' /etc/os-release
++ tr -d '"'
+ platform_id=platform:el9
+ [[ platform:el9 == \p\l\a\t\f\o\r\m\:\e\l\8 ]]
+ echo 'el8 not detected, not updating file limits'
+ echo 'Systemd file limits configured.'
+ systemctl daemon-reload
+ ulimit -n 1048576
+ ulimit -u 12000
+ grep -q '^vm.swappiness=' /etc/sysctl.conf
+ echo vm.swappiness=0
+ sysctl -w vm.swappiness=0
+ grep -q '^kernel.core_pattern=' /etc/sysctl.conf
+ echo kernel.core_pattern=/data/home/yugabyte/cores/core_%p_%t_%E
+ sysctl -w kernel.core_pattern=/data/home/yugabyte/cores/core_%p_%t_%E
+ grep -q '^vm.max_map_count=' /etc/sysctl.conf
+ echo vm.max_map_count=262144
+ sysctl -w vm.max_map_count=262144
+ echo 'Kernel settings configured.'
+ echo 'OS Configuration applied successfully.'
+ thirdparty_dir=/data/2024.2.2.2-b2/scripts/ynp/../../thirdparty
+ mkdir -p /opt/prometheus
+ mkdir -p /etc/prometheus
+ mkdir -p /var/log/prometheus
+ mkdir -p /var/run/prometheus
+ mkdir -p /tmp/yugabyte/metrics
+ '[' -f /data/2024.2.2.2-b2/scripts/ynp/../../thirdparty/node_exporter-1.7.0.linux-amd64.tar.gz ']'
+ cp /data/2024.2.2.2-b2/scripts/ynp/../../thirdparty/node_exporter-1.7.0.linux-amd64.tar.gz /opt/prometheus
+ id -u prometheus
++ grep -oP '(?<=^ID=).+' /etc/os-release
++ tr -d '"'
+ os_type=rhel
+ case "$os_type" in
+ useradd --shell /bin/bash --no-create-home -g yugabyte prometheus
+ chown -R prometheus:yugabyte /opt/prometheus
+ chown -R prometheus:yugabyte /etc/prometheus
+ chown -R prometheus:yugabyte /var/log/prometheus
+ chown -R prometheus:yugabyte /var/run/prometheus
+ chown -R yugabyte:yugabyte /tmp/yugabyte/metrics
+ chmod -R 755 /tmp/yugabyte/metrics
+ '[' -f /opt/prometheus/node_exporter-1.7.0.linux-amd64.tar.gz ']'
+ chmod +r /opt/prometheus/node_exporter-1.7.0.linux-amd64.tar.gz
+ su - prometheus -c 'cd /opt/prometheus && tar zxf node_exporter-1.7.0.linux-amd64.tar.gz'
su: warning: cannot change directory to /home/prometheus: No such file or directory
+ file_path=/etc/systemd/system/node_exporter.service
+ cat
+ systemctl daemon-reload
+ systemctl enable node_exporter
Created symlink /etc/systemd/system/multi-user.target.wants/node_exporter.service â†’ /etc/systemd/system/node_exporter.service.
+ systemctl start node_exporter
+ echo 'Node exporter setup is complete.'
+ echo 'Start network configuration'
+ echo 'Nothing to be done'
+ echo 'End network configuration'
+ yba_url=https://35.184.240.7
+ customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
+ token=3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
+ provider_id=2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ skip_tls_verify https://35.184.240.7
++ local yba_url=https://35.184.240.7
++ [[ https://35.184.240.7 == https* ]]
++ echo --insecure
+ tls_verify_option=--insecure
+ readarray -t headers
++ _get_headers 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
++ local token=3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
++ echo 'Accept: application/json'
++ echo 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb'
++ echo 'Content-Type: application/json'
+ header_options=()
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ test -f /tmp/create_provider.json
+ test -f /tmp/update_provider.json
+ test -f /tmp/create_instance.json
+ airgap_flag=
+ installer_dir=/data/home/yugabyte/.install
+ su - yugabyte -c '"/data/home/yugabyte/.install/node-agent-installer.sh" -c install -u https://35.184.240.7 -t 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb --provider_id 2dc133e8-9482-4f77-ab4d-4f617ebbfa28 --instance_type n2 --zone_name us-central1-a --node_name yb-dev-tu-n1 --region_name us-central1 --node_ip 10.128.0.3 --bind_ip 10.128.0.3 --silent --skip_verify_cert '
+ /data/home/yugabyte/.install/node-agent-installer.sh -c install_service --user yugabyte
Created symlink /etc/systemd/system/multi-user.target.wants/yb-node-agent.service â†’ /etc/systemd/system/yb-node-agent.service.
+ test -f /tmp/add_node_to_provider.json
++ _get_nodes_in_provider https://35.184.240.7 cf6d0b42-bae0-49f2-8fa1-28a3f9216a77 2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ local yba_url=https://35.184.240.7
++ local customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
++ local provider_uuid=2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ echo https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
+ get_nodes_in_provider=https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
++ curl -s -w '%{http_code}' -o response.txt -X GET -H 'Accept: application/json' -H 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb' -H 'Content-Type: application/json' --insecure https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
+ response=200
+ http_status=200
+ response_body='[{"nodeUuid":"8dc3bcc1-fb03-42ec-9903-a07c3983e0d0","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n3","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.3","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n3"},"detailsJson":"{\"ip\":\"10.128.0.3\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n3\"}","inUse":true},{"nodeUuid":"716b9c3c-3743-41c7-b2b8-87d92f96bf83","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n2","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.2","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n2"},"detailsJson":"{\"ip\":\"10.128.0.2\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n2\"}","inUse":true},{"nodeUuid":"ed729a1c-89d8-44cf-8031-110cddfdd47f","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n1","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.15.231","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n1"},"detailsJson":"{\"ip\":\"10.128.15.231\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n1\"}","inUse":true}]'
+ matched=false
+ '[' 200 -ge 200 ']'
+ '[' 200 -lt 300 ']'
+ echo 'HTTP GET request successful. Processing response...'
++ echo '[{"nodeUuid":"8dc3bcc1-fb03-42ec-9903-a07c3983e0d0","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n3","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.3","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n3"},"detailsJson":"{\"ip\":\"10.128.0.3\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n3\"}","inUse":true},{"nodeUuid":"716b9c3c-3743-41c7-b2b8-87d92f96bf83","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n2","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.2","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n2"},"detailsJson":"{\"ip\":\"10.128.0.2\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n2\"}","inUse":true},{"nodeUuid":"ed729a1c-89d8-44cf-8031-110cddfdd47f","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n1","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.15.231","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n1"},"detailsJson":"{\"ip\":\"10.128.15.231\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n1\"}","inUse":true}]'
++ sed -n 's/.*"ip":"\([a-zA-Z0-9\.-]*\)".*/\1/p'
+ ips=10.128.15.231
+ for ip in $ips
+ [[ 10.128.15.231 == \1\0\.\1\2\8\.\0\.\3 ]]
+ [[ false == false ]]
++ _get_provider_url https://35.184.240.7 cf6d0b42-bae0-49f2-8fa1-28a3f9216a77 2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ local yba_url=https://35.184.240.7
++ local customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
++ local provider_uuid=2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ echo https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28
+ get_provider_endpoint=https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ curl -s -w '%{http_code}' -o response.txt -X GET -H 'Accept: application/json' -H 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb' -H 'Content-Type: application/json' --insecure https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28
+ response=200
+ http_status=200
+ response_body='{"uuid":"2dc133e8-9482-4f77-ab4d-4f617ebbfa28","code":"onprem","name":"onPrem","active":true,"customerUUID":"cf6d0b42-bae0-49f2-8fa1-28a3f9216a77","config":{"YB_HOME_DIR":"/data/home/yugabyte"},"details":{"sshPort":22,"airGapInstall":false,"passwordlessSudoAccess":true,"provisionInstanceScript":"","installNodeExporter":true,"nodeExporterPort":9300,"nodeExporterUser":"prometheus","skipProvisioning":true,"setUpChrony":false,"ntpServers":[],"showSetUpChrony":false,"cloudInfo":{"onprem":{"ybHomeDir":"/data/home/yugabyte"}},"enableNodeAgent":true},"regions":[{"uuid":"435ead87-b36e-4139-bc89-34943671398e","code":"us-central1","name":"us-central1","longitude":0.0,"latitude":0.0,"zones":[{"uuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","code":"us-central1-a","name":"us-central1-a","active":true,"details":{"cloudInfo":{}}}],"active":true,"details":{"cloudInfo":{}}}],"imageBundles":[],"allAccessKeys":[],"version":3,"usabilityState":"READY","airGapInstall":false,"sshPort":22}'
+ '[' 200 -ge 200 ']'
+ '[' 200 -lt 300 ']'
+ echo 'HTTP GET request successful. Processing response...'
+ zone_uuid=
+ echo '{"uuid":"2dc133e8-9482-4f77-ab4d-4f617ebbfa28","code":"onprem","name":"onPrem","active":true,"customerUUID":"cf6d0b42-bae0-49f2-8fa1-28a3f9216a77","config":{"YB_HOME_DIR":"/data/home/yugabyte"},"details":{"sshPort":22,"airGapInstall":false,"passwordlessSudoAccess":true,"provisionInstanceScript":"","installNodeExporter":true,"nodeExporterPort":9300,"nodeExporterUser":"prometheus","skipProvisioning":true,"setUpChrony":false,"ntpServers":[],"showSetUpChrony":false,"cloudInfo":{"onprem":{"ybHomeDir":"/data/home/yugabyte"}},"enableNodeAgent":true},"regions":[{"uuid":"435ead87-b36e-4139-bc89-34943671398e","code":"us-central1","name":"us-central1","longitude":0.0,"latitude":0.0,"zones":[{"uuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","code":"us-central1-a","name":"us-central1-a","active":true,"details":{"cloudInfo":{}}}],"active":true,"details":{"cloudInfo":{}}}],"imageBundles":[],"allAccessKeys":[],"version":3,"usabilityState":"READY","airGapInstall":false,"sshPort":22}'
+ read -r zone
++ grep -oP '"zones":\s*\[[^]]*\]' temp_response.txt
++ grep -oP '{[^}]*}'
++ echo '{"uuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","code":"us-central1-a","name":"us-central1-a","active":true,"details":{"cloudInfo":{}'
++ grep -oP '"code":\s*"\K[^"]+'
+ zone_code=us-central1-a
++ echo '{"uuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","code":"us-central1-a","name":"us-central1-a","active":true,"details":{"cloudInfo":{}'
++ grep -oP '"uuid":\s*"\K[^"]+'
+ uuid=559eb98c-4f80-4304-ac69-a89a36fb2a4f
+ '[' us-central1-a == us-central1-a ']'
+ zone_uuid=559eb98c-4f80-4304-ac69-a89a36fb2a4f
+ echo 'Match found: Zone Code = us-central1-a, UUID = 559eb98c-4f80-4304-ac69-a89a36fb2a4f'
+ break
+ '[' -z 559eb98c-4f80-4304-ac69-a89a36fb2a4f ']'
++ cat /tmp/add_node_to_provider.json
+ add_node_data='{
    "nodes": [
        {
            "instanceType": "n2",
            "ip": "10.128.0.3",
            "region": "us-central1",
            "zone": "us-central1-a",
            "nodeName": "yb-dev-tu-n1",
            "instanceName": "n2"
        }
    ]
}'
++ _add_node_to_provider https://35.184.240.7 cf6d0b42-bae0-49f2-8fa1-28a3f9216a77 559eb98c-4f80-4304-ac69-a89a36fb2a4f
++ local yba_url=https://35.184.240.7
++ local customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
++ local zone_uuid=559eb98c-4f80-4304-ac69-a89a36fb2a4f
++ echo https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/zones/559eb98c-4f80-4304-ac69-a89a36fb2a4f/nodes
+ add_node_url=https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/zones/559eb98c-4f80-4304-ac69-a89a36fb2a4f/nodes
++ curl -s -w '%{http_code}' -o response.txt -X POST -H 'Accept: application/json' -H 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb' -H 'Content-Type: application/json' -d '{
    "nodes": [
        {
            "instanceType": "n2",
            "ip": "10.128.0.3",
            "region": "us-central1",
            "zone": "us-central1-a",
            "nodeName": "yb-dev-tu-n1",
            "instanceName": "n2"
        }
    ]
}' --insecure https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/zones/559eb98c-4f80-4304-ac69-a89a36fb2a4f/nodes
+ response=400
+ http_status=400
+ '[' 400 -ge 200 ']'
+ '[' 400 -lt 300 ']'
+ echo 'Error: POST request failed with HTTP status 400'
+ exit 1

2025-06-02 15:00:01,808 - commands.provision_command - INFO - Return Code: 1
2025-06-02 15:00:44,400 - commands.provision_command - INFO - Output: 200 OK
System clock synchronized
User yugabyte exists
/data/home/yugabyte has the correct ownership and acceptable permissions
[PASS] Directory /data is owned by yugabyte or has '7' (rwx) permissions.
[PASS] Sufficient disk space available: 196G
Systemd unit yb-tserver.service is configured.
Systemd unit yb-master.service is configured.
Systemd unit clock-sync.sh.j2 is configured.
Systemd unit yb-bind_check.service is configured.
Systemd unit yb-clean_cores.service is configured.
Systemd unit yb-clean_cores.timer is configured.
Systemd unit yb-collect_metrics.service is configured.
Systemd unit yb-collect_metrics.timer is configured.
Systemd unit yb-controller.service is configured.
Systemd unit yb-zip_purge_yb_logs.service is configured.
Systemd unit yb-zip_purge_yb_logs.timer is configured.
[PASS] vm.swappiness is set to 0 (expected: 0)
[PASS] vm.max_map_count is set to 262144 (expected: 262144)
[PASS] kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E
"node_exporter.service" is active
Port 7000 is open on 10.128.0.3
Port 7100 is open on 10.128.0.3
Port 9000 is open on 10.128.0.3
Port 9100 is open on 10.128.0.3
Port 18018 is open on 10.128.0.3
Port 22 is open on 10.128.0.3
Port 5433 is open on 10.128.0.3
Port 9042 is open on 10.128.0.3
Port 9070 is open on 10.128.0.3
Port 9300 is open on 10.128.0.3
Port 12000 is open on 10.128.0.3
Port 13000 is open on 10.128.0.3
"yb-node-agent.service" is active
MemoryCurrent is greater than 0: 9715712
HTTP GET request successful. Processing response...
Node addition to the provider passed: 10.128.0.3
Marker file does not exist: /tmp/.reboot-required
Reboot requirement cleared.
{
"results":[
    {
      "check": "Clock Synchronization",
      "result": "PASS",
      "message": "System clock synchronized"
    },
    {
      "check": "Yugabyte User Existence Check",
      "result": "PASS",
      "message": "User yugabyte exists"
    },
    {
      "check": "Yugabyte Home Directory Permissions Check",
      "result": "PASS",
      "message": "/data/home/yugabyte has the correct ownership and acceptable permissions"
    },
    {
      "check": "/data Ownership/Permission Check",
      "result": "PASS",
      "message": "Directory /data is owned by yugabyte or has '7' (rwx) permissions."
    },
    {
      "check": "/data Free Space Check",
      "result": "PASS",
      "message": "Sufficient disk space available: 196G"
    },
    {
      "check": "Systemd Unit File Check - yb-tserver.service",
      "result": "PASS",
      "message": "Systemd unit yb-tserver.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-master.service",
      "result": "PASS",
      "message": "Systemd unit yb-master.service is configured."
    },
    {
      "check": "Systemd Unit File Check - clock-sync.sh.j2",
      "result": "PASS",
      "message": "Systemd unit clock-sync.sh.j2 is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-bind_check.service",
      "result": "PASS",
      "message": "Systemd unit yb-bind_check.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.service",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.timer",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.service",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.timer",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-controller.service",
      "result": "PASS",
      "message": "Systemd unit yb-controller.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.service",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.timer",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.timer is configured."
    },
    {
      "check": "vm.swappiness",
      "result": "PASS",
      "message": "vm.swappiness is set to 0 (expected: 0)"
    },
    {
      "check": "vm.max_map_count",
      "result": "PASS",
      "message": "vm.max_map_count is set to 262144 (expected: 262144)"
    },
    {
      "check": "kernel.core_pattern",
      "result": "PASS",
      "message": "kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "node_exporter.service is active"
    },
    {
      "check": "Port 7000 Check",
      "result": "PASS",
      "message": "Port 7000 is open on 10.128.0.3"
    },
    {
      "check": "Port 7100 Check",
      "result": "PASS",
      "message": "Port 7100 is open on 10.128.0.3"
    },
    {
      "check": "Port 9000 Check",
      "result": "PASS",
      "message": "Port 9000 is open on 10.128.0.3"
    },
    {
      "check": "Port 9100 Check",
      "result": "PASS",
      "message": "Port 9100 is open on 10.128.0.3"
    },
    {
      "check": "Port 18018 Check",
      "result": "PASS",
      "message": "Port 18018 is open on 10.128.0.3"
    },
    {
      "check": "Port 22 Check",
      "result": "PASS",
      "message": "Port 22 is open on 10.128.0.3"
    },
    {
      "check": "Port 5433 Check",
      "result": "PASS",
      "message": "Port 5433 is open on 10.128.0.3"
    },
    {
      "check": "Port 9042 Check",
      "result": "PASS",
      "message": "Port 9042 is open on 10.128.0.3"
    },
    {
      "check": "Port 9070 Check",
      "result": "PASS",
      "message": "Port 9070 is open on 10.128.0.3"
    },
    {
      "check": "Port 9300 Check",
      "result": "PASS",
      "message": "Port 9300 is open on 10.128.0.3"
    },
    {
      "check": "Port 12000 Check",
      "result": "PASS",
      "message": "Port 12000 is open on 10.128.0.3"
    },
    {
      "check": "Port 13000 Check",
      "result": "PASS",
      "message": "Port 13000 is open on 10.128.0.3"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "yb-node-agent.service is active"
    },
    {
      "check": "Memory Usage Check",
      "result": "PASS",
      "message": "MemoryCurrent is greater than 0: 9715712"
    },
    {
      "check": "Node addition failed",
      "result": "PASS",
      "message": "Node addition to the provider passed: 10.128.0.3"
    },
    {
      "check": "REBOOT REQUIRED",
      "result": "PASS",
      "message": "No Marker file present"
    }
]}
Pre-flight checks successful

2025-06-02 15:00:44,400 - commands.provision_command - INFO - Error: + json_results='{
"results":[
'
+ SUDO_ACCESS=false
++ id -u
+ '[' 0 = 0 ']'
+ SUDO_ACCESS=true
+ '[' true = true ']'
+ chronyc makestep
+ '[' 0 -eq 0 ']'
+ add_result 'Clock Synchronization' PASS 'System clock synchronized'
+ local 'check=Clock Synchronization'
+ local result=PASS
+ local 'message=System clock synchronized'
+ '[' 14 -gt 20 ']'
+ json_results+='    {
'
+ json_results+='      "check": "Clock Synchronization",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "System clock synchronized"
'
+ json_results+='    }'
+ echo 'System clock synchronized'
+ id yugabyte
+ echo 'User yugabyte exists'
+ add_result 'Yugabyte User Existence Check' PASS 'User yugabyte exists'
+ local 'check=Yugabyte User Existence Check'
+ local result=PASS
+ local 'message=User yugabyte exists'
+ '[' 134 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Yugabyte User Existence Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "User yugabyte exists"
'
+ json_results+='    }'
+ '[' -d /data/home/yugabyte ']'
++ stat -c %U /data/home/yugabyte
+ owner=yugabyte
++ stat -c %G /data/home/yugabyte
+ group=yugabyte
++ stat -c %a /data/home/yugabyte
+ permissions=750
+ '[' yugabyte '!=' yugabyte ']'
+ '[' yugabyte '!=' yugabyte ']'
+ '[' 750 -lt 711 ']'
+ echo '/data/home/yugabyte has the correct ownership and acceptable permissions'
+ add_result 'Yugabyte Home Directory Permissions Check' PASS '/data/home/yugabyte has the correct ownership and acceptable permissions'
+ local 'check=Yugabyte Home Directory Permissions Check'
+ local result=PASS
+ local 'message=/data/home/yugabyte has the correct ownership and acceptable permissions'
+ '[' 259 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Yugabyte Home Directory Permissions Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "/data/home/yugabyte has the correct ownership and acceptable permissions"
'
+ json_results+='    }'
+ yb_home_dir=/data/home/yugabyte
+ threshold=49
+ mount_points=/data
+ IFS=' '
+ read -ra mount_points_array
+ for mount_point in "${mount_points_array[@]}"
+ is_subdirectory /data /data/home/yugabyte
+ local dir=/data
+ local parent=/data/home/yugabyte
++ realpath -m /data
+ dir=/data
++ realpath -m /data/home/yugabyte
+ parent=/data/home/yugabyte
+ case "$dir/" in
+ return 1
+ '[' -d /data ']'
++ stat -c %U /data
+ owner_check=root
++ stat -c %A /data
++ cut -c 2-4
+ perm_check=rwx
+ '[' root == yugabyte ']'
+ [[ rwx == \r\w\x ]]
+ result=PASS
+ message='Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions.'
+ echo '[PASS] Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions.'
+ add_result '/data Ownership/Permission Check' PASS 'Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions.'
+ local 'check=/data Ownership/Permission Check'
+ local result=PASS
+ local 'message=Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions.'
+ '[' 448 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "/data Ownership/Permission Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions."
'
+ json_results+='    }'
+ '[' -d /data ']'
++ df -BG --output=avail /data
++ tail -n 1
++ tr -d 'G '
+ free_space_gb=196
+ '[' 196 -gt 49 ']'
+ result=PASS
+ message='Sufficient disk space available: 196G'
+ echo '[PASS] Sufficient disk space available: 196G'
+ add_result '/data Free Space Check' PASS 'Sufficient disk space available: 196G'
+ local 'check=/data Free Space Check'
+ local result=PASS
+ local 'message=Sufficient disk space available: 196G'
+ '[' 622 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "/data Free Space Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Sufficient disk space available: 196G"
'
+ json_results+='    }'
+ systemd_dir=/etc/systemd/system
+ systemd_dir=/data/home/yugabyte/.config/systemd/user
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-tserver.service ']'
+ echo 'Systemd unit yb-tserver.service is configured.'
+ add_result 'Systemd Unit File Check - yb-tserver.service' PASS 'Systemd unit yb-tserver.service is configured.'
+ local 'check=Systemd Unit File Check - yb-tserver.service'
+ local result=PASS
+ local 'message=Systemd unit yb-tserver.service is configured.'
+ '[' 757 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-tserver.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-tserver.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-master.service ']'
+ echo 'Systemd unit yb-master.service is configured.'
+ add_result 'Systemd Unit File Check - yb-master.service' PASS 'Systemd unit yb-master.service is configured.'
+ local 'check=Systemd Unit File Check - yb-master.service'
+ local result=PASS
+ local 'message=Systemd unit yb-master.service is configured.'
+ '[' 923 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-master.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-master.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/clock-sync.sh.j2 ']'
+ echo 'Systemd unit clock-sync.sh.j2 is configured.'
+ add_result 'Systemd Unit File Check - clock-sync.sh.j2' PASS 'Systemd unit clock-sync.sh.j2 is configured.'
+ local 'check=Systemd Unit File Check - clock-sync.sh.j2'
+ local result=PASS
+ local 'message=Systemd unit clock-sync.sh.j2 is configured.'
+ '[' 1087 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - clock-sync.sh.j2",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit clock-sync.sh.j2 is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-bind_check.service ']'
+ echo 'Systemd unit yb-bind_check.service is configured.'
+ add_result 'Systemd Unit File Check - yb-bind_check.service' PASS 'Systemd unit yb-bind_check.service is configured.'
+ local 'check=Systemd Unit File Check - yb-bind_check.service'
+ local result=PASS
+ local 'message=Systemd unit yb-bind_check.service is configured.'
+ '[' 1249 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-bind_check.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-bind_check.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-clean_cores.service ']'
+ echo 'Systemd unit yb-clean_cores.service is configured.'
+ add_result 'Systemd Unit File Check - yb-clean_cores.service' PASS 'Systemd unit yb-clean_cores.service is configured.'
+ local 'check=Systemd Unit File Check - yb-clean_cores.service'
+ local result=PASS
+ local 'message=Systemd unit yb-clean_cores.service is configured.'
+ '[' 1421 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-clean_cores.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-clean_cores.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-clean_cores.timer ']'
+ echo 'Systemd unit yb-clean_cores.timer is configured.'
+ add_result 'Systemd Unit File Check - yb-clean_cores.timer' PASS 'Systemd unit yb-clean_cores.timer is configured.'
+ local 'check=Systemd Unit File Check - yb-clean_cores.timer'
+ local result=PASS
+ local 'message=Systemd unit yb-clean_cores.timer is configured.'
+ '[' 1595 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-clean_cores.timer",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-clean_cores.timer is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.service ']'
+ echo 'Systemd unit yb-collect_metrics.service is configured.'
+ add_result 'Systemd Unit File Check - yb-collect_metrics.service' PASS 'Systemd unit yb-collect_metrics.service is configured.'
+ local 'check=Systemd Unit File Check - yb-collect_metrics.service'
+ local result=PASS
+ local 'message=Systemd unit yb-collect_metrics.service is configured.'
+ '[' 1765 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-collect_metrics.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-collect_metrics.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-collect_metrics.timer ']'
+ echo 'Systemd unit yb-collect_metrics.timer is configured.'
+ add_result 'Systemd Unit File Check - yb-collect_metrics.timer' PASS 'Systemd unit yb-collect_metrics.timer is configured.'
+ local 'check=Systemd Unit File Check - yb-collect_metrics.timer'
+ local result=PASS
+ local 'message=Systemd unit yb-collect_metrics.timer is configured.'
+ '[' 1947 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-collect_metrics.timer",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-collect_metrics.timer is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-controller.service ']'
+ echo 'Systemd unit yb-controller.service is configured.'
+ add_result 'Systemd Unit File Check - yb-controller.service' PASS 'Systemd unit yb-controller.service is configured.'
+ local 'check=Systemd Unit File Check - yb-controller.service'
+ local result=PASS
+ local 'message=Systemd unit yb-controller.service is configured.'
+ '[' 2125 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-controller.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-controller.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.service ']'
+ echo 'Systemd unit yb-zip_purge_yb_logs.service is configured.'
+ add_result 'Systemd Unit File Check - yb-zip_purge_yb_logs.service' PASS 'Systemd unit yb-zip_purge_yb_logs.service is configured.'
+ local 'check=Systemd Unit File Check - yb-zip_purge_yb_logs.service'
+ local result=PASS
+ local 'message=Systemd unit yb-zip_purge_yb_logs.service is configured.'
+ '[' 2297 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.service",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-zip_purge_yb_logs.service is configured."
'
+ json_results+='    }'
+ '[' '!' -f /data/home/yugabyte/.config/systemd/user/yb-zip_purge_yb_logs.timer ']'
+ echo 'Systemd unit yb-zip_purge_yb_logs.timer is configured.'
+ add_result 'Systemd Unit File Check - yb-zip_purge_yb_logs.timer' PASS 'Systemd unit yb-zip_purge_yb_logs.timer is configured.'
+ local 'check=Systemd Unit File Check - yb-zip_purge_yb_logs.timer'
+ local result=PASS
+ local 'message=Systemd unit yb-zip_purge_yb_logs.timer is configured.'
+ '[' 2483 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.timer",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Systemd unit yb-zip_purge_yb_logs.timer is configured."
'
+ json_results+='    }'
++ grep -oP '(?<=^PLATFORM_ID=).+' /etc/os-release
++ tr -d '"'
+ platform_id=platform:el9
+ [[ platform:el9 == \p\l\a\t\f\o\r\m\:\e\l\8 ]]
+ verify_sysctl vm.swappiness 0
+ local param=vm.swappiness
+ local expected_value=0
+ local current_value
++ sysctl -n vm.swappiness
+ current_value=0
+ '[' 0 -eq 0 ']'
+ echo '[PASS] vm.swappiness is set to 0 (expected: 0)'
+ add_result vm.swappiness PASS 'vm.swappiness is set to 0 (expected: 0)'
+ local check=vm.swappiness
+ local result=PASS
+ local 'message=vm.swappiness is set to 0 (expected: 0)'
+ '[' 2665 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "vm.swappiness",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "vm.swappiness is set to 0 (expected: 0)"
'
+ json_results+='    }'
+ verify_sysctl vm.max_map_count 262144
+ local param=vm.max_map_count
+ local expected_value=262144
+ local current_value
++ sysctl -n vm.max_map_count
+ current_value=262144
+ '[' 262144 -eq 262144 ']'
+ echo '[PASS] vm.max_map_count is set to 262144 (expected: 262144)'
+ add_result vm.max_map_count PASS 'vm.max_map_count is set to 262144 (expected: 262144)'
+ local check=vm.max_map_count
+ local result=PASS
+ local 'message=vm.max_map_count is set to 262144 (expected: 262144)'
+ '[' 2793 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "vm.max_map_count",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "vm.max_map_count is set to 262144 (expected: 262144)"
'
+ json_results+='    }'
++ sysctl -n kernel.core_pattern
+ kernel_core_pattern_value=/data/home/yugabyte/cores/core_%p_%t_%E
+ '[' /data/home/yugabyte/cores/core_%p_%t_%E == /data/home/yugabyte/cores/core_%p_%t_%E ']'
+ echo '[PASS] kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E'
+ add_result kernel.core_pattern PASS 'kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E'
+ local check=kernel.core_pattern
+ local result=PASS
+ local 'message=kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E'
+ '[' 2937 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "kernel.core_pattern",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E"
'
+ json_results+='    }'
++ grep -oP '(?<=^PLATFORM_ID=).+' /etc/os-release
++ tr -d '"'
+ platform_id=platform:el9
+ [[ platform:el9 == \p\l\a\t\f\o\r\m\:\e\l\8 ]]
+ service_name=node_exporter.service
++ systemctl show -p ActiveState node_exporter.service
++ grep ActiveState
++ awk -F= '{print $2}'
+ status=active
+ '[' active = active ']'
+ echo '"node_exporter.service" is active'
+ add_result 'Service Status Check' PASS 'node_exporter.service is active'
+ local 'check=Service Status Check'
+ local result=PASS
+ local 'message=node_exporter.service is active'
+ '[' 3101 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Service Status Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "node_exporter.service is active"
'
+ json_results+='    }'
+ ports=(7000 7100 9000 9100 18018 22 5433 9042 9070 9300 12000 13000)
+ vm_ip=10.128.0.3
+ for port in "${ports[@]}"
++ start_server 7000
++ local port=7000
++ echo 5618
++ python3 -m http.server 7000
+ server_pid=5618
+ sleep 2
+ check_port 10.128.0.3 7000
+ local ip=10.128.0.3
+ local port=7000
+ '[' 0 -eq 0 ']'
+ echo 'Port 7000 is open on 10.128.0.3'
+ add_result 'Port 7000 Check' PASS 'Port 7000 is open on 10.128.0.3'
+ local 'check=Port 7000 Check'
+ local result=PASS
+ local 'message=Port 7000 is open on 10.128.0.3'
+ '[' 3228 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 7000 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 7000 is open on 10.128.0.3"
'
+ json_results+='    }'
+ ps -p 5618
+ force_kill 5618
+ local pid=5618
+ kill 5618
+ sleep 2
+ ps -p 5618
+ for port in "${ports[@]}"
++ start_server 7100
++ local port=7100
++ echo 5626
++ python3 -m http.server 7100
+ server_pid=5626
+ sleep 2
+ check_port 10.128.0.3 7100
+ local ip=10.128.0.3
+ local port=7100
+ '[' 0 -eq 0 ']'
+ echo 'Port 7100 is open on 10.128.0.3'
+ add_result 'Port 7100 Check' PASS 'Port 7100 is open on 10.128.0.3'
+ local 'check=Port 7100 Check'
+ local result=PASS
+ local 'message=Port 7100 is open on 10.128.0.3'
+ '[' 3350 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 7100 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 7100 is open on 10.128.0.3"
'
+ json_results+='    }'
+ ps -p 5626
+ force_kill 5626
+ local pid=5626
+ kill 5626
+ sleep 2
+ ps -p 5626
+ for port in "${ports[@]}"
++ start_server 9000
++ local port=9000
++ echo 5634
++ python3 -m http.server 9000
+ server_pid=5634
+ sleep 2
+ check_port 10.128.0.3 9000
+ local ip=10.128.0.3
+ local port=9000
+ '[' 0 -eq 0 ']'
+ echo 'Port 9000 is open on 10.128.0.3'
+ add_result 'Port 9000 Check' PASS 'Port 9000 is open on 10.128.0.3'
+ local 'check=Port 9000 Check'
+ local result=PASS
+ local 'message=Port 9000 is open on 10.128.0.3'
+ '[' 3472 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9000 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9000 is open on 10.128.0.3"
'
+ json_results+='    }'
+ ps -p 5634
+ force_kill 5634
+ local pid=5634
+ kill 5634
+ sleep 2
+ ps -p 5634
+ for port in "${ports[@]}"
++ start_server 9100
++ local port=9100
++ echo 5643
++ python3 -m http.server 9100
+ server_pid=5643
+ sleep 2
+ check_port 10.128.0.3 9100
+ local ip=10.128.0.3
+ local port=9100
+ '[' 0 -eq 0 ']'
+ echo 'Port 9100 is open on 10.128.0.3'
+ add_result 'Port 9100 Check' PASS 'Port 9100 is open on 10.128.0.3'
+ local 'check=Port 9100 Check'
+ local result=PASS
+ local 'message=Port 9100 is open on 10.128.0.3'
+ '[' 3594 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9100 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9100 is open on 10.128.0.3"
'
+ json_results+='    }'
+ ps -p 5643
+ force_kill 5643
+ local pid=5643
+ kill 5643
+ sleep 2
+ ps -p 5643
+ for port in "${ports[@]}"
++ start_server 18018
++ local port=18018
++ echo 5760
++ python3 -m http.server 18018
+ server_pid=5760
+ sleep 2
+ check_port 10.128.0.3 18018
+ local ip=10.128.0.3
+ local port=18018
+ '[' 0 -eq 0 ']'
+ echo 'Port 18018 is open on 10.128.0.3'
+ add_result 'Port 18018 Check' PASS 'Port 18018 is open on 10.128.0.3'
+ local 'check=Port 18018 Check'
+ local result=PASS
+ local 'message=Port 18018 is open on 10.128.0.3'
+ '[' 3716 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 18018 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 18018 is open on 10.128.0.3"
'
+ json_results+='    }'
+ ps -p 5760
+ force_kill 5760
+ local pid=5760
+ kill 5760
+ sleep 2
+ ps -p 5760
+ for port in "${ports[@]}"
++ start_server 22
++ local port=22
++ echo 5770
++ python3 -m http.server 22
+ server_pid=5770
+ sleep 2
+ check_port 10.128.0.3 22
+ local ip=10.128.0.3
+ local port=22
+ '[' 0 -eq 0 ']'
+ echo 'Port 22 is open on 10.128.0.3'
+ add_result 'Port 22 Check' PASS 'Port 22 is open on 10.128.0.3'
+ local 'check=Port 22 Check'
+ local result=PASS
+ local 'message=Port 22 is open on 10.128.0.3'
+ '[' 3840 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 22 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 22 is open on 10.128.0.3"
'
+ json_results+='    }'
+ ps -p 5770
+ for port in "${ports[@]}"
++ start_server 5433
++ local port=5433
++ echo 5776
++ python3 -m http.server 5433
+ server_pid=5776
+ sleep 2
+ check_port 10.128.0.3 5433
+ local ip=10.128.0.3
+ local port=5433
+ '[' 0 -eq 0 ']'
+ echo 'Port 5433 is open on 10.128.0.3'
+ add_result 'Port 5433 Check' PASS 'Port 5433 is open on 10.128.0.3'
+ local 'check=Port 5433 Check'
+ local result=PASS
+ local 'message=Port 5433 is open on 10.128.0.3'
+ '[' 3958 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 5433 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 5433 is open on 10.128.0.3"
'
+ json_results+='    }'
+ ps -p 5776
+ force_kill 5776
+ local pid=5776
+ kill 5776
+ sleep 2
+ ps -p 5776
+ for port in "${ports[@]}"
++ start_server 9042
++ local port=9042
++ echo 5784
++ python3 -m http.server 9042
+ server_pid=5784
+ sleep 2
+ check_port 10.128.0.3 9042
+ local ip=10.128.0.3
+ local port=9042
+ '[' 0 -eq 0 ']'
+ echo 'Port 9042 is open on 10.128.0.3'
+ add_result 'Port 9042 Check' PASS 'Port 9042 is open on 10.128.0.3'
+ local 'check=Port 9042 Check'
+ local result=PASS
+ local 'message=Port 9042 is open on 10.128.0.3'
+ '[' 4080 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9042 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9042 is open on 10.128.0.3"
'
+ json_results+='    }'
+ ps -p 5784
+ force_kill 5784
+ local pid=5784
+ kill 5784
+ sleep 2
+ ps -p 5784
+ for port in "${ports[@]}"
++ start_server 9070
++ local port=9070
++ echo 6115
++ python3 -m http.server 9070
+ server_pid=6115
+ sleep 2
+ check_port 10.128.0.3 9070
+ local ip=10.128.0.3
+ local port=9070
+ '[' 0 -eq 0 ']'
+ echo 'Port 9070 is open on 10.128.0.3'
+ add_result 'Port 9070 Check' PASS 'Port 9070 is open on 10.128.0.3'
+ local 'check=Port 9070 Check'
+ local result=PASS
+ local 'message=Port 9070 is open on 10.128.0.3'
+ '[' 4202 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9070 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9070 is open on 10.128.0.3"
'
+ json_results+='    }'
+ ps -p 6115
+ for port in "${ports[@]}"
++ start_server 9300
++ local port=9300
++ echo 6120
++ python3 -m http.server 9300
+ server_pid=6120
+ sleep 2
+ check_port 10.128.0.3 9300
+ local ip=10.128.0.3
+ local port=9300
+ '[' 0 -eq 0 ']'
+ echo 'Port 9300 is open on 10.128.0.3'
+ add_result 'Port 9300 Check' PASS 'Port 9300 is open on 10.128.0.3'
+ local 'check=Port 9300 Check'
+ local result=PASS
+ local 'message=Port 9300 is open on 10.128.0.3'
+ '[' 4324 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 9300 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 9300 is open on 10.128.0.3"
'
+ json_results+='    }'
+ ps -p 6120
+ for port in "${ports[@]}"
++ start_server 12000
++ local port=12000
++ echo 6126
++ python3 -m http.server 12000
+ server_pid=6126
+ sleep 2
+ check_port 10.128.0.3 12000
+ local ip=10.128.0.3
+ local port=12000
+ '[' 0 -eq 0 ']'
+ echo 'Port 12000 is open on 10.128.0.3'
+ add_result 'Port 12000 Check' PASS 'Port 12000 is open on 10.128.0.3'
+ local 'check=Port 12000 Check'
+ local result=PASS
+ local 'message=Port 12000 is open on 10.128.0.3'
+ '[' 4446 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 12000 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 12000 is open on 10.128.0.3"
'
+ json_results+='    }'
+ ps -p 6126
+ force_kill 6126
+ local pid=6126
+ kill 6126
+ sleep 2
+ ps -p 6126
+ for port in "${ports[@]}"
++ start_server 13000
++ local port=13000
++ echo 6134
++ python3 -m http.server 13000
+ server_pid=6134
+ sleep 2
+ check_port 10.128.0.3 13000
+ local ip=10.128.0.3
+ local port=13000
+ '[' 0 -eq 0 ']'
+ echo 'Port 13000 is open on 10.128.0.3'
+ add_result 'Port 13000 Check' PASS 'Port 13000 is open on 10.128.0.3'
+ local 'check=Port 13000 Check'
+ local result=PASS
+ local 'message=Port 13000 is open on 10.128.0.3'
+ '[' 4570 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Port 13000 Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Port 13000 is open on 10.128.0.3"
'
+ json_results+='    }'
+ ps -p 6134
+ force_kill 6134
+ local pid=6134
+ kill 6134
+ sleep 2
+ ps -p 6134
+ service_name=yb-node-agent.service
+ yba_url=https://35.184.240.7
+ customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
+ token=3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
+ provider_id=2dc133e8-9482-4f77-ab4d-4f617ebbfa28
+ provider_name=onPrem
++ skip_tls_verify https://35.184.240.7
++ local yba_url=https://35.184.240.7
++ [[ https://35.184.240.7 == https* ]]
++ echo --insecure
+ tls_verify_option=--insecure
+ readarray -t headers
++ _get_headers 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
++ local token=3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb
++ echo 'Accept: application/json'
++ echo 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb'
++ echo 'Content-Type: application/json'
+ header_options=()
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
+ for header in "${headers[@]}"
+ header_options+=(-H "$header")
++ systemctl show -p ActiveState yb-node-agent.service
++ grep ActiveState
++ awk -F= '{print $2}'
+ status=active
+ '[' active = active ']'
+ echo '"yb-node-agent.service" is active'
+ add_result 'Service Status Check' PASS 'yb-node-agent.service is active'
+ local 'check=Service Status Check'
+ local result=PASS
+ local 'message=yb-node-agent.service is active'
+ '[' 4694 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Service Status Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "yb-node-agent.service is active"
'
+ json_results+='    }'
++ systemctl show -p MemoryCurrent yb-node-agent.service
++ grep MemoryCurrent
++ awk -F= '{print $2}'
+ memory=9715712
+ '[' 9715712 -gt 0 ']'
+ echo 'MemoryCurrent is greater than 0: 9715712'
+ add_result 'Memory Usage Check' PASS 'MemoryCurrent is greater than 0: 9715712'
+ local 'check=Memory Usage Check'
+ local result=PASS
+ local 'message=MemoryCurrent is greater than 0: 9715712'
+ '[' 4821 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Memory Usage Check",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "MemoryCurrent is greater than 0: 9715712"
'
+ json_results+='    }'
+ '[' -z 2dc133e8-9482-4f77-ab4d-4f617ebbfa28 ']'
++ _get_nodes_in_provider https://35.184.240.7 cf6d0b42-bae0-49f2-8fa1-28a3f9216a77 2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ local yba_url=https://35.184.240.7
++ local customer_uuid=cf6d0b42-bae0-49f2-8fa1-28a3f9216a77
++ local provider_uuid=2dc133e8-9482-4f77-ab4d-4f617ebbfa28
++ echo https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
+ get_nodes_in_provider=https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
++ curl -s -w '%{http_code}' -o response.txt -X GET -H 'Accept: application/json' -H 'X-AUTH-YW-API-TOKEN: 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb' -H 'Content-Type: application/json' --insecure https://35.184.240.7/api/v1/customers/cf6d0b42-bae0-49f2-8fa1-28a3f9216a77/providers/2dc133e8-9482-4f77-ab4d-4f617ebbfa28/nodes/list
+ response=200
+ http_status=200
+ response_body='[{"nodeUuid":"8dc3bcc1-fb03-42ec-9903-a07c3983e0d0","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n3","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.3","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n3"},"detailsJson":"{\"ip\":\"10.128.0.3\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n3\"}","inUse":true},{"nodeUuid":"716b9c3c-3743-41c7-b2b8-87d92f96bf83","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n2","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.2","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n2"},"detailsJson":"{\"ip\":\"10.128.0.2\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n2\"}","inUse":true},{"nodeUuid":"ed729a1c-89d8-44cf-8031-110cddfdd47f","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n1","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.15.231","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n1"},"detailsJson":"{\"ip\":\"10.128.15.231\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n1\"}","inUse":true}]'
+ matched=false
+ '[' 200 -ge 200 ']'
+ '[' 200 -lt 300 ']'
+ echo 'HTTP GET request successful. Processing response...'
++ echo '[{"nodeUuid":"8dc3bcc1-fb03-42ec-9903-a07c3983e0d0","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n3","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.3","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n3"},"detailsJson":"{\"ip\":\"10.128.0.3\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n3\"}","inUse":true},{"nodeUuid":"716b9c3c-3743-41c7-b2b8-87d92f96bf83","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n2","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.0.2","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n2"},"detailsJson":"{\"ip\":\"10.128.0.2\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n2\"}","inUse":true},{"nodeUuid":"ed729a1c-89d8-44cf-8031-110cddfdd47f","instanceTypeCode":"n2","nodeName":"yb-dev-TU-n1","instanceName":"n2","zoneUuid":"559eb98c-4f80-4304-ac69-a89a36fb2a4f","state":"USED","details":{"ip":"10.128.15.231","region":"us-central1","zone":"us-central1-a","instanceType":"n2","instanceName":"n2","nodeName":"yb-dev-TU-n1"},"detailsJson":"{\"ip\":\"10.128.15.231\",\"region\":\"us-central1\",\"zone\":\"us-central1-a\",\"instanceType\":\"n2\",\"instanceName\":\"n2\",\"nodeName\":\"yb-dev-TU-n1\"}","inUse":true}]'
++ grep -o '"ip":"[a-zA-Z0-9.:_-]*"'
++ cut -d '"' -f4
+ ips='10.128.0.3
10.128.0.2
10.128.15.231'
+ for ip in $ips
+ [[ 10.128.0.3 == \1\0\.\1\2\8\.\0\.\3 ]]
+ matched=true
+ break
+ [[ true == false ]]
+ echo 'Node addition to the provider passed: 10.128.0.3'
+ add_result 'Node addition failed' PASS 'Node addition to the provider passed: 10.128.0.3'
+ local 'check=Node addition failed'
+ local result=PASS
+ local 'message=Node addition to the provider passed: 10.128.0.3'
+ '[' 4955 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "Node addition failed",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "Node addition to the provider passed: 10.128.0.3"
'
+ json_results+='    }'
+ check_marker_file
+ MARKER_FILE=/tmp/.reboot-required
+ '[' -f /tmp/.reboot-required ']'
+ echo 'Marker file does not exist: /tmp/.reboot-required'
+ return 1
+ echo 'Reboot requirement cleared.'
+ add_result 'REBOOT REQUIRED' PASS 'No Marker file present'
+ local 'check=REBOOT REQUIRED'
+ local result=PASS
+ local 'message=No Marker file present'
+ '[' 5099 -gt 20 ']'
+ json_results+=',
'
+ json_results+='    {
'
+ json_results+='      "check": "REBOOT REQUIRED",
'
+ json_results+='      "result": "PASS",
'
+ json_results+='      "message": "No Marker file present"
'
+ json_results+='    }'
+ print_results
+ any_fail=0
+ [[ {
"results":[
    {
      "check": "Clock Synchronization",
      "result": "PASS",
      "message": "System clock synchronized"
    },
    {
      "check": "Yugabyte User Existence Check",
      "result": "PASS",
      "message": "User yugabyte exists"
    },
    {
      "check": "Yugabyte Home Directory Permissions Check",
      "result": "PASS",
      "message": "/data/home/yugabyte has the correct ownership and acceptable permissions"
    },
    {
      "check": "/data Ownership/Permission Check",
      "result": "PASS",
      "message": "Directory /data is owned by yugabyte or has '7' (rwx) permissions."
    },
    {
      "check": "/data Free Space Check",
      "result": "PASS",
      "message": "Sufficient disk space available: 196G"
    },
    {
      "check": "Systemd Unit File Check - yb-tserver.service",
      "result": "PASS",
      "message": "Systemd unit yb-tserver.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-master.service",
      "result": "PASS",
      "message": "Systemd unit yb-master.service is configured."
    },
    {
      "check": "Systemd Unit File Check - clock-sync.sh.j2",
      "result": "PASS",
      "message": "Systemd unit clock-sync.sh.j2 is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-bind_check.service",
      "result": "PASS",
      "message": "Systemd unit yb-bind_check.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.service",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.timer",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.service",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.timer",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-controller.service",
      "result": "PASS",
      "message": "Systemd unit yb-controller.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.service",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.timer",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.timer is configured."
    },
    {
      "check": "vm.swappiness",
      "result": "PASS",
      "message": "vm.swappiness is set to 0 (expected: 0)"
    },
    {
      "check": "vm.max_map_count",
      "result": "PASS",
      "message": "vm.max_map_count is set to 262144 (expected: 262144)"
    },
    {
      "check": "kernel.core_pattern",
      "result": "PASS",
      "message": "kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "node_exporter.service is active"
    },
    {
      "check": "Port 7000 Check",
      "result": "PASS",
      "message": "Port 7000 is open on 10.128.0.3"
    },
    {
      "check": "Port 7100 Check",
      "result": "PASS",
      "message": "Port 7100 is open on 10.128.0.3"
    },
    {
      "check": "Port 9000 Check",
      "result": "PASS",
      "message": "Port 9000 is open on 10.128.0.3"
    },
    {
      "check": "Port 9100 Check",
      "result": "PASS",
      "message": "Port 9100 is open on 10.128.0.3"
    },
    {
      "check": "Port 18018 Check",
      "result": "PASS",
      "message": "Port 18018 is open on 10.128.0.3"
    },
    {
      "check": "Port 22 Check",
      "result": "PASS",
      "message": "Port 22 is open on 10.128.0.3"
    },
    {
      "check": "Port 5433 Check",
      "result": "PASS",
      "message": "Port 5433 is open on 10.128.0.3"
    },
    {
      "check": "Port 9042 Check",
      "result": "PASS",
      "message": "Port 9042 is open on 10.128.0.3"
    },
    {
      "check": "Port 9070 Check",
      "result": "PASS",
      "message": "Port 9070 is open on 10.128.0.3"
    },
    {
      "check": "Port 9300 Check",
      "result": "PASS",
      "message": "Port 9300 is open on 10.128.0.3"
    },
    {
      "check": "Port 12000 Check",
      "result": "PASS",
      "message": "Port 12000 is open on 10.128.0.3"
    },
    {
      "check": "Port 13000 Check",
      "result": "PASS",
      "message": "Port 13000 is open on 10.128.0.3"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "yb-node-agent.service is active"
    },
    {
      "check": "Memory Usage Check",
      "result": "PASS",
      "message": "MemoryCurrent is greater than 0: 9715712"
    },
    {
      "check": "Node addition failed",
      "result": "PASS",
      "message": "Node addition to the provider passed: 10.128.0.3"
    },
    {
      "check": "REBOOT REQUIRED",
      "result": "PASS",
      "message": "No Marker file present"
    } == *\"\r\e\s\u\l\t\"\:\ \"\F\A\I\L\"* ]]
+ json_results+='
]}'
+ echo '{
"results":[
    {
      "check": "Clock Synchronization",
      "result": "PASS",
      "message": "System clock synchronized"
    },
    {
      "check": "Yugabyte User Existence Check",
      "result": "PASS",
      "message": "User yugabyte exists"
    },
    {
      "check": "Yugabyte Home Directory Permissions Check",
      "result": "PASS",
      "message": "/data/home/yugabyte has the correct ownership and acceptable permissions"
    },
    {
      "check": "/data Ownership/Permission Check",
      "result": "PASS",
      "message": "Directory /data is owned by yugabyte or has '\''7'\'' (rwx) permissions."
    },
    {
      "check": "/data Free Space Check",
      "result": "PASS",
      "message": "Sufficient disk space available: 196G"
    },
    {
      "check": "Systemd Unit File Check - yb-tserver.service",
      "result": "PASS",
      "message": "Systemd unit yb-tserver.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-master.service",
      "result": "PASS",
      "message": "Systemd unit yb-master.service is configured."
    },
    {
      "check": "Systemd Unit File Check - clock-sync.sh.j2",
      "result": "PASS",
      "message": "Systemd unit clock-sync.sh.j2 is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-bind_check.service",
      "result": "PASS",
      "message": "Systemd unit yb-bind_check.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.service",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-clean_cores.timer",
      "result": "PASS",
      "message": "Systemd unit yb-clean_cores.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.service",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-collect_metrics.timer",
      "result": "PASS",
      "message": "Systemd unit yb-collect_metrics.timer is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-controller.service",
      "result": "PASS",
      "message": "Systemd unit yb-controller.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.service",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.service is configured."
    },
    {
      "check": "Systemd Unit File Check - yb-zip_purge_yb_logs.timer",
      "result": "PASS",
      "message": "Systemd unit yb-zip_purge_yb_logs.timer is configured."
    },
    {
      "check": "vm.swappiness",
      "result": "PASS",
      "message": "vm.swappiness is set to 0 (expected: 0)"
    },
    {
      "check": "vm.max_map_count",
      "result": "PASS",
      "message": "vm.max_map_count is set to 262144 (expected: 262144)"
    },
    {
      "check": "kernel.core_pattern",
      "result": "PASS",
      "message": "kernel.core_pattern is set to /data/home/yugabyte/cores/core_%p_%t_%E"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "node_exporter.service is active"
    },
    {
      "check": "Port 7000 Check",
      "result": "PASS",
      "message": "Port 7000 is open on 10.128.0.3"
    },
    {
      "check": "Port 7100 Check",
      "result": "PASS",
      "message": "Port 7100 is open on 10.128.0.3"
    },
    {
      "check": "Port 9000 Check",
      "result": "PASS",
      "message": "Port 9000 is open on 10.128.0.3"
    },
    {
      "check": "Port 9100 Check",
      "result": "PASS",
      "message": "Port 9100 is open on 10.128.0.3"
    },
    {
      "check": "Port 18018 Check",
      "result": "PASS",
      "message": "Port 18018 is open on 10.128.0.3"
    },
    {
      "check": "Port 22 Check",
      "result": "PASS",
      "message": "Port 22 is open on 10.128.0.3"
    },
    {
      "check": "Port 5433 Check",
      "result": "PASS",
      "message": "Port 5433 is open on 10.128.0.3"
    },
    {
      "check": "Port 9042 Check",
      "result": "PASS",
      "message": "Port 9042 is open on 10.128.0.3"
    },
    {
      "check": "Port 9070 Check",
      "result": "PASS",
      "message": "Port 9070 is open on 10.128.0.3"
    },
    {
      "check": "Port 9300 Check",
      "result": "PASS",
      "message": "Port 9300 is open on 10.128.0.3"
    },
    {
      "check": "Port 12000 Check",
      "result": "PASS",
      "message": "Port 12000 is open on 10.128.0.3"
    },
    {
      "check": "Port 13000 Check",
      "result": "PASS",
      "message": "Port 13000 is open on 10.128.0.3"
    },
    {
      "check": "Service Status Check",
      "result": "PASS",
      "message": "yb-node-agent.service is active"
    },
    {
      "check": "Memory Usage Check",
      "result": "PASS",
      "message": "MemoryCurrent is greater than 0: 9715712"
    },
    {
      "check": "Node addition failed",
      "result": "PASS",
      "message": "Node addition to the provider passed: 10.128.0.3"
    },
    {
      "check": "REBOOT REQUIRED",
      "result": "PASS",
      "message": "No Marker file present"
    }
]}'
+ '[' 0 -eq 1 ']'
+ echo 'Pre-flight checks successful'

2025-06-02 15:00:44,400 - commands.provision_command - INFO - Return Code: 0
INI file has been created successfully at: /data/2024.2.2.2-b2/scripts/ynp/configs/config.ini
File written successfully and crash-consistent.
2025-06-02 15:00:44,739 - INFO - Running: /opt/yugabyte/software/active/yb-platform/yugaware/yba-cli/yba_cli-2024.2.2.2-b2-linux-amd64/yba universe node reprovision -H https://35.184.240.7/ -a 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb --name TU --node-name yb-dev-TU-n3

Waiting for node yb-dev-TU-n3 operation Reprovision to be completed
The node yb-dev-TU-n3 operation Reprovision has been completed
Node Name      Node UUID                              IP           State     Is Master process running   Is Tserver process running   Master state
yb-dev-TU-n3   8dc3bcc1-fb03-42ec-9903-a07c3983e0d0   10.128.0.3   Stopped   false                       false                        -
2025-06-02 15:03:08,228 - INFO - Running: /opt/yugabyte/software/active/yb-platform/yugaware/yba-cli/yba_cli-2024.2.2.2-b2-linux-amd64/yba universe node start -H https://35.184.240.7/ -a 3.a53f0440-db9b-400a-8c13-e850717adca7.d704d3b3-bcfc-4347-a0e2-50a14e0e89eb --name TU --node-name yb-dev-TU-n3

Waiting for node yb-dev-TU-n3 operation Start to be completed
The node yb-dev-TU-n3 operation Start has been completed
Node Name      Node UUID                              IP           State     Is Master process running   Is Tserver process running   Master state
yb-dev-TU-n3   8dc3bcc1-fb03-42ec-9903-a07c3983e0d0   10.128.0.3   Live      true                        true                         
2025-06-02 15:05:39,708 - INFO - âœ… Rehydration complete for node yb-dev-TU-n3
2025-06-02 15:05:39,708 - INFO - 
================================================================================
REHYDRATION PROCESS SUMMARY
================================================================================

Total Nodes Processed: 3
Successful Nodes: 3
Failed Nodes: 0

================================================================================
NODE DETAILS
================================================================================

Node: yb-dev-TU-n1
IP: 10.128.15.231
Instance: yb-tu-dev-n5
Zone: us-central1-c
Status: âœ… SUCCESS
--------------------------------------------------------------------------------

Node: yb-dev-TU-n2
IP: 10.128.0.2
Instance: yb-tu-dev-n1
Zone: us-central1-a
Status: âœ… SUCCESS
--------------------------------------------------------------------------------

Node: yb-dev-TU-n3
IP: 10.128.0.3
Instance: yb-tu-dev-n4
Zone: us-central1-b
Status: âœ… SUCCESS
--------------------------------------------------------------------------------

2025-06-02 15:05:39,709 - INFO - Summary saved to rehydration_summary_20250602_150539.txt
